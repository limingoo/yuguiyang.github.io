<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[愿人生从容(置顶)]]></title>
    <url>%2F2017%2F12%2F31%2Ftop%2F</url>
    <content type="text"><![CDATA[Hi，欢迎访问我的博客。 在这里分享BI、数据分析等相关的技术内容；个人随笔、感悟。 欢迎和我交流分享。 愿你出走半生，归来仍是少年。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书笔记-数据化运营(1章）]]></title>
    <url>%2F2017%2F10%2F27%2Freading_notes_data_analysis_03%2F</url>
    <content type="text"><![CDATA[读书笔记数据化运营速成手册第一章 这本书，大概翻了一遍，感觉还不错，作者结合自身经验从实际问题角度出发给我们介绍了数据化运营、数据分析，书中图表、分析模型都是基于Excel的，不需要学习其他的工具，对于我们上手来说很简单。后面就让我们跟随作者的脚步，一步一步来学习下。 对于数据分析、数据化运营来说，最终的表现形式可能是一份数据分析报告，都说“字不如表，表不如图”，所以报告中可能会是各种图表，然而，是不是所有的报告或者需求都需要使用图表呢？这是我们首先要判断的问题。 真的要做图吗？我们拿到一个需求，面对收集好的数据，我们首先要思考的是：真的要做图吗？有时候，文字报告或者一个表格的表达效果会比图表好得多。我们最终的目的，其实是准确传达信息或者解决业务方的疑问，不要盲目的认为图表一定是最好的。 短期内了解性需求一些临时性的了解某个指标实际情况的需求：“近一个月的日活、上个月的支出”，我们只要提供准确的数据就好了。 多维度相互组合的需求这里说的就是指标涉及的维度比较多，用图表没有办法准确的传达信息，作者有举一个具体的例子: 老板说他想看上个月各个城市中VIP客户和非VIP客户数量的对比和同比上上月的情况。 基础数据是这样的 转化成图表可能会是这样，图看上去还不错上面的折线图用的不太妥，折线图一般可能用在时间序列上或者具有演进关系的，而地区之间并没有这种演进关系，当然，我们可以改一下图表： 这样虽然满足了，但是图的效果差些，而且，如果老板又要增加一个VIP用户占比情况，图表就没法好好表达了，所以，最直接的，我们就是使用一个表格来展现数据 当然，可以再好看些，像这样： 图表传达的信息有限我们要用正确的图表表达正确的理念，每种图表都有他擅长表达的领域；如果一些观点不能用图表完全表达出来，一定要放弃使用图表 数据间存在复杂逻辑这里说的其实也是不能用图表完全表达观点的情况，有时候，数据集之间可能互相影响，互为因果，这里作者举了一个用户行为路径转化的例子 图表的基本构成排除上面几种情况，最后确定要做图表的话，我们就要了解图表的基本特征，这里作者主要介绍了图表的基本元素 坐标轴 图例的位置 辅助线 数据标签等等这里就不详细介绍了，后面，我们再具体的图标中，再介绍 控制图标中的信息量在最开始，我们就要注意，图标中表达的内容一定要直观，简单，不要融合太多的指标，指标多了以后，就会产生混淆，产生误解。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>读书笔记</tag>
        <tag>实例</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见图表-直方图]]></title>
    <url>%2F2017%2F10%2F25%2Fstatistics-handbook-02%2F</url>
    <content type="text"><![CDATA[常见图表介绍及实例 什么是直方图 直方图(Histogram)又称质量分布图。是一种统计报告图，由一系列高度不等的纵向条纹或线段表示数据分布的情况。 一般用横轴表示数据类型，纵轴表示分布情况。为了构建直方图，第一步是将值的范围分段，即将整个值的范围分成一系列间隔，然后计算每个间隔中有多少值。 这些值通常被指定为连续的，不重叠的变量间隔。 间隔必须相邻，并且通常是（但不是必须的）相等的大小。 直方图一开始可能是应用在工业生产领域，来做质量评估，判断生产稳定性 组数：在统计数据时，我们把数据按照不同的范围分成几个组，分成的组的个数称为组数。组距：每一组两个端点的差。 作用（优势） 显示数据波动状态 直观的表达数据分布趋势 方便找到应该关注的点 绘制方法– from 百度百科 ①集中和记录数据，求出其最大值和最小值。数据的数量应在100个以上，在数量不多的情况下，至少也应在 50个以上。 我们把分成组的个数称为组数，每一个组的两个端点的差称为组距。②将数据分成若干组，并做好记号。分组的数量在5－12之间较为适宜。③计算组距的宽度。用最大值和最小值之差去除组数，求出组距的宽度。④计算各组的界限位。各组的界限位可以从第一组开始依次计算，第一组的下界为最小值减去最小测定单位的一半，第一组的上界为其下界值加上组距。第二组的下界限位为第一组的上界限值，第二组的下界限值加上组距，就是第二组的上界限位，依此类推。⑤统计各组数据出现频数，作频数分布表。⑥作直方图。以组距为底长，以频数为高，作各组的矩形图。 实例假设我们有公司所有部门的KPI完成情况，如下图： 我们想要了解公司的整体运营情况，用图表来看会更直观，这就正好用上了直方图，来看公司在每一个区间的分布情况；我们以0.3为组距，分成5个组（数据是随机生成的，貌似有点儿不太靠谱，如果公司的KPI完成率是这样，我觉得KPI定义会稍微有问题，基本上都没达标啊） 然后，我们在Excel中，创建一个图表，就可以直观的看到KPI的完成情况分布了]]></content>
      <categories>
        <category>统计知识</category>
      </categories>
      <tags>
        <tag>统计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[平均数小记]]></title>
    <url>%2F2017%2F10%2F24%2Fstatistics-handbook-01%2F</url>
    <content type="text"><![CDATA[我们日常生活中，经常会遇到平均数像什么平均工资，平均身高，平均成绩之类的，曾经在课上学习的相关知识估计早就忘记了，这里我们就来回顾下。(下面部分介绍及例子摘自百度百科) 基本概念算术平均数(arithmetic mean)通常我们说的平均数都是“算术平均数”：平均成绩、平均身高、平均收入… 一组数据中所有数据之和再除以这组数据的个数，他反映一组数据的集中趋势– 百度百科 加权算术平均数 加权平均值即将各数值乘以相应的权数，然后加总求和得到总体值，再除以总的单位数。 这个表示每个数据都有一个权重，也比较常见，比如： 通常我们再绩效考核中，有自评、领导评价，领导的评价一般权重较高， 最后的84分（90*40%+80*60%）是本月的绩效评分 中位数(median) 中位数是指将数据按大小顺序排列起来，形成一个数列，居于数列中间位置的那个数据 当一组数据中有极端值时，”中位数”比“算术平均数”可以更好的表示平均水平下面是一组学生的数学成绩，看“算术平均数”的话，平均分是63勉强超过了及格线，看上去可能还好，但实际上呢？这组学生大部分人都没有及格，都在50分一下，是因为有2名学生考了100分，整体拉高了平均成绩；而中位数就就49分，已经大大低于60分了，可以更直观的表现该组学生的一个整体成绩，还需要多多关注数学的学习。 这就比较类似“被平均”的情况，光看平均分是及格了，但实际上，大多数学生并没有好好的掌握这次数学考试的内容，普通的学生被2个学霸平均了。 就平均收入来说，新闻上经常会出现哪个城市的平均工资是多少，同比增长了多少，每次看到这样的数据，都会觉得自己拖了后腿。首先社会财富的分配本身就不是均匀的，根据二八定律，社会上20%的人掌握着80%的财富，一平均数据很好看，而实际上贫富分化依然很严重。 众数 众数是指一组数据中出现次数最多的那个数据，一组数据可以有多个众数，也可以没有众数。 以上面成绩的数据为例，众数就是100，他代表了这组数据的一个峰值，对于我们想看整体情况，没有太多参考意义。 在其他一些场景下，智库百科上有这样一个例子： 从表中可以看到，25.5厘米的鞋号销售量最多，如果我们计算算术平均数，则平均号码为25.65厘米，而这个号码显然是没有实际意义的，而直接用25.5厘米作为顾客对男皮鞋所需尺寸的集中趋势既便捷又符合实际。 几何平均数 几何平均数是n个变量值连乘积的n次方根 这个平时用的好像比较少，他主要用在计算比率、平均速度等，他受极端值影响比“算术平均数”小，还以上面考试成绩为例，他的几何平均数是59 百度上有这样一个例子： 就平均数的话，还有很多其他的内容，这里暂时不举例了，后面用到了再说 对比中位数与平均数与众数这里的平均数指“算术平均数” 平均数主要表示数据整体水平，容易受极端数据影响 中位数表示数据的中等水平 众数反映的是一组数据中的集中程度，日常遇到的什么“最佳”、“最受欢迎”等等，都是众数的概念]]></content>
      <categories>
        <category>统计基础</category>
      </categories>
      <tags>
        <tag>统计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-每周练习答案(2017-10-20)]]></title>
    <url>%2F2017%2F10%2F21%2Fmysql-handbook-14%2F</url>
    <content type="text"><![CDATA[MySQL每周练习答案 这里和大家分享下本周练习题的一种解题思路 将题目简化一下，其实就是实现这样一个功能： 我们将使用逗号分隔的数据，拆分为多行数据，熟悉MySQL的同学，可能会想到，这有点儿像group_concat函数，但这是他的逆过程 测试数据1234567891011create table tm_company( company_name varchar(10), company_industry varchar(20));insert into tm_company(company_name,company_industry) values('A公司','移动互联网,金融');insert into tm_company(company_name,company_industry) values('B公司','移动互联网');insert into tm_company(company_name,company_industry) values('C公司','教育,招聘,魔法');select *from tm_company; 解决方案我们先来思考这样一个问题，怎样才能把用逗号分隔的数据拆分呢？这里需要了解MySQL的字符串函数https://dev.mysql.com/doc/refman/5.7/en/string-functions.html SUBSTRING_INDEX找一下，会发现这个函数，挺符合我们需求的 SUBSTRING_INDEX(str,delim,count)str 就是我们要分隔的字符串delim 就是我们的分隔符当count为正数时，我们从左侧开始截取，截止到delim第count次出现时；当count为负数时，则从右侧开始 1234mysql&gt; SELECT SUBSTRING_INDEX('www.mysql.com', '.', 2); -&gt; 'www.mysql'mysql&gt; SELECT SUBSTRING_INDEX('www.mysql.com', '.', -2); -&gt; 'mysql.com' 我们来试一下1234567select company_name, substring_index(company_industry,',',1), substring_index(company_industry,',',2), substring_index(company_industry,',',-1)from tm_company; 看上去，好像接近我们想要的数据，但还是有些问题比如，我们需要判断到底要分隔几次，那该怎样判断要分隔几次呢？其实就是看有几个逗号，那怎样判断有几个逗号呢？好像并没有判断字符出现次数的函数，但我们可以绕个弯来实现像这样，我们把逗号都替换掉，看数据长度减少了多少，就可以了123456select company_name, company_industry, length(company_industry)-length(replace(company_industry,',',''))from tm_company; 我们再来观察下这个数据 1个逗号的时候，我们需要拆分2次 0个逗号的时候，我们需要拆分1次 2个逗号的时候，我们需要拆分3次 我们以C公司的数据来测试下1234select substring_index('教育,招聘,魔法',',',1), substring_index('教育,招聘,魔法',',',2), substring_index('教育,招聘,魔法',',',3) 我们观察下，会发现，最右边的数据就是我们想要的 1234select substring_index(substring_index('教育,招聘,魔法',',',1),',',-1), substring_index(substring_index('教育,招聘,魔法',',',2),',',-1), substring_index(substring_index('教育,招聘,魔法',',',3),',',-1) 哎，好像是了嘛，数据现在已经分隔好了，现在的问题，好像变成了列转行 列转行因为我们的数据是动态分隔的，所以，做列转行的话，不能用之前的方法，可能需要使用另一种方法我们看下上面分隔的时候，使用的下标，是自增的，而且和逗号的数量也有关畅想一下（估计是经验），可以这样来做1234567select *from ( select 0 as id union select 1 union select 2 union select 3 union select 4) base 通常，我们会创建一张基础表，来方便使用。我们可以这个数据来表示逗号的数量，然后这样 123456789101112131415select a.company_name, a.company_industry, substring_index(substring_index(a.company_industry,',',b.id+1),',',-1)from tm_company a join ( select 0 as id union select 1 union select 2 union select 3 union select 4) b on b.id &lt;= length(a.company_industry)-length(replace(a.company_industry,',',''))order by a.company_name; 好了，完成，我们通过1对多的关系，进行了列转行，再配合substring_index函数，我们就完成了上面的问题。 先这样，大家先理解下，欢迎反馈。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-每周练习(2017-10-20)]]></title>
    <url>%2F2017%2F10%2F18%2Fmysql-handbook-13%2F</url>
    <content type="text"><![CDATA[MySQL每周练习 本周我们来一道数据处理的练习题。 数据背景不知道大家学会爬虫了没，拉勾网的数据大家会爬取了吗？这道题和拉勾网有关哦。假设你已经学会爬取数据了，可以将数据爬取下来，数据可能是这个样子(demo库中的tm_lagou_data表)：1234567891011121314151617CREATE TABLE `tm_lagou_data` ( `city` varchar(20) DEFAULT NULL COMMENT '城市', `company_short_name` varchar(100) DEFAULT NULL COMMENT '公司简称', `company_full_name` varchar(200) DEFAULT NULL COMMENT '公司全称', `company_industry` varchar(100) DEFAULT NULL COMMENT '所属行业', `company_location` varchar(100) DEFAULT NULL COMMENT '工作地点', `position_advantage` varchar(100) DEFAULT NULL COMMENT '岗位特点', `position_salary` varchar(20) DEFAULT NULL COMMENT '薪资', `position_workyear` varchar(20) DEFAULT NULL COMMENT '工作经验', `position_name` varchar(50) DEFAULT NULL COMMENT '职位名称', `position_first_type` varchar(100) DEFAULT NULL COMMENT '岗位类型-大类', `position_second_type` varchar(100) DEFAULT NULL COMMENT '岗位类型-小类', `position_lables` varchar(100) DEFAULT NULL COMMENT '岗位标签', `position_id` varchar(20) DEFAULT NULL COMMENT '岗位ID', `create_time` datetime DEFAULT NULL COMMENT '发布时间', `job_desc` text comment '岗位描述') ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='拉勾网-数据分析数据'; 在Python中，我们没有过多的处理，数据是这个样子的 这一次呢，我们只需要关注一个字段即可company_industry，这是公司所属行业这个行业呢，一般会有多个，像上海的这个挖财网，就是互联网+金融，有2个标签，中间是逗号分隔符 问题描述原始数据：tm_lagou_data表中，company_industry（所属行业）字段可能会有多个值，并用逗号分隔 我们的目的是把这个字段拆分，变成下面这样的数据 处理后数据： 知识点为了解决上面的问题，我们需要掌握的知识点如下： 多表关联:MySQL-关联查询Mysql 连接的使用 字符串函数: https://dev.mysql.com/doc/refman/5.7/en/string-functions.html 附有了拆分后的数据，我们就可以看数据分析师的行业分布，哪个行业招的数据分析师最多，哪个行业招的最少了。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析师是否需要掌握SQL？]]></title>
    <url>%2F2017%2F10%2F18%2Fjotting-03%2F</url>
    <content type="text"><![CDATA[前面，我们分析了一下数据分析师的前景要不要找一份数据分析的工作？ 这里，我们研究下，数据分析师是否需要掌握SQL。 什么是数据分析师？我们从字面来理解，数据分析师=数据+分析+师 数据对于数据分析师来说，数据是基础，俗话说，巧妇难为无米之炊，没有数据，做什么可视化，做什么分析，没有数据支撑的分析报告，一定是异想天开。数据的来源通常有2中，一种是公司内部数据，一种是公司外部数据。外部数据，我们可能需要通过第三方渠道购买，或者使用爬虫从网上爬取，通常的竞品分析啊，舆情分析啊，都属于这种；内部数据，一般是公司业务数据，存储在数据库中，我们可以使用SQL来实现自己的业务需求。 分析有了数据，我们需要在业务的基础上，通过分析方法、分析思维、模型等来发现问题，验证问题。这个分析一定要基于业务，脱离业务的分析是没有任何价值的。 真的要掌握SQL吗？上面，我们说了，分析公司内部的数据，需要使用SQL，行业内的其他公司是不是也是这样呢？前面，我们爬取了拉勾网的数据，这里，我们就可以通过招聘数据证明一下，数据分析师是否需要掌握SQL 我们根据岗位需求的标签和岗位描述来判断是否需要掌握SQL12345678IF ( CONTAINS(upper([position_lables]),'SQL') OR CONTAINS(upper([job_desc]),'SQL') )THEN '是'ELSE '否'END 就岗位数量来看，每个城市的岗位中，需要掌握SQL的岗位都超过50% 就算SQL不是数据分析师必备的技能，掌握了SQL，你就多了50%的机会，我们再来看下，哪些公司的岗位需要掌握SQL呢 这些都是互联网知名的企业，跟着他们走没有问题。 总上所述，掌握了SQL，不会吃亏，不会上当，不说啦，我得学习SQL去了。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[要不要找一份数据分析的工作？]]></title>
    <url>%2F2017%2F10%2F18%2Fjotting-02%2F</url>
    <content type="text"><![CDATA[前面，我们使用Python将拉勾网的数据爬取了下来，下面，我们就用Tableau做下探索式分析。 数据基本情况介绍我们在Python中只做了简单的清洗和整合，数据保存在MySQL中，数据只抓取了几个热门城市表结构如下1234567891011121314151617CREATE TABLE `tm_lagou_data` ( `city` varchar(20) DEFAULT NULL COMMENT '城市', `company_short_name` varchar(100) DEFAULT NULL COMMENT '公司简称', `company_full_name` varchar(200) DEFAULT NULL COMMENT '公司全称', `company_industry` varchar(100) DEFAULT NULL COMMENT '所属行业', `company_location` varchar(100) DEFAULT NULL COMMENT '工作地点', `position_advantage` varchar(100) DEFAULT NULL COMMENT '岗位特点', `position_salary` varchar(20) DEFAULT NULL COMMENT '薪资', `position_workyear` varchar(20) DEFAULT NULL COMMENT '工作经验', `position_name` varchar(50) DEFAULT NULL COMMENT '职位名称', `position_first_type` varchar(100) DEFAULT NULL COMMENT '岗位类型-大类', `position_second_type` varchar(100) DEFAULT NULL COMMENT '岗位类型-小类', `position_lables` varchar(100) DEFAULT NULL COMMENT '岗位标签', `position_id` varchar(20) DEFAULT NULL COMMENT '岗位ID', `create_time` datetime DEFAULT NULL COMMENT '发布时间', `job_desc` text comment '岗位描述') ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='拉勾网-数据分析数据'; 洗个脸，刷个牙我们要开始分析啦。 哪儿的数据分析岗位最多呢？作为一名求职者，我想知道哪里的数据分析岗位最多，我以后要去哪个城市上班呢？是不是我喜欢的城市呢？打开Tableau，连上数据库，我们开始我们的数据中，有城市信息，也有招聘公司信息，岗位信息，所以这个很简单 我们会发现，北上深杭广招聘数据分析的公司和岗位最多，公司多，招聘的岗位也多嘛，所以，如果要找数据分析的工作，在这几个城市还是很有机会的，随着北上广深等城市的带动，新一线城市后续应该也会逐渐提高对数据的重视，数据分析岗位应该会越来越多。这里也发现一个有趣的问题，就是杭州和广州，以前我们说一线城市，可能主要是北上广深，最近几年随着杭州的崛起（我觉得和阿里有很大的关系），广州是不是逐渐的掉队了呢？ 最近正好十九大，推出了纪念邮票，好多新闻都是这张图 广州到底咋了，后面我们也可以找些数据分析下。单从拉勾的招聘信息的确可以看出，杭州的对数据分析的需求量的确稍大于广州。我们顺便看下杭州和广州，在哪些行业存在差距 就招聘数据分析的公司数量来看，移动互联网、金融、O2O等行业，杭州都明显多于广州，但一些行业，像游戏、娱乐，广州还是多于杭州的，这可能和创业的环境也有关系 是我喜欢的行业吗？以前刚毕业的时候，觉得行业不重要，哪里工资高我就去哪儿（当然，去哪儿不归我管，人家得要我啊）工作这些年发现，选择一个喜欢的行业真的很重要，前面，我们也看了杭州和广州的行业分布，整体环境呢？ 需求量最高的依然的移动互联网，现在都是互联网+，移动端更加的流量越来越大，这些行业也都走在技术前沿。 数据分析对工作经验有啥要求？前面我们知道了，北上深杭对数据分析师的需求量很大，我刚毕业，或者工作1，2年，可以去应聘吗？我们来看看。 就工作经验来看，北上深杭广70%以上的岗位都是1-3年、3-5年的工作经验，所以，好好准备下，放心大胆的投入数据分析的事业中去吧。应届生也不用怕，可能校招的岗位这些互联网公司没有发在拉钩上，多关注企业的招聘公告和宣讲会之类的；这里还有不限工作经验的工作呢，只要你掌握数据分析的技能和相关知识，一定可以的。 薪资够付房租吗？找工作一个是前景，一个是钱景，房租那么贵，数据分析师的工资咋样？由于网上招聘的薪资，都是一个区间，比如 10K-20K，我就取了个平均值，然后取中位数，这样不是很严谨，但也能反映一些问题 整体来看，薪资还是基本符合大众情况的，但是北京那个10年以上的咋才16K呢？一定还有别的附加福利，数据是上周爬的，现在已经没有了，等我后面重跑下脚本看看 单从工作经验来看薪资分布 练好技术，练好口才，面试时要谈好自己的工资啊，物价越来越高，不能光给我们谈理想，我们还得生活呢！ 有啥别的福利（面试时咋忽悠你）拉勾上的招聘信息都有一个标签，岗位的一些优势，我们做成一个词云图看看 主要是基本的福利保障，弹性工作啊，带薪年假啊，年终奖啊，发展空间啥的，面试时，一定得问好，别不好意思问。 好辣，回到主题，我们要不要找一份数据分析的工作呢？ 机会很多，北上深杭广需求量都不少 行业也不错，紧随时代潮流 薪资也可以啊，主要还是看自己实力哈 综上所述，数据分析很不靠谱，大家都不要来呦（老板，招不招数据分析，觉得我咋样）]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
        <tag>数据分析报告</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-疑问汇总]]></title>
    <url>%2F2017%2F10%2F12%2Fmysql-handbook-12%2F</url>
    <content type="text"><![CDATA[MySQL疑问汇总 这里吧同学们遇到的问题都汇总起来，方便大家一起查阅。 update at 2017-10-13Workbench安装问题昨天就说过Workbench的安装问题，具体的可以往下看，这里记录一个类似问题因为安装Workbench需要一些依赖先安装，比如那个.NET Framework，官网上提供的连接地址应该没有修改，如果直接跳转去下载，应该是.NET FRAMEWORK４.５的，但实际安装的时候，是需要4.5.2的 而且在安装4.5的时候，可能还会遇到这样的情况，说本地已经安装过了，所以去下载4.5.2就可以了， 后面经过同学的验证，就是这样解决的，没有问题。 连接测试数据库今天遇到一个使用客户端连接测试数据库一直不行的问题。背景是这样的：测试数据库在阿里云上，很多人都可以访问，说明数据库配置啥的没有问题，但是有一位同学的电脑就是访问不了，报下面这个错误 网上也找过，都说是服务器端配置问题，但是已知服务器端没有问题，这就奇了怪了，该电脑也是可以ping通服务器的。后面了解到，这位同学是在外企，公司的网络是美国的网络，但是网上查说国外访问国内的阿里云是可以的。没有想到其他什么原因，估计就是所在的公司网络有问题，导致连不到阿里云。最后，同学回家后，用家里的网络就可以访问了。这种问题，常见解决方法就是，排查2个方面 服务器端配置问题 本地网络问题 ###SQLZoo练习题原题地址http://zh.sqlzoo.net/wiki/More_JOIN_operations第13题 这道题呢，不是很难，有2点比较难 英文的，哈哈，借助了有道翻译 一个字段没理解含义，导致出错 题目是啥意思呢？就是说要返回一个演员列表，按照字母顺序，主演过30个以上的角色我们只要搞明白那几张表就行了 这个演员名单表，存的是电影ID和演员ID，注意这个ord表示的是，电影中演员的排名，ord=1才表示主演参考介绍http://zh.sqlzoo.net/wiki/More_details_about_the_database. 这些都搞明白，SQL就容易了123456789select name from actor where id in (select actorid from casting where ord=1 group by actorid having count(distinct movieid)&gt;=30)order by name desc-- 或者这样select a.name from actor a join casting b on a.id=b.actorid where b.ord=1 group by a.namehaving count(distinct b.movieid)&gt;=30 上面的order by可以不要，好辣，今天问题整理到这里。 update at 2017-10-121. Workbench、Navicat是干嘛的，和SQL有啥关系？Workbench和Navicat都是数据库管理工具，让我们可以方便快捷的管理和使用数据库。类似的工具有很多，免费的、收费的好多好多，就好像共享单车，ofo、mobike、小蓝单车、小鸣单车……用哪个都可以，看我们的选择了。 SQL呢？是一种查询语言，虽然和写代码编程有关，但是不用怕，记住九九乘法表，后面就是活学活用了。我们为了和数据库成为好朋友，得多和他聊天吧，但我们和他不是一个星球的，那咋办？巧了，他听得懂SQL，那我们学下SQL，就可以愉快的玩耍了。这里呢，我们还得注意一下，目前市场上在使用的数据库有很多， SQL是一套标准，其他每个数据库都会实现规定的基本功能，然后拓展些自己的语法，所以很不幸，换一个数据库，我们的SQL不一定完全跑的通。但是，只要我们掌握了标准的SQL语法然后针对不同的数据库，关注下特殊语法就可以了（会骑ofo，换了Mobike就不会骑了吗？） 综上所述，我们是通过管理工具（Workbench、Navicat……）去使用和管理数据库（MySQL…..），在管理工具中，我们使用SQL来和数据库互动。 2. Workbench的使用参考：MySQL-Workbench使用 3. sqlzoo第九题题目地址: sqlzoo首先是关于all的疑问 关于all可以参考这里先简单了解下：MySQL-子查询的使用 SQL这样写是没问题的1234567891011121314151617SELECT name, continent, population FROM world x -- 返回符合条件的所有数据WHERE -- 使用all，判断该州的所有人口数是否都 &lt;= 25000000 25000000 &gt;= ALL( -- 我们使用子查询，获取每一个州的所有人口数 SELECT population FROM world y WHERE x.continent =y.continent ) 疑问1： 能不能把这个all放到后面像这样 答案是不行的，这个不符合all的使用语法，具体语法参考官方文档：https://dev.mysql.com/doc/refman/5.7/en/all-subqueries.html all一定要放在一个比较运算符的后面才行，所以，替换是不符合规范的，会报错 疑问2：为什么下面的写法不对123456789101112131415SELECT name, continent, population FROM world WHERE 25000000 &gt;= ALL( SELECT MAX(population) FROM world GROUP BY continent ) 我们分析下题目，他是说要查询出州下面每个城市的人口数都小于等于25000000的记录上面我们就是使用all来实现去判断每个州下面的所有城市的人口数都&lt;= 25000000，现在呢，不想去判断每个城市的人口数了，我只要拿出该州下面最大的人口数，然后判断这个最大值是不是&lt;= 25000000就可以了。思路是对的，但是SQL稍微有些问题12345678910111213141516171819-- 要么返回所有记录，要么没有返回记录SELECT name, continent, population FROM world WHERE -- 因为主表world和子查询没有关联关系，所以这里的all就变成了“每个州的最大人口数是不是都 &lt;= 25000000 -- 也就是说，下面子查询的返回值，如果都是 &lt;= 25000000，那就会返回world表的所有数据；如果有一个州的人口数 &gt;25000000，那就没有记录返回 25000000 &gt;= ALL( -- 获取每个州，最大的人口数 SELECT MAX(population) FROM world GROUP BY continent ) 所以，我们要做的改动，就是让主表和子查询有关联关系，即判断每一个州的最大值 不使用all也可以，12345-- 查询符合条件的州即可select name,continent,population from world where continent in ( -- 使用having直接过滤，判断每个州的最大人口是否&lt;= 25000000 select continent from world group by continent having max(population) &lt;= 25000000) 好了，这个问题，也写到这，大家可以看看还有没有别的写法。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-Workbench使用]]></title>
    <url>%2F2017%2F10%2F12%2Fmysql-handbook-11%2F</url>
    <content type="text"><![CDATA[MySQLWorkbench使用 简单介绍下Workbench的使用Workbench是MySQL官方提供的一个可视化管理工具，跨多个平台而且免费的，详情参考官网。我们从下载地址下载，安装就行了 安装可以单独下载，也可以使用提供的一个管理工具统一下载管理，管理工具提供了整个MySQL所有相关组件的统一管理维护，也挺方便。 我们这里就是用独立安装包了，下载完之后安装，可能会失败，因为他有2个依赖要先安装，失败的话，会提示你缺少的依赖。 这里有个小坑，这里说的是.NET Framework 4.5，但实际上需要的是4.5.2貌似不太一样，所以刚刚一直安装失败，注意下就行了.NET Framework 4.5.2 安装过程就是一路Next就行了，没啥特殊的。安装完之后，我们在桌面或者菜单找到这个 主界面应该是这样： 新建连接我们点击加号，新增一个连接 在弹出的界面上，输入数据库的连接信息，就可以了，密码下图所示的地方输入 输入完成后，我们测试下连接是否正常 都正常后，我们单击OK就好了 返回主界面后，可以看到我们刚刚新建的连接，我们双击就可以打开这个连接，开始我们的SQL学习了 功能介绍我们目前主要关心的是SQL的学习，所以那些管理功能可以先忽略掉我们主要关注“当前可以使用的数据库”和SQL的编辑界面就OK了 我们以demo库为例 这里就是demo库中的表，数据库中的数据是以表的形式保存的比如，我们看看学生表的表结构信息，单击这个表旁边的”i”图标 右边会显示一个表的信息界面，字段啊，索引啊，触发器之类的，我们这里就看看DDL 这里展示的就是t_student表的表结构信息 SQL练习下面，我们看看，怎样去练习SQL我们回到刚刚的query界面 首先，我们要选择我们使用的数据库1use demo; 写好SQL，我们单击那个像闪电一样的图标执行，下面的输出窗口会显示执行结果，绿色的对号表示正确执行 然后，就可以各种SQL开始练习了1select *from t_student; 后面，我们就可以参考前面的博客，开始学习吧MySQL-基本语法介绍]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tableau实例-帕累托图]]></title>
    <url>%2F2017%2F09%2F20%2FTableau-handbook-06%2F</url>
    <content type="text"><![CDATA[Tableau实例帕累托图 前面，我们了解了《帕累托的故事》 和 二八定律与长尾理论，这里，我们学习下，在Tableau中，如何适用Tableau来绘制帕累托图。 准备数据源：官方数据源“示例-超市”因为，帕累托分布，主要是20%的商品可以产生80%的价值，所以，我们可以使用示例数据中的订单数据，来看看订单的销量是否符合帕累托分布。 初级-每个类别的销售额分布我们先来研究下，看产品的类别销售额是否符合帕累托分布，帕累托图有一个柱形图，有一个折线图，柱形图，表示每个类别的销售额，而折线图表示每个类别的销售额占比 柱形图就直接使用子类别和销售额就行了 然后，我们实现折线图在行中，再拖一个销售额 然后修改标记的类型，改为“线” 效果图 但这个折线图，并不是我们要的，我们想要的是一个占总额的百分比我们要修改第2个“总计销售额”，添加表计算 我们主要计算类型，选择汇总-总计，并且勾选“添加辅助计算”，选择“总额百分比” 我们把修改好的“总额百分比”，拖到“标签”上，然后效果如图所示 顺手，我们修改下折线图的颜色，因为后面，我们还要合并呢 我们再折线图的y轴上，右键，单击选择“双轴” 他自动会变成这样，我们得手动改成柱形图和折线图（颜色也白改了…） 调整完之后，就是这样啦， 我们调整下柱形图的y轴刻度，让他更像些 进阶-产品占比与销售额占比上面，我们做的是，哪些类别的销售额占80%，这里，我们想根据产品来看，并且想知道产品的百分之多少占了80%的销售额 和上面的步骤类似，这里，我们使用产品名称，效果如图 但是这里，产品实在是太多了，我们想让x轴更简洁一些，就显示百分比好了 未完待续……]]></content>
      <categories>
        <category>Tableau</category>
      </categories>
      <tags>
        <tag>数据可视化</tag>
        <tag>Tableau</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二八定律与长尾理论]]></title>
    <url>%2F2017%2F09%2F18%2Fanalysis-method-01%2F</url>
    <content type="text"><![CDATA[二八定律前面，我们整理了《帕累托的故事》，里面有说过“帕累托法则”，就是这个二八定律，简单说来，就是 世界上20%的人占有80%的财富，即财富的分布式是不平衡的 这里理论不单单应用在经济学领域，其他领域也一样适用。比如：在零售或销售行业，80%的利润可能来自20%的客户，所以运用该分析方法，就可以专注于维护这20%的客户，而不是将主要精力放在那80%的客户身上，因为他们只产生了20%的利润。通过80/20分析方法，可以有效的找到影响利润的主要因素 二八定律应用场景这里列举几个广泛使用的场景 在管理学中，企业80%的利润来自20%的项目或客户 心理学中，20%的人身上集中了80%的智慧，他们一出生就鹤立鸡群 20%的有目标，80%的人爱瞎想 20%的人把握机会，80%错失机会 20%的人会坚持，80%的人会放弃 长尾理论长尾理论的关注点和二八定律不同，上面是说要关注产生重大影响的20%，而长尾理论则提倡关注剩下的那80%，在一些互联网公司，如果那80%的尾巴足够长，就可能会产生超过前面20%的那部分产生的利润。 在互联网时代，关注成本或是生产、运输成本都大大的降低，这长尾巴就产生了优势。该理论在Google、亚马逊、沃尔玛，都用数据证实过，互联网行业尤其适用。 小结二八定律的话，告诉我们关注可以产生重大影响的事情，抓住主要的；长尾理论的应用应该还是处在互联网公司中，等看看那本书，详细了解下。有的时候，换个角度看问题，的确可以发现不一样的世界。 参考资料资料来源“百度百科”，“维基百科”]]></content>
      <categories>
        <category>数据分析思维</category>
      </categories>
      <tags>
        <tag>数据分析思维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[帕累托的故事]]></title>
    <url>%2F2017%2F09%2F18%2Fjotting-01%2F</url>
    <content type="text"><![CDATA[周末听Tableau的一个培训，遇到一个帕累托图，就是关于二八定律的那个帕累托，就想着，他到底是干嘛的呢，然后，就有了这篇《帕累托的故事》，让我们开始了解一下这个经济学家。 帕累托简介 维弗雷多·帕累托（Vilfredo Pareto ，1848年7月15日—1923年8月19日），意大利经济学家、社会学家，洛桑学派的主要代表之一。生于巴黎，曾就读于意大利都灵大学，后来任瑞士洛桑大学教授。– 百度百科 哦，他是一个经济学家、社会学家，怪不得会提出“20%的人掌握着百分之80%的财富”，以前，是先知道这句话，然后才知道帕累托的大名的。 洛桑学派洛桑学派主要代表之一，什么是洛桑学派，干嘛的呢？估计是因为在洛桑大学任教，所以统称为“洛桑学派”吧 “洛桑学派”指的是，以法国人瓦尔拉斯和意大利人帕累托为中心的新古典思想流派的一支。洛桑学派的主要特点在于它对一般均衡理论的推进，从而在广度上和深度上都扩大了新古典研究方法在经济学上的适用性。洛桑学派也被称为“数理学派”（因为它们强调数理的解释说明）或者“意大利学派”（因为在早期的队伍里有很多意大利人）。– 百度百科 数理学派，难道就是用数学和统计学来证明一些经济学理论？感觉好像是这样，他们一定是实战派，用数理方法来验证实际问题，厉害。“对一般均衡理论的推进”，看看这个一般均衡理论是啥？ 一般均衡理论 一般均衡理论（General Equilibrium Theory） 是1874年法国经济学家瓦尔拉斯在他的《纯粹经济学要义》中创立的。瓦尔拉斯认为，整个经济体系处于均衡状态时，所有消费品和生产要素的价格将有一个确定的均衡值，它们的产出和供给，将有一个确定的均衡量。他还认为在“完全竞争”的均衡条件下，出售一切生产要素的总收入和出售一切消费品的总收入必将相等。该理论的实质是说明资本主义经济可以处于稳定的均衡状态。在资本主义经济中，消费者可以获得最大效用，企业家可以获得最大利润，生产要素的所有者可以得到最大报酬。– 百度百科 有点儿不明觉厉，反正是一个经济上了一个什么理论，果然是通过各种数理知识来解决真正改的经济学问题，大师就是大师，膜拜。帕累托基于这个思路，进一步延伸和拓展了这一理论。 主要成就下面，我们来了解下他的主要成就，仰望下大师 帕累托法则这就是前面说到的那个80/20法则、二八定律 帕累托法则是指在任何大系统中，约80%的结果是由该系统中约20%的变量产生的。 帕累托主要是研究经济学和社会学的，在1906年，他观察了意大利的财富分配情况，发现80%的财富掌握在20%的人手里，好残酷，但现实就是这样，我一定是那个80%里面的80%的里面的80%…… 抽象一下，就变成这样： 在任何特定群体中，重要的因子通常只占少数，而不重要的因子则占多数，因此只要能控制具有重要性的少数因子即能控制全局。 一开始这一理论只限定于经济领域，后来逐步推广到各个领域，而且深为人们认同。根据帕累托法则，延伸出很多的有趣的结论，像： 世界上80%的资源是由15%的人口耗尽的 企业中80%的利润来自于20%的项目或客户 时间管理中，20%的项目或付出可以得来80%的成绩 我们可以从不同的角度去理解和应用这一原则，比如：时间管理，财务管理、员工管理、市场营销等等，都会有意想不到的收获。 帕累托最优帕累托最优(Pareto Optimality)也称为帕累托效率（Pareto Efficiency）、帕雷托最佳配置，是博弈论中的重要概念，并且在经济学， 工程学和社会科学中有着广泛的应用。 帕累托最优是指资源分配的一种理想状态，假定固有的一群人和可分配的资源，从一种分配状态到另一种状态的变化中，在没有使任何人境况变坏的前提下，使得至少一个人变得更好，这就是帕累托改进或帕累托最优化。帕累托最优的状态就是不可能再有更多的帕累托改进的余地；换句话说，帕累托改进是达到帕累托最优的路径和方法。帕累托最优是公平与效率的“理想王国”。 这个也很有意思，直白点儿说就是，你想要过的更好，就一定会有一个人过的更差，你想要挣更多的钱，就一定会有一个人挣更少的钱，又是一个残酷的现实啊。上面理解错了，帕累托最优说的是，在你挣了更多的钱的同时，也没有人因此少挣钱，强调的是不能减少任何一个人的福利。维基百科上有一个例子：说是有假设一个社会上，只有一个百万富翁和一个快饿死的乞丐，如果富翁拿出自己财富的万分之一，就可以让乞丐免于死亡，但是这样就损害了富翁的福利（假设乞丐没有可以回报富翁的服务或资源），这种财富转移，并不是“帕累托改进”，而这种社会被称为帕累托最优的。帕累托最优，说的应该是一种理想社会，社会资源的分配已经非常合理，不会损害任何人的福利，但是这种社会里依然存在着富有与贫穷，依然存在贫富差距。 帕累托图帕累托图又叫排列图、主次图,是按照发生频率大小顺序绘制的直方图，表示有多少结果是由已确认类型或范畴的原因所造成。帕累托图在项目管理中主要用来找出产生大多数问题的关键原因，用来解决大多数问题。通过这个图，就可以看出80%的财富掌握在20%的人手里。 小结上面的内容有些比较难懂，需要慢慢的消化，后续等了解的更多了，会再补充。 参考资料这里整理的时候，综合了“百度百科”，“维基百科”等等网上资料]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-自增列]]></title>
    <url>%2F2017%2F09%2F12%2Fmysql-handbook-10%2F</url>
    <content type="text"><![CDATA[MySQL自增列 什么是自增列自增列就是一个自动增长的列，他没有什么业务含义，一般可能用来做主键，作为唯一标识。自增列一般是一个整数，相比其他的UUID占用的存储更少，网络资源占用也少。如果考虑其他因素的话，UUID使用也很多。实际应用还要考虑很多问题，不能单纯的使用 自增列是使用我们可以再create table的时候，就定义好自增列我们使用关键字 auto_increment 来指定。12345mysql&gt; create table t_book_1( -&gt; id int auto_increment, -&gt; f_name varchar(10), -&gt; primary key(id) -&gt; ); 这里的话，一定要让自增列是主键，不然会报错 123456mysql&gt; create table t_book_2( -&gt; id int auto_increment, -&gt; f_name varchar(10), -&gt; f_other varchar(10) -&gt; );ERROR 1075 (42000): Incorrect table definition; there can be only one auto column and it must be defined as a key 另一种方式就是先建表，后面再修改为auto_increment12345678910111213mysql&gt; create table t_book_2( -&gt; id int , -&gt; f_name varchar(10) -&gt; );Query OK, 0 rows affected (0.01 sec)mysql&gt; alter table t_book_2 add primary key(id);Query OK, 0 rows affected (0.02 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; alter table t_book_2 modify id int auto_increment;Query OK, 0 rows affected (0.02 sec)Records: 0 Duplicates: 0 Warnings: 0 重置自增列一般自增列，都是自动赋值的，我们先插入几条记录试试123456789101112131415161718mysql&gt; insert into t_book_2(f_name) values('aa');Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t_book_2(f_name) values('bb');Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t_book_2(f_name) values('cc');Query OK, 1 row affected (0.01 sec)mysql&gt; select *from t_book_2;+----+--------+| id | f_name |+----+--------+| 1 | aa || 2 | bb || 3 | cc |+----+--------+3 rows in set (0.00 sec) 这时候，自增列已经到3了，如果我们删除了一条数据，123456789101112131415mysql&gt; delete from t_book_2 where id=3;Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t_book_2(f_name) values('dd');Query OK, 1 row affected (0.00 sec)mysql&gt; select *from t_book_2;+----+--------+| id | f_name |+----+--------+| 1 | aa || 2 | bb || 4 | dd |+----+--------+3 rows in set (0.00 sec) 序列并不会重新从3开始，如果我们清空，也是一样的。那怎样可以重置自增列呢？ 使用truncate自动重置12345678910111213mysql&gt; truncate table t_book_2;Query OK, 0 rows affected (0.01 sec)mysql&gt; insert into t_book_2(f_name) values('ee');Query OK, 1 row affected (0.00 sec)mysql&gt; select *from t_book_2;+----+--------+| id | f_name |+----+--------+| 1 | ee |+----+--------+1 row in set (0.00 sec) 手工修改我们可以使用命令123-- 我们可以修改为我们想要的值，但如果表中有数据的话，如果修改-- 的值比当前最大值小，则会重置为最大值+1alter table table_name auto_increment = 1; 12345678910111213141516171819mysql&gt; delete from t_book_2 where id=3;Query OK, 1 row affected (0.00 sec)mysql&gt; alter table t_book_2 auto_increment=3;Query OK, 0 rows affected (0.00 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; insert into t_book_2(f_name) values('hh');Query OK, 1 row affected (0.00 sec)mysql&gt; select *from t_book_2;+----+--------+| id | f_name |+----+--------+| 1 | ee || 2 | ff || 3 | hh |+----+--------+3 rows in set (0.00 sec) 直接drop，重新create这个方法就不练习了， 手动给自增列赋值在insert的时候，手动给自增列赋值，也是可以的，手动赋值后，我们再插入的时候，就会使用当前自增序列的最大值1234567891011121314151617mysql&gt; insert into t_book_2(id,f_name) values(9,'xx');Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t_book_2(f_name) values('yy');Query OK, 1 row affected (0.01 sec)mysql&gt; select *from t_book_2;+----+--------+| id | f_name |+----+--------+| 1 | ee || 2 | ff || 3 | hh || 9 | xx || 10 | yy |+----+--------+5 rows in set (0.00 sec) 附录发现一篇好文章，讲的还不错：数据库自增列]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-聚合函数]]></title>
    <url>%2F2017%2F09%2F11%2Fmysql-handbook-09%2F</url>
    <content type="text"><![CDATA[MySQL聚合函数 聚合函数也是函数的一种，比较常用，这里我们就单独拿出来介绍下。聚合函数一般配合group by来使用，经常是用来对数据集中的数值求和、平均值啊这里类的。 聚合函数的默认特性 忽略NULL值 如果没有匹配的记录，返回NULL 如果没有使用group by，则默认对所有字段进行group by 常用聚合函数这里的测试数据依然使用前面的数据，可以参考前面的文章。 count统计结果集的数量,没有结果时，返回012345678910111213-- 我们以学生表为例，来统计每个班级的学生人数select c_id,count(1), count(s_id), -- 这里学生ID是唯一的，所以是否使用distinct是一样的 count(distinct s_id), -- 统计班级的个数 count(distinct c_id)from t_student group by c_id; 当我们只使用count，不使用group by的时候，相当于对所有字段进行group by12345select count(1)from t_student; 这里，我们再来看下count对于null值得处理1234567891011121314select -- count(*),是包括null值的 count(*), -- count(1),也包括null值 count(1), -- 指定字段的时候，是不包括null值的 count(id), -- 同样也不包括null值 count(DISTINCT id)from ( select 1 as id union select NULL union select 2)x avg、sum计算结果集的平局值和结果集的累加和12345678910-- 统计每个学生的平均分和总分select s_id, avg(score), sum(score) from t_scoregroup by s_id; 我们再来看看avg和sum对null值的处理1234567891011121314select -- count指定字段，是不包括null值的 count(score), -- avg也是不包括null值的 avg(score), -- 如果想要统计score的记录，需要使用ifnull进行判断 avg(IFNULL(score,0)), -- 求和 sum(score)from ( select 10 as score union select NULL union select 20)x; min、max统计结果集的最小值和最大值123456789101112select s_id, -- 最低分 min(score), -- 最高分 max(score)from t_scoregroup by s_id; 如果没有匹配的记录，则返回null；如果结果集中有null值，会忽略null123456789select max(score), min(score)from ( select 10 as score union select NULL union select 20)x; group_concatgroup_concat会将函数聚合后的所有值以逗号分隔，以字符串展现1234GROUP_CONCAT([DISTINCT] expr [,expr ...] [ORDER BY &#123;unsigned_integer | col_name | expr&#125; [ASC | DESC] [,col_name ...]] [SEPARATOR str_val]) 123456789select s_id, -- 将该学生所有的成绩以逗号分隔显示 group_concat(score)from t_scoregroup by s_id; having在聚合函数的使用过程中，通常还会使用having来对聚合后的数据进行过滤12345678910select s_id, sum(score) sum_scorefrom t_scoregroup by s_id-- 总分大于150分having sum_score &gt; 150]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-子查询的使用]]></title>
    <url>%2F2017%2F09%2F11%2Fmysql-handbook-08%2F</url>
    <content type="text"><![CDATA[MySQL变量的使用 什么是子查询子查询是将一个 SELECT 语句的查询结果作为中间结果，供另一个 SQL 语句调用。像这样：12-- 我们将学生表中的所有班级ID当做中间结果select *from t_class where c_id in (select distinct c_id from t_student); 常用比较符子查询最常用的用法： non_subquery_operand comparison_operator (subquery)其中操作符通常为= &gt; &lt; &gt;= &lt;= &lt;&gt; != &lt;=&gt; 其他的都不说了，这里说下这个&lt;=&gt;,以前还真没用过&lt;=&gt;和=比较类似，也是判断是否相等，相等返回1，不相等返回2123456789101112131415mysql&gt; select 1&lt;=&gt;1,1&lt;=&gt;2;+-------+-------+| 1&lt;=&gt;1 | 1&lt;=&gt;2 |+-------+-------+| 1 | 0 |+-------+-------+1 row in setmysql&gt; select 1=1,1=2;+-----+-----+| 1=1 | 1=2 |+-----+-----+| 1 | 0 |+-----+-----+1 row in set 和=不一样的地方，是对NULL的支持，用&lt;=&gt;可以判断是否为null，而等号则是出现null，结果就为null1234567mysql&gt; select 1&lt;=&gt;null,null&lt;=&gt;null,1=null,null=null;+----------+-------------+--------+-----------+| 1&lt;=&gt;null | null&lt;=&gt;null | 1=null | null=null |+----------+-------------+--------+-----------+| 0 | 1 | NULL | NULL |+----------+-------------+--------+-----------+1 row in set any、in、some在子查询中，in平时用的比较多，这个any、some，这里简单说下any和some operand comparison_operator ANY (subquery)operand comparison_operator SOME (subquery)可以为 = > < >= all ()：表示大于所有值，即&gt;子查询中最大值&lt; all() : 表示小于所有值，即&lt; 子查询中的最小值= all(): 返回单个值时和=一样，返回多个值时貌似没啥用&lt;&gt; all(): 和not in 一样 1234select *from t_student where s_id &gt; all( select s_id from t_student where s_id in (105,109)); 标量子查询这种情况下，子查询返回单个值，可以在任何地方使用它。1234567891011121314select c_id, c_name, (select max(s_id) from t_student) as max_s_idfrom t_class;select *from t_class where c_id = (select max(c_id) from t_class); 行子查询上面我们介绍的子查询，都是返回1列多行，行子查询的话，是返回1行多列123-- 查询一班所有男生select *from t_studentwhere (c_id,s_gender) = (901,0); 这里也可以返回多行多列（也叫做表子查询）12select *from t_studentwhere (c_id,s_gender) in (select 901,0 union select 902,0); 参考资料官方文档：https://dev.mysql.com/doc/refman/5.7/en/subqueries.html]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小白学习Tableau-使用计算进行聚焦]]></title>
    <url>%2F2017%2F09%2F10%2FTableau-handbook-05%2F</url>
    <content type="text"><![CDATA[Tableau实例 什么是聚焦聚焦是一种技术，用于根据某个度量的值显示离散阈值。聚焦计算其实是一种可产生离散度量的特殊计算。 其实就是新构造了一个维度，这个维度将度量值进行分段，类似于年龄段这样的维度。这个应用场景很多，比如我们有一个达标值，想要哪些数据达标，哪些未达标。 实例数据源使用Tableau中自带的“示例-超市”，其中的订单表 交叉列表我们来看每个地区，每个子类别的数量 现在，我们想要实现这样的功能，就是我有一个达标值500，只有数量达到500，才达标，怎样展示最好呢？ 创建计算字段我们来创建一个计算字段，如果数量大于500则达标，否则不达标 然后，把他拖到颜色上 到这里，我们就可以看到，已经按照Good、Bad来区分颜色，达标、不达标 使用参数上面，我们已经达成了目的，思考下，这个达标值的问题，这个月是500，下个月可能会浮动为600或者400，但我们在代码中写死了500，不够灵活，我们就可以使用参数来控制。 然后，我们修改上面的计算字段 好了，最后，我们就可以动态的修改这个达标值了 参考官方文档：示例 — 使用计算进行聚焦]]></content>
      <categories>
        <category>Tableau</category>
      </categories>
      <tags>
        <tag>数据可视化</tag>
        <tag>Tableau</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-变量的使用]]></title>
    <url>%2F2017%2F09%2F10%2Fmysql-handbook-07%2F</url>
    <content type="text"><![CDATA[MySQL变量的使用 这里，我们简单介绍下MySQL中变量的使用 变量的作用域在MySQL中，变量的作用域主要有全局变量（Global）和会话级变量（Session）全局变量会影响当前server上的所有操作，而会话级变量则只会影响当前用户的会话，server上其他用户的会话不受影响 系统变量系统变量主要涉及MySQL Server的一些配置参数，系统变量，一般会在配置文件中进行配置，或者在使用命令启动MySQL的使用去指定。当然，在MySQL启动后，也可以动态的去修改系统变量（有些变量是不能动态修改的，可以修改的变量请参照官方文档）。 那我们怎样动态的修改系统变量呢？可以使用 SET GLOBAL or SET SESSION@@global. | @@session. 123456SET GLOBAL max_connections = 1000;SET @@global.max_connections = 1000;SET SESSION sql_mode = 'TRADITIONAL';SET @@session.sql_mode = 'TRADITIONAL';SET @@sql_mode = 'TRADITIONAL'; 查看当前系统变量的值1234567SHOW VARIABLES;SHOW VARIABLES LIKE 'max_join_size';SHOW SESSION VARIABLES LIKE 'max_join_size';SHOW VARIABLES LIKE '%size%';SHOW GLOBAL VARIABLES LIKE '%size%'; 用户自定义变量我们可以定义自己的变量，变量名为 @var_name，用户自定义变量都是会话级的 SET @var_name = expr [, @var_name = expr] …For SET, either = or := can be used as the assignment operator. 我们也可以使用select来使用变量，或者给变量赋值，但必须使用:=12345678910mysql&gt; SET @t1=1, @t2=2, @t3:=4;Query OK, 0 rows affectedmysql&gt; SELECT @t1, @t2, @t3, @t4 := @t1+@t2+@t3;+-----+-----+-----+--------------------+| @t1 | @t2 | @t3 | @t4 := @t1+@t2+@t3 |+-----+-----+-----+--------------------+| 1 | 2 | 4 | 7 |+-----+-----+-----+--------------------+1 row in set 在使用自定义变量的时候，会有一个顺序的问题，12set @p_rank:= 0;select *,@p_rank,@p_rank:=@p_rank+1 from t_student; 这样虽然可以得到我们想要的结果，但是官方并不推荐 自定义变量，是在结果返回到客户端时，才进行处理的，所以我们HAVING, GROUP BY, or ORDER BY中使用的时候，并没有效果。 参考资料官方文档：user-variablesserver-system-variables]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-常用函数]]></title>
    <url>%2F2017%2F09%2F10%2Fmysql-handbook-06%2F</url>
    <content type="text"><![CDATA[MySQL常用函数 这里介绍下，MySQL中常用的函数，函数有太多太多，不一定都需要记住，只需要有个印象，需要的时候去文档中找一下，记住一些常用的就好了 数学函数abs(x)返回x的绝对值1select abs(-10),abs(10) ceil(x)、ceiling(x)向上取整,比该值大的第一个整数1select CEIL(9.3),CEIL(9.5),CEIL(9.6),CEIL(-9.5),CEIL(-9.1) floor(x)向下取整，比该值小的第一个整数 1select FLOOR(9.3),FLOOR(9.5),FLOOR(9.6),FLOOR(-9.5),FLOOR(-9.1) round(x),round(x,y)四舍五入，round(x)，最近的一个整数，round(x,y)，这个y可以指定精度 1select ROUND(9.3),ROUND(9.6),ROUND(9.378,2),ROUND(9.455,2) rand(),rand(x)rand() 返回0~1之间的随机数rand(x) 返回0~1之间的随机数，如果x值相等，则返回值相等 1select RAND(),RAND(),RAND(10),RAND(10) 字符串函数获取字符串长度 length(str)，返回str的长度,这里要注意下中文，占3个长度,这里的长度单位是bytes1234567mysql&gt; select length('abc'),length('中国'),length('hi中国');+---------------+----------------+------------------+| length('abc') | length('中国') | length('hi中国') |+---------------+----------------+------------------+| 3 | 6 | 8 |+---------------+----------------+------------------+1 row in set CHAR_LENGTH(str)，返回str的长度，中文和英文一样，占1个字符，这里长度单位是字符1234567mysql&gt; select char_length('abc'),char_length('中国'),char_length('hi中国');+--------------------+---------------------+-----------------------+| char_length('abc') | char_length('中国') | char_length('hi中国') |+--------------------+---------------------+-----------------------+| 3 | 2 | 4 |+--------------------+---------------------+-----------------------+1 row in set 字符串拼接CONCAT(str1,str2,…)，将str1，str2拼接在一起12345678mysql&gt; select concat('h','e','gogo','中国');+-------------------------------+| concat('h','e','gogo','中国') |+-------------------------------+| hegogo中国 |+-------------------------------+1 row in set 这里要注意下NULL，如果其中有参数为NULL，则结果为NULL1234567mysql&gt; select concat('h','e','gogo',NULL,'中国');+------------------------------------+| concat('h','e','gogo',NULL,'中国') |+------------------------------------+| NULL |+------------------------------------+1 row in set CONCAT_WS(separator,str1,str2,…)，使用指定的separator进行拼接1234567mysql&gt; select concat_ws('^','e','gogo',NULL,'中国');+---------------------------------------+| concat_ws('^','e','gogo',NULL,'中国') |+---------------------------------------+| e^gogo^中国 |+---------------------------------------+1 row in set 这里是如果有NULL，对结果是没有影响的，会直接忽略NULL值 剔除空格或指定字符剔除字符串左右的空格ltrim(str),剔除左侧空格rtrim(str),剔除右侧空格TRIM([{BOTH | LEADING | TRAILING} [remstr] FROM] str), TRIM([remstr FROM] str)trim可以使用参数来控制剔除空格或者是指定的remstr，默认是空格123456789101112131415mysql&gt; select concat(ltrim(' hi '),'oo'),concat(rtrim(' hi '),'oo'),concat(trim(' hi '),'oo');+--------------------------------+--------------------------------+-------------------------------+| concat(ltrim(' hi '),'oo') | concat(rtrim(' hi '),'oo') | concat(trim(' hi '),'oo') |+--------------------------------+--------------------------------+-------------------------------+| hi oo | hioo | hioo |+--------------------------------+--------------------------------+-------------------------------+1 row in setmysql&gt; select trim('a' from 'aabbbccaa'),trim(leading 'a' from 'aabbbccaa'),trim(trailing 'a' from 'aabbbccaa');+----------------------------+------------------------------------+-------------------------------------+| trim('a' from 'aabbbccaa') | trim(leading 'a' from 'aabbbccaa') | trim(trailing 'a' from 'aabbbccaa') |+----------------------------+------------------------------------+-------------------------------------+| bbbcc | bbbccaa | aabbbcc |+----------------------------+------------------------------------+-------------------------------------+1 row in set 字符串填充LPAD(str,len,padstr)，左侧填充RPAD(str,len,padstr)，右侧填充len是指定str的长度，如果不够，则使用padstr填充，如果超了，则进行截取1234567mysql&gt; select lpad('hi',6,'@'),lpad('higogo',4,'@'),rpad('hi',6,'@'),rpad('higogo',4,'@');+------------------+----------------------+------------------+----------------------+| lpad('hi',6,'@') | lpad('higogo',4,'@') | rpad('hi',6,'@') | rpad('higogo',4,'@') |+------------------+----------------------+------------------+----------------------+| @@@@hi | higo | hi@@@@ | higo |+------------------+----------------------+------------------+----------------------+1 row in set 字符串截取LEFT(str,len)，从左侧开始截取len个字符RIGHT(str,len)，从右侧截取len个字符SUBSTR(str,pos), SUBSTR(str FROM pos), SUBSTR(str,pos,len), SUBSTR(str FROM pos FOR len)，从指定pos开始截取len个字符1234567891011121314151617181920212223mysql&gt; select left('hello',1),left('hello',3),right('hello',3);+-----------------+-----------------+------------------+| left('hello',1) | left('hello',3) | right('hello',3) |+-----------------+-----------------+------------------+| h | hel | llo |+-----------------+-----------------+------------------+1 row in setmysql&gt; select substr('hello',1),substr('hello',2),substr('hello' from 2);+-------------------+-------------------+------------------------+| substr('hello',1) | substr('hello',2) | substr('hello' from 2) |+-------------------+-------------------+------------------------+| hello | ello | ello |+-------------------+-------------------+------------------------+1 row in setmysql&gt; select substr('hello',1,3),substr('hello',2,3),substr('hello' from 2 for 2);+---------------------+---------------------+------------------------------+| substr('hello',1,3) | substr('hello',2,3) | substr('hello' from 2 for 2) |+---------------------+---------------------+------------------------------+| hel | ell | el |+---------------------+---------------------+------------------------------+1 row in set 大小写转换LOWER(str) LCASE(str)，将str转为小写UPPER(str) UCASE(str)，将str转为大写1234567mysql&gt; select lower('AppLE'),lcase('AppLE'),upper('AppLE'),ucase('AppLE');+----------------+----------------+----------------+----------------+| lower('AppLE') | lcase('AppLE') | upper('AppLE') | ucase('AppLE') |+----------------+----------------+----------------+----------------+| apple | apple | APPLE | APPLE |+----------------+----------------+----------------+----------------+1 row in set 更多字符串函数参考官网： https://dev.mysql.com/doc/refman/5.7/en/string-functions.html 日期和函数获取当前日期、时间1SELECT CURRENT_DATE,CURRENT_DATE(),CURRENT_TIME,CURRENT_TIME(),CURRENT_TIMESTAMP(),NOW() 时间戳相关函数UNIX_TIMESTAMP() 返回当前时间的时间戳，UNIX_TIMESTAMP(x) 返回指定日期的时间戳FROM_UNIXTIME(x) 将时间戳转为日期FROM_UNIXTIME(x,y) 将时间戳转为指定格式的日期 1234567891011121314151617mysql&gt; select UNIX_TIMESTAMP(),UNIX_TIMESTAMP('2017-09-10');+------------------+------------------------------+| UNIX_TIMESTAMP() | UNIX_TIMESTAMP('2017-09-10') |+------------------+------------------------------+| 1505096065 | 1504972800 |+------------------+------------------------------+1 row in setmysql&gt; select FROM_UNIXTIME(1505096033),FROM_UNIXTIME(1504972800),FROM_UNIXTIME(1505096033,'%Y-%m-%d');+---------------------------+---------------------------+--------------------------------------+| FROM_UNIXTIME(1505096033) | FROM_UNIXTIME(1504972800) | FROM_UNIXTIME(1505096033,'%Y-%m-%d') |+---------------------------+---------------------------+--------------------------------------+| 2017-09-11 10:13:53 | 2017-09-10 00:00:00 | 2017-09-11 |+---------------------------+---------------------------+--------------------------------------+1 row in setmysql&gt; extractEXTRACT(unit FROM date)返回日期/时间的单独部分，比如年、月、日、小时、分钟等等date 参数是合法的日期表达式。unit 参数可以是下列的值： 123456789mysql&gt; select extract(YEAR FROM NOW()),extract(MONTH from now()),extract(HOUR from now());+--------------------------+---------------------------+--------------------------+| extract(YEAR FROM NOW()) | extract(MONTH from now()) | extract(HOUR from now()) |+--------------------------+---------------------------+--------------------------+| 2017 | 9 | 10 |+--------------------------+---------------------------+--------------------------+1 row in setmysql&gt; datediff、timediff、timestampdiffdatediff(expr1,expr2),获取2个日期相差的天数123456789mysql&gt; select datediff('2017-09-11 10:00:00','2017-09-08 00:00:00');+-------------------------------------------------------+| datediff('2017-09-11 10:00:00','2017-09-08 00:00:00') |+-------------------------------------------------------+| 3 |+-------------------------------------------------------+1 row in setmysql&gt; TIMEDIFF(expr1,expr2)，返回expr1-expr2的时间差123456789mysql&gt; select timediff('2017-09-11 10:00:00','2017-09-08 00:00:00');+-------------------------------------------------------+| timediff('2017-09-11 10:00:00','2017-09-08 00:00:00') |+-------------------------------------------------------+| 82:00:00 |+-------------------------------------------------------+1 row in setmysql&gt; TIMESTAMPDIFF(unit,datetime_expr1,datetime_expr2)返回指定unit的datetime_expr2 − datetime_expr1时间差unit可以是MICROSECOND (microseconds), SECOND, MINUTE, HOUR, DAY, WEEK, MONTH, QUARTER, or YEAR.123456789101112131415mysql&gt; select timestampdiff(DAY,'2017-09-11 10:00:00','2017-09-08 00:00:00');+----------------------------------------------------------------+| timestampdiff(DAY,'2017-09-11 10:00:00','2017-09-08 00:00:00') |+----------------------------------------------------------------+| -3 |+----------------------------------------------------------------+1 row in setmysql&gt; select timestampdiff(HOUR,'2017-09-11 10:00:00','2017-09-08 00:00:00');+-----------------------------------------------------------------+| timestampdiff(HOUR,'2017-09-11 10:00:00','2017-09-08 00:00:00') |+-----------------------------------------------------------------+| -82 |+-----------------------------------------------------------------+1 row in set 时间加减函数对日期进行加减操作，有很多方法可以使用，最简单的是直接使用interval1234567mysql&gt; select now(),now() + interval 3 DAY,now()+interval 1 Hour;+---------------------+------------------------+-----------------------+| now() | now() + interval 3 DAY | now()+interval 1 Hour |+---------------------+------------------------+-----------------------+| 2017-09-11 10:57:18 | 2017-09-14 10:57:18 | 2017-09-11 11:57:18 |+---------------------+------------------------+-----------------------+1 row in set 当然也可使用提供的函数 ADDDATE(date,INTERVAL expr unit), ADDDATE(expr,days)DATE_ADD(date,INTERVAL expr unit), DATE_SUB(date,INTERVAL expr unit) 1234567mysql&gt; select now(),date_add(now(),interval 1 Day),adddate(now(),interval 3 Hour);+---------------------+--------------------------------+--------------------------------+| now() | date_add(now(),interval 1 Day) | adddate(now(),interval 3 Hour) |+---------------------+--------------------------------+--------------------------------+| 2017-09-11 10:59:09 | 2017-09-12 10:59:09 | 2017-09-11 13:59:09 |+---------------------+--------------------------------+--------------------------------+1 row in set 更多日期、时间函数参考官方介绍: https://dev.mysql.com/doc/refman/5.7/en/date-and-time-functions.html 条件判断函数if if(expr,v1,v2)如果expr为真，则返回v1，为假，则返回v2123456789mysql&gt; select if(1&gt;0,'ok','no'),if(1=0,'ok','no');+-------------------+-------------------+| if(1&gt;0,'ok','no') | if(1=0,'ok','no') |+-------------------+-------------------+| ok | no |+-------------------+-------------------+1 row in setmysql&gt; ifnull ifnull(v1,v2)如果v1的值为null，则返回v2，如果v1不为null，则返回v1123456789mysql&gt; select ifnull(99,20),ifnull(NULL,99);+---------------+-----------------+| ifnull(99,20) | ifnull(NULL,99) |+---------------+-----------------+| 99 | 99 |+---------------+-----------------+1 row in setmysql&gt; case when 这里可以根据多个条件来判断，在不同的情况下，返回不同的值 123456789101112select s_id,s_name,s_gender, case when s_gender=0 then '男' when s_gender=1 then '女' end gender, s_birthday, s_hobby, c_idfrom t_student;]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-regexp]]></title>
    <url>%2F2017%2F09%2F09%2Fmysql-handbook-04%2F</url>
    <content type="text"><![CDATA[MySQL正则表达式 在前面，我们了解了like的使用，它可以做一些简单的匹配 在like中，我们使用%来代替任意个或多个字符，_表示任意单个字符 但在实际的应用场景中，我们可能还会需要更强大的匹配方式，比如“正则表达式”，这就需要使用regexp。 常用的正则表达式，更多的内容，大家可以百度下 我们就以前面的t_student表为例 查询学生姓路或乔的学生信息12345-- like select *from t_student where s_name like '路%' or s_name like '乔%'-- regexpselect *from t_student where s_name regexp '^(路|乔)' 查询名字是以美结尾的学生信息1select *from t_student where s_name regexp '美$' 查询学生爱好中，有吃肉的学生信息1select *from t_student where s_hobby regexp '吃肉' 查询学生姓乔或者名字中有美字的学生信息1select *from t_student where s_name regexp '(^乔)|美' 小结这里先整理这几个简单的例子，后续会再补充。 可以参考官方的文档练习：https://dev.mysql.com/doc/refman/5.7/en/regexp.html]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-分组排序]]></title>
    <url>%2F2017%2F09%2F09%2Fmysql-handbook-05%2F</url>
    <content type="text"><![CDATA[MySQL分组排序 使用过其他数据库的同学，一定知道那些好用的开窗函数，像什么rank() over(),sum() over() 等等，实现一些功能的时候很好用，但是呢，MySQL中并没有这些函数，那怎样实现这些功能呢？我们可以使用MySQL中的变量来实现这个功能。 我们先来看第一个类型的问题，关于排序的问题 排名就是按照指定的字段，给每一条记录分配一个行号（序号），作为他的排名在postgresql中可以使用像rank() over()这样的函数1234567select course_name, s_id, score, rank() over(order by score desc) rank_scorefrom t_score 但在MySQL中，就没这么方便了，我们需要使用变量，来模拟实现12345678910select course_name, s_id, score, -- 根据排序规则，每条记录增加1 @rn:=@rn+1 as rank_score -- 初始化变量@rn，从0开始from t_score cross join (select @rn:=0) xorder by score desc; 细心的同学，会发现，上面的排名会有些差别，以前2条记录为例，因为都是87，所以postgresql中排名都是1，而在MySQL中呢，依然是自增，一个1，一个2，那我们是否能让他也是2个1并列呢？ 当然可以，我们改一下SQL只要我们判断一下，当前记录的score和上一条记录的score是不是一样就可以了，那怎样才能获取上一条记录的score呢？就是增加一个变量来记录上一条记录的score12345678910111213select course_name, s_id, score, @pre pre_score, -- 判断当前score是否和上一条记录的score相等， -- 如果相等则使用和上一条一样的排名 if(@pre=score ,@rn:=@rn,@rn:=@rn+1) as rank_score, @pre:=score cur_score -- 使用pre来保存上一条记录的scorefrom t_score cross join (select @rn:=0,@pre:=null) xorder by score desc; 上面的排序还是有点儿问题，比如有2个第1之后，依然从第2开始，那我们能不能跳过直接从3开始呢？ 我们回想下，最开始他的排名其实就是3，只是我们上边把它变成了2，那我们再有一个和之前一样的变量就够了。 1234567891011select course_name, s_id, score, @pre pre_score, @rn_1:=@rn_1+1 rank_score_1, if(@pre=score ,@rn:=@rn,@rn:=@rn_1) as rank_score, @pre:=score cur_score from t_score cross join (select @rn:=0,@pre:=null,@rn_1:=0) xorder by score desc 我们使用@rn_1来正常记录排名，当当前记录和上一条记录的score不一样时，我们使用@rn_1的排名信息。 分组排序就是给每个分组中的数据，分别进行排名如果有类似rank() over()的函数12345678-- 在postgresql中select course_name, s_id, score, rank() over(partition by course_name order by score desc) rank_scorefrom t_score 但在MySQL中，就没这么方便了，我们需要使用变量，来模拟实现这里的思路和上面的思路一样，我们需要一个变量来保存上一条记录的课程名称，如果课程名称一样，则继续排名，如果不一样，则重新开始排名1234567891011set @rn:=0;set @pre_course:=null;select course_name, s_id, score, if(@pre_course=course_name,@rn:=@rn+1,@rn:=0), @pre_course:= course_name from t_score order by course_name,score desc;]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL笔试题-MySQL练习题]]></title>
    <url>%2F2017%2F09%2F09%2Fdata-analyst-interview-sql-04%2F</url>
    <content type="text"><![CDATA[SQL笔试题 下面的SQL基于MySQL 下面整理些MySQL学习过程中，基本的练习题，题目来源于网上及个人总结。 测试数据12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970create table t_student( s_id int comment '学生ID', s_name varchar(20) comment '学生姓名', s_gender int comment '学生性别 0-男,1-女', s_birthday date comment '出生日期', s_hobby varchar(100) comment '爱好', c_id int comment '班级ID') comment '学生表';create table t_class( c_id int comment '班级ID', c_name varchar(20) comment '班级名称') comment '班级表';create table t_score( sc_id int comment '成绩ID', s_id int comment '学生ID', course_name varchar(20) comment '课程名称', score numeric(10,0) comment '成绩' ) comment '成绩表';insert into t_class values(901,'一班');insert into t_class values(902,'二班');insert into t_class values(903,'三班');insert into t_class values(905,'五班');insert into t_student values(101,'路飞',0,'1990-01-26','吃肉,睡觉',901);insert into t_student values(102,'娜美',1,'1995-10-05','足球,篮球',901);insert into t_student values(103,'乔巴',0,'1992-08-11','唱歌,吃肉',901);insert into t_student values(104,'鸣人',0,'1991-03-29','拉面,忍术',901);insert into t_student values(105,'卡卡西',1,'1989-05-10','看书,吃肉',902);insert into t_student values(106,'乌索普',1,'1988-02-02','跳舞,篮球',902);insert into t_student values(107,'乔峰',0,'1990-12-12','跑步,羽毛球',902);insert into t_student values(108,'段誉',0,'1990-12-13','吃肉,加班',903);insert into t_student values(109,'虚竹',1,'1991-01-22','看电影,旅行',903);insert into t_student values(110,'杨过',0,'2000-03-04','旅行',903);insert into t_student values(111,'令狐冲',0,'1997-03-04','喝酒',904);insert into t_score values(1,101,'数学',39);insert into t_score values(2,102,'数学',20);insert into t_score values(3,103,'数学',54);insert into t_score values(4,104,'数学',38);insert into t_score values(5,105,'数学',70);insert into t_score values(6,106,'数学',15);insert into t_score values(7,107,'数学',75);insert into t_score values(8,108,'数学',84);insert into t_score values(9,109,'数学',87);insert into t_score values(10,110,'数学',67);insert into t_score values(11,101,'语文',73);insert into t_score values(12,102,'语文',71);insert into t_score values(13,103,'语文',82);insert into t_score values(14,104,'语文',83);insert into t_score values(15,105,'语文',36);insert into t_score values(16,106,'语文',87);insert into t_score values(17,107,'语文',74);insert into t_score values(18,108,'语文',19);insert into t_score values(19,109,'语文',29);insert into t_score values(20,110,'语文',26);insert into t_score values(21,101,'英语',55);insert into t_score values(22,102,'英语',24);insert into t_score values(23,103,'英语',38);insert into t_score values(24,104,'英语',82);insert into t_score values(25,105,'英语',12);insert into t_score values(26,106,'英语',15);insert into t_score values(27,107,'英语',50);insert into t_score values(28,108,'英语',68);insert into t_score values(29,109,'英语',77);insert into t_score values(30,110,'英语',19); 查询所有课程分数都大于50分的学生信息12345678910111213141516select *from t_student where s_id in ( -- 按学生ID分组，看每个学生的最低分数是否大于50分 select s_id from t_score group by s_id having min(score)&gt;=50); 换另一种方式，实现上一题123456789101112select *from t_student where s_id not in ( -- 查询分数小于50分的学生ID select s_id from t_score where score &lt; 50) and s_id in ( -- 加上这个判断，是因为有的学生没有考试成绩 select s_id from t_score); 查询最少有2门课程都&gt;=60分的学生信息123456789101112131415161718select *from t_student where s_id in ( select s_id from t_score where score&gt;=60 group by s_id having count(1)&gt;=2); 查询每个学生的个人信息及班级信息及所有科目分数，按照班级ID升序排列，课程分数降序排列12345678910111213141516select a.*,b.c_name,c.course_name,c.scorefrom t_student ajoin t_class b on b.c_id = a.c_idjoin t_score c on c.s_id = a.s_idorder by a.c_id asc, c.score desc ; 查询每个学生的学生ID、学生姓名、班级名称、总分、平均分，按照班级名称升序、总分降序排列123456789101112131415161718192021222324select a.s_id, a.s_name, b.c_name, sum(c.score) total_score, avg(c.score) avg_scorefrom t_student ajoin t_class b on b.c_id = a.c_idjoin t_score c on c.s_id = a.s_idgroup by a.s_id, a.s_name, b.c_nameorder by b.c_name, total_score desc; 上一题，加上平均分大于60分1234567891011121314151617181920212223242526select a.s_id, a.s_name, b.c_name, sum(c.score) total_score, avg(c.score) avg_scorefrom t_student ajoin t_class b on b.c_id = a.c_idjoin t_score c on c.s_id = a.s_idgroup by a.s_id, a.s_name, b.c_namehaving avg_score &gt;= 60order by b.c_name, total_score desc; 查询数学成绩比语文成绩高的所有学生信息及数学、语文的成绩123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960-- 使用子查询select x.*, a.course_name course_math, a.score math_score, b.course_name course_language , b.score language_scorefrom t_student xjoin ( -- 所有学生的数学成绩 select * from t_score WHERE course_name = '数学') aon x.s_id = a.s_idjoin ( -- 所有学生的语文成绩 select * from t_score WHERE course_name = '语文') b on a.s_id = b.s_idand b.score &lt;= a.score;-- 2. 使用joinselect x.*, y.course_name course_math, y.score math_score, z.course_name course_language , z.score language_scorefrom t_student xjoin t_score yON x.s_id = y.s_idand course_name = '数学'join t_score zon z.course_name = '语文'and z.s_id = y.s_id and z.score &lt;= y.score; 查询英语和语文成绩都大于60的学生信息1234567891011121314151617select x.* from t_student xjoin ( select a.s_id from t_score a where a.score &gt; 60 and a.course_name in ('英语','语文') group by a.s_id having count(1) = 2) y on y.s_id=x.s_id; 查询每个学生的学生ID、学生姓名及平均分,按照平局分降序排列123456789101112131415select b.s_id, b.s_name, avg(a.score) avg_scorefrom t_score ajoin t_student b on b.s_id = a.s_idgroup by b.s_id, b.s_nameorder by avg_score desc 上一题，加上，平局分大于60分1234567891011121314151617select b.s_id, b.s_name, avg(a.score) avg_scorefrom t_score ajoin t_student b on b.s_id = a.s_idgroup by b.s_id, b.s_namehaving avg_score &gt; 60order by avg_score desc 上上一题，加上只查看平均分最高的学生信息1234567891011121314151617select b.s_id, b.s_name, avg(a.score) avg_scorefrom t_score ajoin t_student b on b.s_id = a.s_idgroup by b.s_id, b.s_nameorder by avg_score desclimit 1; 上上上一题，加上平局分第3高的学生信息1234567891011121314151617select b.s_id, b.s_name, avg(a.score) avg_scorefrom t_score ajoin t_student b on b.s_id = a.s_idgroup by b.s_id, b.s_nameorder by avg_score desclimit 1 offset 2; 换一种方式实现上一题12345678910111213141516171819202122232425select x.s_id, x.s_name, x.avg_score, @cur_rank := @cur_rank+1 as score_rankfrom ( select b.s_id, b.s_name, avg(a.score) avg_score from t_score a join t_student b on b.s_id = a.s_id group by b.s_id, b.s_name) xcross join ( select @cur_rank:=0)yorder by x.avg_score desc;]]></content>
      <categories>
        <category>笔试题</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>笔试题</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-关联查询]]></title>
    <url>%2F2017%2F09%2F09%2Fmysql-handbook-03%2F</url>
    <content type="text"><![CDATA[MySQL关联查询 前面，我们介绍的都是单表查询（就是只从一张表中获取数据），而实际应用的时候，我们都会同时查询多张表，这里，我们就介绍下，多表关联查询的使用。 SQL join 用于根据两个或多个表中的列之间的关系，从这些表中查询数据 前置知识 主键（Primary Key）：可以唯一确定一条记录的字段，比如学生表中的学生ID，生活中我们的身份证号外键（Foreign Key）：指向另一张表的主键，比如学生表中的班级ID，班级ID是班级表中的主键，但在学生表中是外键 主键和外键可以在建表的时候指定，他可以在数据库层面，控制你的数据的完整性、一致性。 测试数据参考：http://yuguiyang.github.io/2017/09/09/mysql-handbook-01/ inner joininner join 可以简写为 join，结果集是两张表中 都存在的记录，是一个交集，详情参考上面的图片。比如：在学生表中，有一个班级ID，我们想根据班级ID，在班级表中找到班级信息1234567891011select *from t_student a -- 要关联查询的表join t_class b -- 使用什么字段去关联这两张表on a.c_id = b.c_id; left join左关联，以左边的表为主表，不管外键在右表中是否存在，左表的数据都会存在。比如学生表中，有这样一条记录，他的班级ID是904，但是班级表中并没有904的班级信息，所以，使用join的话是查不到这条记录的1234567891011-- 2. left join -- 学生表为主表，包含所有学生信息select *from t_student a left join t_class b on a.c_id = b.c_id; right join右关联，和做关联类似，但已右表为主表123456789101112-- 3. right join -- 班级表为主表，不管改班级是否有学生信息select *from t_student a right join t_class b on a.c_id = b.c_id; full outer join全关联，mysql没有full join 语法，我们可以通过使用union来实现123456789101112131415161718select *from t_student a left join t_class b on a.c_id = b.c_idUNIONselect *from t_student a right join t_class b on a.c_id = b.c_id; cross joincross join 是对2个表做笛卡尔积123select *from t_class a cross join t_class b order by a.c_id,b.c_id 小结关联查询的话，我们主要是选择好主表，然后找好表与表之间的关联关系，注意多对多、一对多的这种关系，验证号结果数据就行了。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-中文排序]]></title>
    <url>%2F2017%2F09%2F09%2Fmysql-handbook-02%2F</url>
    <content type="text"><![CDATA[MySQL中文排序 测试数据参考：http://yuguiyang.github.io/2017/09/09/mysql-handbook-01/ 以前还真没有关注这个中文排序的问题，这里记录下。 一张学生表1select *from t_student; 我们根据s_name来排序1select *from t_student order by s_name; 这里的中文排序，是不对的，应该是由于字符集的问题，一般情况下，数据库中的编码都是使用UTF-8的，所以，对于中文会有问题。 从网上找到2中解决办法 create table的时候加上binary属性（经测试，不好用）注意下s_name字段，我们添加了binary属性12345678CREATE TABLE `t_student_test` ( `s_id` int(11) DEFAULT NULL COMMENT '学生ID', `s_name` varchar(20) binary DEFAULT NULL COMMENT '学生姓名', `s_gender` int(11) DEFAULT NULL COMMENT '学生性别 0-男,1-女', `s_birthday` date DEFAULT NULL COMMENT '出生日期', `s_hobby` varchar(100) DEFAULT NULL COMMENT '爱好', `c_id` int(11) DEFAULT NULL COMMENT '班级ID') ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='学生表'; 这里，我试验是失败的，中文排序依然不对 在order by 后面，使用 convert函数1select *from t_student order by convert(s_name using gbk); 使用convert函数是可以的，没有问题]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-基本语法介绍]]></title>
    <url>%2F2017%2F09%2F09%2Fmysql-handbook-01%2F</url>
    <content type="text"><![CDATA[MySQL基本语法介绍 1. 什么是SQLSQL（Structured Query Language）结构化查询语言，通过SQL，我们就可以查询数据库中的数据，而数据再数据库中又是以表的形式保存的，所以SQL查询，主要就是对表进行查询。 SQL的语法就和学习英语的语法、汉语拼音一样，满足给定的套路，去使用就可以了。 当我们拿到了数据库的连接信息，连接到一个数据库上，我们就可以开始写SQL了。 2. Navicat的使用MySQL的客户端有很多，通常使用的，可能有Navicat，还有MySQL自带的workbench。Navicat是收费产品，但在网上可以找到XX版，workbench是免费的。 这里以Navicat为例，简单介绍下。 在这里，输入数据库地址、用户名、密码等等就行了。 这一个一个圆柱形的，就是一个数据库实例，下面那些电子表格图标的就是表，数据就存储在表中。 默认是不会看到表结构信息的，我们勾选下面的配置之后，就可以看到了 3. 基本语法数据准备12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970create table t_student( s_id int comment '学生ID', s_name varchar(20) comment '学生姓名', s_gender int comment '学生性别 0-男,1-女', s_birthday date comment '出生日期', s_hobby varchar(100) comment '爱好', c_id int comment '班级ID') comment '学生表';create table t_class( c_id int comment '班级ID', c_name varchar(20) comment '班级名称') comment '班级表';create table t_score( sc_id int comment '成绩ID', s_id int comment '学生ID', course_name varchar(20) comment '课程名称', score numeric(10,0) comment '成绩' ) comment '成绩表';insert into t_class values(901,'一班');insert into t_class values(902,'二班');insert into t_class values(903,'三班');insert into t_student values(101,'路飞',0,'1990-01-26','吃肉,睡觉',901);insert into t_student values(102,'娜美',1,'1995-10-05','足球,篮球',901);insert into t_student values(103,'乔巴',0,'1992-08-11','唱歌,吃肉',901);insert into t_student values(104,'鸣人',0,'1991-03-29','拉面,忍术',901);insert into t_student values(105,'卡卡西',1,'1989-05-10','看书,吃肉',902);insert into t_student values(106,'乌索普',1,'1988-02-02','跳舞,篮球',902);insert into t_student values(107,'乔峰',0,'1990-12-12','跑步,羽毛球',902);insert into t_student values(108,'段誉',0,'1990-12-13','吃肉,加班',903);insert into t_student values(109,'虚竹',1,'1991-01-22','看电影,旅行',903);insert into t_student values(110,'杨过',0,'2000-03-04','旅行',903);insert into t_score values(1,101,'数学',39);insert into t_score values(2,102,'数学',20);insert into t_score values(3,103,'数学',54);insert into t_score values(4,104,'数学',38);insert into t_score values(5,105,'数学',70);insert into t_score values(6,106,'数学',15);insert into t_score values(7,107,'数学',75);insert into t_score values(8,108,'数学',84);insert into t_score values(9,109,'数学',87);insert into t_score values(10,110,'数学',67);insert into t_score values(11,101,'语文',73);insert into t_score values(12,102,'语文',71);insert into t_score values(13,103,'语文',82);insert into t_score values(14,104,'语文',83);insert into t_score values(15,105,'语文',36);insert into t_score values(16,106,'语文',87);insert into t_score values(17,107,'语文',74);insert into t_score values(18,108,'语文',19);insert into t_score values(19,109,'语文',29);insert into t_score values(20,110,'语文',26);insert into t_score values(21,101,'英语',55);insert into t_score values(22,102,'英语',24);insert into t_score values(23,103,'英语',38);insert into t_score values(24,104,'英语',82);insert into t_score values(25,105,'英语',12);insert into t_score values(26,106,'英语',15);insert into t_score values(27,107,'英语',50);insert into t_score values(28,108,'英语',68);insert into t_score values(29,109,'英语',77);insert into t_score values(30,110,'英语',19); select下面，我们来看看，怎样查看一张表的数据；SQL的语法呢，就好比是一个公式，初学的话我们去套用就可以了。 SELECT 列名称 FROM 表名称或者SELECT * FROM 表名称 使用Navicat执行查询12-- 查看学生表数据，指定字段select s_id,s_name from t_student; 12-- 查看所有字段select *from t_student; 排序排序是很常用的功能，我们想要对结果集进行指定的排序，就要使用order by order by 字段名默认升序，可以使用desc降序排列 12-- 学生ID降序排列select *from t_student order by s_id desc; 多字段排序12-- 班级ID升序排列，班级ID一样的按学生ID降序排列select *from t_student order by c_id,s_id desc; limit 指定返回记录的数目 我们上面，都是查询一张表所有的数据，有的时候表的数据量很大，或者我们只想看看排名前3的数据，我们就可以使用limit12-- 学生ID降序排列,取前3条记录select *from t_student order by s_id desc limit 3; where前面，我们可以查看一张表的所有数据、做排序、然后只取前几行，实际使用时，一定会有这样的需求，比如我们只想看学生ID是105的记录，就需要使用where了，它可以对数据进行过滤。 SELECT 列名称 FROM 表名称 WHERE 列 运算符 值 12345678-- 查看学生ID是105的学生信息select *from t_student where s_id = 105;-- 查看学生ID不是105的其他学生信息select *from t_student where s_id &lt;&gt; 105 ;-- 查看学生ID在103和108之间的学生信息select *from t_student where s_id between 103 and 108; 这里还有一个操作符很常用，就是 in 和 not in。in 表示在多个值中存在，加上not则表示不存在12345-- 查看学生ID是103,105，,107的学生信息select *from t_student where s_id in (103,105,107);-- 查看学生ID不在102中的其他学生信息select *from t_student where s_id not in (102); like匹配字符串，像‘xxxx’一样 %： 表示任意个或多个字符_：表示任意单个字符 123-- 查看喜欢吃肉的学生信息select *from t_student where s_hobby like '吃肉%';select *from t_student where s_hobby like '%吃肉%'; and 和 or上面，我们都是一个单独的过滤条件，实际上，我们的会有各种各样的情况，需要同时满足多种过滤条件，这就用到了 and 和 or。 AND 和 OR 可在 WHERE 子语句中把两个或多个条件结合起来。如果第一个条件和第二个条件都成立，则 AND 运算符显示一条记录。如果第一个条件和第二个条件中只要有一个成立，则 OR 运算符显示一条记录。 12345-- 查看班级ID是901的所有男生信息select *from t_student where c_id=901 and s_gender=0;-- 查看班级ID是901或者s_id大于107的学生信息select *from t_student where c_id=901 or s_id &gt; 107;]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-序(怎样学习MySQL)]]></title>
    <url>%2F2017%2F09%2F09%2Fmysql-handbook-00%2F</url>
    <content type="text"><![CDATA[MySQL怎样学习MySQL 作为一名BI开发工程师，SQL是必须要掌握的一门技能。数据分析师请参考这篇《数据分析师是否要掌握SQL?》 SQL是干嘛的呢？大街上随便拉个人过来，可能都听说过“大数据”，不管大数据、小数据，他这个数据到底在哪儿呢？其中一种方式，就是存储在关系型数据库中（其他的还有什么非关系型数据库、HDFS等等），就像我们把货物都放在仓库里一样，如果我们想要查看数据库中的数据，就用到了SQL。 SQL也是一种编程语言，偏底层，所以学起来会枯燥些，不像Tableau那样可视化效果好，可以托拉拽。只要掌握了SQL的基本语法，他就像数学公式一样，直接去套用就行了。SQL的知识点也非常多，我们不需要都记住，要学会查文档、用Google，而且不同的数据库，他的语法可能不太一样，很容易记混。开发人员平时说的SQL脚本，就是一段或者多段SQL或者存储过程。 怎么学习SQL?随着互联网的发展，现在的学习成本非常的低，网络上的免费资源非常多。评价好些的像w3school的SQL 教程在这，就可以了解SQL的基本语法，记住这些，再配合习题练习就算是入门了。 sqlzoo是一个在线的练习网站,上面有很多SQL的练习题，我们可以直接在线写SQL，而且提交后，还会告诉我们对错。 如果还有时间的话，找一本书《SQL必知必会》、《MySQL必知必会》、《深入浅出SQL》，查漏补缺，可以更好的梳理自己的知识架构。 SQL就像所有的知识一样，学会不用很容易就忘掉，所以最好可以在工作中多用用，而且一定要记住“实践是检验真理的唯一标准”，SQL一定要多练习、多实践。 MySQL客户端我们想要连接数据库，使用SQL去查询数据库，需要使用一个客户端程序，就像我们访问网页，需要使用浏览器一样。 有的同学已经在学习或者使用SQL，只要电脑中已经安装了可以连接MySQL的客户端工具即可，不需要重复安装。没有安装的同学可以根据下面的推荐，来选择适合自己的工具。 Navicat(推荐)Navicat官网：http://www.navicat.com.cn/products Navicat使用起来比较方便，但它是一款收费软件，所以我们使用的时候需要破解下。支持正版的同学，可以去官网下载试用。 破解版的百度云下载地址：https://pan.baidu.com/s/1c1SNSru 我们从百度云上下载压缩包，选择适合自己系统的版本安装即可。 x86 是32位 x64 是64位 破解程序都是一个，安装完成后，执行破解程序即可。 WorkbenchMySQL官方提供的一个客户端工具，免费，也蛮好用的，官方下载地址：https://dev.mysql.com/downloads/workbench/ 百度云下载地址：https://pan.baidu.com/s/1c1SNSru 这里，我只下载到了64位的版本，需要32位的同学，去这个地址下载下：https://dev.mysql.com/downloads/workbench/ 安装Workbench，需要安装一些依赖项，稍微有些麻烦，我们可以先执行下安装程序，如果缺少依赖项，会提示我们安装。 Microsoft .NET Framework 4.5 Visual C++ Redistributable for Visual Studio 2015 百度云中的压缩包只提供了Visual C++的安装程序，.NET Framework的安装程序，请同学们到上面的下载地址自行下载。 附录SQL资料PDF https://pan.baidu.com/s/1jInAusI 后面也整理了一些MySQL的知识点MySQL分类，欢迎大家访问。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书笔记-数据分析实战(3章)]]></title>
    <url>%2F2017%2F09%2F04%2Freading_notes_data_analysis_02%2F</url>
    <content type="text"><![CDATA[读书笔记 《数据分析实战》 从第3章开始，都是从一个实际问题出发，套用，前面的数据分析思路，来进行模拟分析。第3章的主题是“销售额为什么会减少？”：一款社交游戏本月的销售额相较于上月有所下滑，于是想调查下滑的原因，来提升销售额。 现状和预期现状肯定是当月销售额下降，预期肯定是保持上升，等于甚至高于上月销售额，这里的话，要确定销售额下降是不是一个问题，因为该社交游戏一直保持稳定增长，所以突然下滑，一定是不正常的， 是一个问题。 发现问题我们明确了现状和预期，需要从中，找出影响最大的因素。上面说到，有3种方法去发现问题： 观察数据大小 数据分解（指标拆解） 数据对比 这一步，也是一个根据经验来提出假设的过程，我们需要从宏观角度，找到可能影响销售额的因素。我感觉，这一步是数据分析切入的点，比较重要，如果这一步没有发现核心问题，那后面的数据收集和分析都会有问题。 书中，在这一步，提出的问题是“商业宣传上存在问题”，对了，这一步，还需要及时和其他部门去沟通，像这种市场推广、商业宣传，本身我们可能不知道，所以，假设后要去确认是否有这样的情况。 数据的收集和加工数据分析解决对策]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>读书笔记</tag>
        <tag>实例</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析案例-购物篮分析]]></title>
    <url>%2F2017%2F09%2F01%2Fdata-analyst-method-01%2F</url>
    <content type="text"><![CDATA[数据分析案例 说到数据分析、数据挖掘，我们首先想到的可能就是沃尔玛那个“啤酒与尿布”的故事，它告诉我们，世间万物都有着千丝万缕的联系。这其中使用的数据分析方法就是“关联分析”。 什么是购物篮分析购物篮分析（Market Basket Analysis），购物篮就是我们去超市使用的篮子，结账的时候，购物篮中所有的商品都会被一起结算。所谓的购物篮分析就是通过购物篮子所反应的信息来==研究顾客的购买行为== 关联分析要解决的问题是：一群用户购买了很多产品之后，哪些产品同时购买的几率比较高？买A产品的同时，买哪个产品的几率比较高 这里介绍完概念，暂时还没啥写的，后面再补充下怎样实现，这里会有一个Tableau的简单购物篮分析。 参考文章上文的部分理论知识，摘自下面的博客 一起大数据]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>购物篮分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书笔记-数据分析实战(1、2章)]]></title>
    <url>%2F2017%2F09%2F01%2Freading_notes_data_analysis_01%2F</url>
    <content type="text"><![CDATA[读书笔记 《数据分析实战》 1. 什么是数据科学家书中通过“什么是数据”和“数据在商业中的应用”，推导出数据科学家的定义。 人们通过观测数据来推测出某种因果关系，再用这种因果关系来预测未来或者控制原因以达到预期的结果。把从事这种工作的人成为数据科学家。– 书中摘录 上面的定义觉得不是很清晰，就百度上找了找： 数据科学家是指能采用科学方法、运用数据挖掘工具对复杂多量的数字、符号、文字、网址、音频或视频等信息进行数字化重现与认识，并能寻找新的数据洞察的工程师或专家(不同于统计学家或分析师)。一个优秀的数据科学家需要具备的素质有：懂数据采集、懂数学算法、懂数学软件、懂数据分析、懂预测分析、懂市场应用、懂决策分析等。– 百度百科 我觉得数据科学家就是对于数据相关的所有门类都有一个整体的认识，感觉是个“杂家”，精通算法、什么深度学习、机器学习、AI之类的都是信手拈来，对我就是神一样的存在了，努力吧，同学。 2. 3中类型的数据科学家书中将数据科学家分成了3类，主要从所在领域分类： 商业领域出身 统计学出身 工程领域出身 这应该也是数据科学家成长的3条路线，从不同的路线出发，最终殊途同归。当然，这3个领域需要综合，才称得上是合格的数据科学家。 书中的技能配图，可以瞻仰下 数据分析的5个流程书中，将数据分析分为5个步骤，看完后，感觉很靠谱，真的很实用，这里分享下 商业数据分析的目的是解决问题，要解决问题，需要使用统计分析、机器学习、数据挖掘等各种方法。 现状和预期首先我们要确认“什么才是数据分析中的问题”。比如，“某种商品销售额下降”，这是一个现象，但它是不是一个问题呢？如果，该产品不是公司主打商品，并且就要下架了，那销售额下降并不是一个问题，或者，该商品处于正常的波动，或是季节、市场环境的外部因素导致的，可能都不是一个问题；相反，如果该商品是公司主打商品，并且没有其他外部因素导致，那销售额下降就是个问题了。 这里记录下，其实，还需要确认下，销售额取数逻辑是否有问题，确保数据没有问题，并且要知道这个下降是怎么定义的，是和什么商品，或时间段对比发现下降的。 有对比，才会有差距，既然下降了，说明他心里一定有个预期，即现状和预期之间是有差距的 发现问题有了上面的“现状和预期”，我们需要区别”现象和问题“。像“销售额下降”，“顾客流失”，这都是一个现象，我们需要从中去发现问题 现象 前提 预期 是否有问题 销售额下降 销售额比例低 维持现状 无 销售额下降 销售额比例高 将销售额恢复到良好状态 有 销售额上升 广告费用高 降低广告费用 有 销售额上升 广告费用适当 维持现状 无 从3个角度发现问题发现问题的关键是思考并理解现状和预期之间的差距。那怎样发现、理解这个差距呢？ 观察数据大小首先考虑有哪些因素会导致这些差距，并明确这些因素的影响程度大小，即找到影响最大的因素。 将数据分解后观察指从多个角度观察发生的现象，分解出构成这种现象的因素。在分解的时候，必须遵循MECE原则： Mutually 相互性 Exclusive 排重性 Collectively 完整性 Exhaustive 全面性 我感觉这个很抽象，不是很理解，书上有一个例子，说的还不错，常用的拆分方法是因数分解，比如： 销售额=人均销售额*购买人数 拆解后，找到容易调控的因子，才方面后面去解决问题 将数据比较后观察指的是将发生问题是的数据和没发生问题时的数据相互比较，并找出问题出现的原因。比如，按时间对比，看看同比、环比（使用时间序列） 昨天和今天比较 上周和本周比较 同一个商业活动前、后比较 与竞争对手数据比较 公司内部服务之间利益比较 年龄段差异 性别差异 地域差异 3.3 数据收集和整理通过前面，对现状和预期的对比，发现影响最大的因素后，我们就需要开始收集数据，来验证问题。数据收集的话，还会涉及到怎样去采集数据，比如想要的数据，并没有保存下来。已保存下来的数据，通常会保存在文件、数据库或者是Hadoop（HDFS）中收集完数据，我们就需要对数据进行加工，变成我们后面分析需要的格式，比如使用SQL进行处理，或者Python、R进行整合；我们再加工数据的同时，为了方便我们后面的分析，可能还需要增加一下自定义的变量，比如一些标志位，像“已消费（1），未消费（0）”；或者是一些离散变量，类似于区间段： 消费金额较大用户（1） 消费金额一般用户（2） 消费金额较小用户（3） 3.4 数据分析书中把数据分析按目的，分为两大类：“决策支持和自动化、最优化”。其中，“决策支持”使用简单求和、交叉列表的方式分析，还会涉及预测模型；“自动化、最优化”则涉及机器学习、构建算法。 3.5 解决对策通过上面两种分析思路，我们需要针对分析的结果，来判断是否要采取对应的决策，不同的对策， 又会产生不同的沟通成本。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>读书笔记</tag>
        <tag>实例</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL笔试题-连续登录天数]]></title>
    <url>%2F2017%2F08%2F31%2Fdata-analyst-interview-sql-03%2F</url>
    <content type="text"><![CDATA[SQL笔试题 下面的SQL基于PostgreSQL 1.用户连续登录天数背景描述现在我们有一张用户登录日志表，记录用户每天的登录时间，我们想要统计一下，用户每次连续登录的开始日期和结束日期，以及连续登录天数。 用户ID 登录日期 1001 2017-01-01 1001 2017-01-02 1001 2017-01-04 1001 2017-01-06 1002 2017-01-02 1002 2017-01-03 同学们先思考下，整理下思路，如果没有思路或者某几个点不了解，就可以继续往下看了。 测试数据1234567891011121314151617181920212223242526272829303132333435363738CREATE TABLE interview.tm_login_log( user_id integer, login_date date)WITH ( OIDS=FALSE);-- 这里的数据是最简化的情况，每个用户每天只有一条登录信息，insert into interview.tm_login_log values(1001,'2017-01-01');insert into interview.tm_login_log values(1001,'2017-01-02');insert into interview.tm_login_log values(1001,'2017-01-04');insert into interview.tm_login_log values(1001,'2017-01-05');insert into interview.tm_login_log values(1001,'2017-01-06');insert into interview.tm_login_log values(1001,'2017-01-07');insert into interview.tm_login_log values(1001,'2017-01-08');insert into interview.tm_login_log values(1001,'2017-01-09');insert into interview.tm_login_log values(1001,'2017-01-10');insert into interview.tm_login_log values(1001,'2017-01-12');insert into interview.tm_login_log values(1001,'2017-01-13');insert into interview.tm_login_log values(1001,'2017-01-15');insert into interview.tm_login_log values(1001,'2017-01-16');insert into interview.tm_login_log values(1002,'2017-01-01');insert into interview.tm_login_log values(1002,'2017-01-02');insert into interview.tm_login_log values(1002,'2017-01-03');insert into interview.tm_login_log values(1002,'2017-01-04');insert into interview.tm_login_log values(1002,'2017-01-05');insert into interview.tm_login_log values(1002,'2017-01-06');insert into interview.tm_login_log values(1002,'2017-01-07');insert into interview.tm_login_log values(1002,'2017-01-08');insert into interview.tm_login_log values(1002,'2017-01-09');insert into interview.tm_login_log values(1002,'2017-01-10');insert into interview.tm_login_log values(1002,'2017-01-11');insert into interview.tm_login_log values(1002,'2017-01-12');insert into interview.tm_login_log values(1002,'2017-01-13');insert into interview.tm_login_log values(1002,'2017-01-16');insert into interview.tm_login_log values(1002,'2017-01-17'); 步骤拆解我们首先要思考，怎样才算连续登录呢？就是1号登录，2号也登录了，这样就连续2天登录，那我们怎么知道2号他有没有登录呢？一种思路是根据排序来判断：我们来根据日期来排个名1234567select user_id, login_date, row_number() over(partition by user_id order by login_date) day_rankfrom interview.tm_login_log; 现在，我们根据用户ID，对他的登录日期做了排序，但是我们还是没有办法知道，他是不是连续的。我们根据这个排序再思考一下，对于一个用户来说，他的登录日期排序已经是连续的了，如果登录日期也是个数字，那我们根据每行的差值，就可以判断登录日期是否连续了。我们换个角度，我们找一个起始日期，来计算一个相差的天数，用它去和排序相对比，就可以了。12345678select user_id, login_date, date_part('day',login_date::timestamp - timestamp'2017-01-01') day_interval, -- 间隔天数 row_number() over(partition by user_id order by login_date) day_rank -- 日期排序from interview.tm_login_log; 我们观察下数据，因为日期排序是连续的，我们统计的间隔天数都是一个起始日期，所以，如果登录日期是连续的，那么，排序-间隔天数的差值也应该是一样的 12345678910111213select user_id, login_date, date_part('day',login_date::timestamp - timestamp'2017-01-01') day_interval, -- 间隔天数 row_number() over(partition by user_id order by login_date) day_rank, -- 日期排序 ( row_number() over(partition by user_id order by login_date) ) - ( date_part('day',login_date::timestamp - timestamp'2017-01-01') ) diff_valuefrom interview.tm_login_log; 差值一样的记录，就是连续登录的日期 好了，连续登录的判断标准，我们已经确定了，下面就是把题目中要的数据查出来即可1234567891011121314151617181920212223select user_id, --diff_value, --差值 min(login_date) start_date, --开始日期 max(login_date) end_date, --结束日期 count(1) running_days --连续登录天数from ( select user_id, login_date, date_part('day',login_date::timestamp - timestamp'2017-01-01') day_interval, -- 间隔天数 row_number() over(partition by user_id order by login_date) day_rank, -- 日期排序 ( row_number() over(partition by user_id order by login_date) ) - ( date_part('day',login_date::timestamp - timestamp'2017-01-01') ) diff_value from interview.tm_login_log) basegroup by user_id,diff_valueorder by user_id,start_date 拓展：获取用户最大的连续登录天数及开始日期和结束日期123456789101112131415161718192021222324252627282930313233with tmp as (select user_id, diff_value, --差值 min(login_date) start_date, --开始日期 max(login_date) end_date, --结束日期 count(1) running_days --连续登录天数from ( select user_id, login_date, date_part('day',login_date::timestamp - timestamp'2017-01-01') day_interval, -- 间隔天数 row_number() over(partition by user_id order by login_date) day_rank, -- 日期排序 ( row_number() over(partition by user_id order by login_date) ) - ( date_part('day',login_date::timestamp - timestamp'2017-01-01') ) diff_value from interview.tm_login_log) basegroup by user_id,diff_value) select a.user_id,a.start_date,a.end_date,a.running_daysfrom tmp ajoin ( select user_id,max(running_days) running_days from tmp group by user_id) b on a.user_id = b.user_idand a.running_days = b.running_days; 连续5天登录用户这里补充另一个类似的问题，这里，我们想看连续登录5天的用户，使用上面的方法可以实现，这里介绍一个更快的方法：是使用一个函数123456789101112向前取n位lag(value anyelement [, offset integer [, default anyelement ]])select *from ( select a.user_id, a.login_date, --5天前的登录日期 lag(a.login_date,4) over(partition by a.user_id order by a.login_date) pre_five_day from interview.tm_login_log a )xwhere date_part('day',x.login_date::timestamp - pre_five_day::timestamp)=4 这样取连续登录的话，比较方便 思考题：连续7天未登录用户这里留一个类似的小问题，大家自行练习下 小结我们简单整理下思路，上面的例子，我认为主要是一个思路的介绍，核心就是我们要找到一个判断连续的方法，找到方法后，SQL自然就一步一步想出来了。上面只是一种思路，一定还有更优的解法，欢迎大家反馈分享。]]></content>
      <categories>
        <category>笔试题</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>笔试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL笔试题-行转列]]></title>
    <url>%2F2017%2F08%2F28%2Fdata-analyst-interview-sql-02%2F</url>
    <content type="text"><![CDATA[SQL笔试题 下面的SQL基于PostgreSQL 1. 行转列背景我们写SQL的时候，经常会遇到一些列转行、行转列的情况，有的时候是为了展现需要，有的时候是代码里就得这样转一下。总之嘞，得掌握这个技巧。下面就开始我们的练习。 测试数据1234567891011121314151617181920CREATE TABLE interview.tm_score( stu_name character varying(20), -- 学生名称 course_name character varying(20), -- 课程名称 score numeric(10,0) -- 分数)WITH ( OIDS=FALSE);-- 初始化数据insert into interview.tm_score values('路飞','数学',100);insert into interview.tm_score values('路飞','语文',62);insert into interview.tm_score values('路飞','英语',98);insert into interview.tm_score values('索隆','数学',40);insert into interview.tm_score values('索隆','语文',57);insert into interview.tm_score values('索隆','英语',40);insert into interview.tm_score values('娜美','数学',42);insert into interview.tm_score values('娜美','语文',44);insert into interview.tm_score values('娜美','英语',28); 我们先来思考第一个问题：我们怎样可以将课程变成列呢，类似交叉表那样？最容易想到的方法，就是使用case when了 case when1234567891011select stu_name, max(case course_name when '数学' then score else 0 end) as "数学", max(case course_name when '语文' then score else 0 end) as "语文", max(case course_name when '英语' then score else 0 end) as "英语" from interview.tm_scoregroup by stu_name; crosstab在新版本的PostgreSQL中有一个extension，可以方便的实现行转列我们需要先安装这个扩展，我看了下，PostgreSQL8.3以后的版本都可以安装1create extension tablefunc 官网地址：https://www.postgresql.org/docs/9.5/static/tablefunc.html 安装完后，会有这么几个函数 我们可以使用crosstab来实现上面的行转列123select *from crosstab( 'select stu_name,course_name,score from interview.tm_score') as ct(stu_name varchar(20) ,"数学" numeric(10,0),"语文" numeric(10,0), "英语" numeric(10,0)) 结果也是一样的，我们传入一个SQL，SQL里面返回3列（这3列都是默认处理的，第一列是主字段，第2列是要拆成列的字段，第3列是要显示的值），最后因为返回值是record，所以我们要定义一下类型。 这里，顺便看看这个函数的用法 要转成列的字段有Null是这样一种情况：不是所有的同学都有3门课程的分数我们删掉了几条记录来模拟 这时候使用crosstab，结果会有问题 数据会自动从第一列开始，导致错误的数据12345-- crosstab(text source_sql, text category_sql)select *from crosstab( 'select stu_name,course_name,score from interview.tm_score','select distinct course_name from interview.tm_score') as ct(stu_name varchar(20) ,"数学" numeric(10,0),"语文" numeric(10,0), "英语" numeric(10,0)) 这时候，数据就对了 source_sql 多于3列这种情况是，我们查询的结果不单单有上面说的3列，可能还有其他字段，比如，我们可以在上面的测试数据上加一个班级列我们同样也是使用上面的方式解决 12345678alter table interview.tm_score add column stu_class varchar(10);update interview.tm_score set stu_class='一班' where stu_name in ('路飞','索隆');update interview.tm_score set stu_class='二班' where stu_name not in ('路飞','索隆');select *from crosstab( 'select stu_name,stu_class,course_name,score from interview.tm_score','select distinct course_name from interview.tm_score') as ct(stu_name varchar(20) ,stu_class varchar(10),"数学" numeric(10,0),"语文" numeric(10,0), "英语" numeric(10,0)) 这里我们六个思考题，有这样一组数据： 我们想查询出这样格式的数据 大家可以练习下这个SQL该怎么写 列转行我们可以把行转成列，那要怎样把列转成行呢？ union all最简单的方法是直接union all,既然要放到一列里面，那字段类型肯定要一样，所以直接根据不同字段去union all 就好了 12345select stu_name,"数学" from xxxunion all select stu_name,"语文" from xxxunion allselect stu_name,"英语" from xxx 小结这里简单介绍了几种常见的处理方法，实际使用中，一定还有更好地方法、或一些特殊的问题，欢迎大家分享、反馈。]]></content>
      <categories>
        <category>笔试题</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>笔试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL笔试题-累计值（月累计、年累计）]]></title>
    <url>%2F2017%2F08%2F28%2Fdata-analyst-interview-sql-01%2F</url>
    <content type="text"><![CDATA[SQL笔试题 下面的SQL基于PostgreSQL 1. 累计值（月累计、年累计）背景描述比如说，我们有这样一份数据,记录的是图书每天的销量情况： 日期 图书名称 销量 2017-01-01 解忧杂货店 90 2017-01-03 解忧杂货店 50 2017-01-05 解忧杂货店 100 2017-01-01 雪落香杉树 100 2017-01-03 雪落香杉树 44 2017-01-04 雪落香杉树 99 现在，我们要统计每本书，当月的累计销量？即1号是1号的销量，2号是1号+2号当天的销量（注意：这里2号当天虽然没有销量，但是应该为1号的90+2号的0，为90）。大家先思考下，如果可以很快解答，就不需要接着读啦，有疑问的同学可以继续往下看。 测试数据123456789101112131415161718192021222324252627282930313233343536373839-- 图书的销量表CREATE TABLE interview.tm_book_sales( calendar_date date, -- 日期 book_name character varying(100), -- 图书名称 sales numeric(10,0) -- 销量)WITH ( OIDS=FALSE);-- 测试数据insert into tm_book_sales values('2017-01-01','解忧杂货店',56);insert into tm_book_sales values('2017-01-02','解忧杂货店',100);insert into tm_book_sales values('2017-01-03','解忧杂货店',70);insert into tm_book_sales values('2017-01-06','解忧杂货店',11);insert into tm_book_sales values('2017-01-07','解忧杂货店',65);insert into tm_book_sales values('2017-01-08','解忧杂货店',9);insert into tm_book_sales values('2017-01-09','解忧杂货店',30);insert into tm_book_sales values('2017-01-10','解忧杂货店',56);insert into tm_book_sales values('2017-01-01','雪落香杉树',18);insert into tm_book_sales values('2017-01-02','雪落香杉树',40);insert into tm_book_sales values('2017-01-03','雪落香杉树',2);insert into tm_book_sales values('2017-01-04','雪落香杉树',22);insert into tm_book_sales values('2017-01-05','雪落香杉树',48);insert into tm_book_sales values('2017-01-07','雪落香杉树',71);insert into tm_book_sales values('2017-01-09','雪落香杉树',73);insert into tm_book_sales values('2017-01-10','雪落香杉树',37);insert into tm_book_sales values('2017-02-03','解忧杂货店',5);insert into tm_book_sales values('2017-02-05','解忧杂货店',46);insert into tm_book_sales values('2017-02-06','解忧杂货店',35);insert into tm_book_sales values('2017-02-07','解忧杂货店',10);insert into tm_book_sales values('2017-02-09','解忧杂货店',30);insert into tm_book_sales values('2017-02-10','解忧杂货店',12);insert into tm_book_sales values('2017-02-13','解忧杂货店',43);insert into tm_book_sales values('2017-02-01','雪落香杉树',10);insert into tm_book_sales values('2017-02-04','雪落香杉树',78);insert into tm_book_sales values('2017-02-10','雪落香杉树',50);insert into tm_book_sales values('2017-02-20','雪落香杉树',93); 现在呢，我们有了图书每天的销量数据，下面，我们思考1个问题：我想要统计每本图书的当月累计销量，应该怎么做呢？ 如果只是单纯的统计每本书每个月的销量，熟悉SQL的同学，一定可以很快想到12345678910111213select to_char(calendar_date,'yyyy-mm') month_name, book_name, sum(sales) from interview.tm_book_sales group by to_char(calendar_date,'yyyy-mm'), book_nameorder by book_name, to_char(calendar_date,'yyyy-mm'); 下面，我们来想下，这个月累计怎么做？月累计值，其实就是当天的销量再加上当天之前的销量 自关联通过 interview.tm_book_sales 表，我们可以获取每一天的销量，那要怎样获取每天历史的销量呢？最简单的方式就是自关联了。其实就是自己和自己去关联，来获取历史的销量123456789101112131415161718192021222324select t_today.calendar_date, t_today.book_name, sum(t_his.sales) sales_mtdfrom interview.tm_book_sales t_todayleft join interview.tm_book_sales t_hison -- 图书名称相等 t_his.book_name = t_today.book_name and -- 月份相等，只统计当月 to_char(t_his.calendar_date,'yyyy-mm') = to_char(t_today.calendar_date,'yyyy-mm')and -- 获取当天之前的历史日期 t_his.calendar_date &lt;= t_today.calendar_dategroup by t_today.calendar_date, t_today.book_nameorder by t_today.book_name, t_today.calendar_date; 好了，上面，我们通过自关联，获取了每本图书的月累计销量，不要太高兴，我们观察下，就会发现些问题。我们看看日期，就会发现，有些日期是没有销量的，比如：《解忧杂货店》2017-01-04，2017-01-05 就没有销量，但实际上，如果是累计值得花，他是应该有数据的，因为1号、2号、3号都有数据，就算4号当天没有销量，月累计也应该要算上前3天的销量，所以我们的SQL并不严谨，还得修改。 补全没有销量的日期我们需要想办法补全缺失的日期，如果，t_today里面含有每一天每本书的数据就好了，这就要我们手动构造一个了。123456789101112select t_day.calendar_date, t_book.book_namefrom -- 日期维度表，就是存放每一天 base.dm_calendar t_daycross join -- 所有的图书信息 (select distinct book_name from interview.tm_book_sales) t_bookwhere t_day.month_id=201701; 日期维度表的话，其实是数仓中必备的地基础维度中的一个，她里面就是存放了每一天的数据，和其他一些我们会常用的字段，后面写一篇文章详细介绍下。我们通过笛卡尔积，生成了一张包含每一天每本的图书的一个全维度表。1234567891011121314151617181920212223242526272829303132333435363738394041with t_dim as ( select t_day.calendar_date, t_book.book_name from base.dm_calendar t_day cross join (select distinct book_name from interview.tm_book_sales) t_book where t_day.month_id=201701)select t_dim.calendar_date, t_dim.book_name, sum(t_his.sales) sales_mtdfrom t_dimleft join interview.tm_book_sales t_todayon t_today.calendar_date = t_dim.calendar_dateand t_today.book_name = t_dim.book_nameleft join interview.tm_book_sales t_hison -- 图书名称相等 t_his.book_name = t_dim.book_name and -- 月份相等，只统计当月 to_char(t_his.calendar_date,'yyyy-mm') = to_char(t_dim.calendar_date,'yyyy-mm')and -- 获取当天之前的历史日期 t_his.calendar_date &lt;= t_dim.calendar_dategroup by t_dim.calendar_date, t_dim.book_nameorder by t_dim.book_name, t_dim.calendar_date; 好啦，补全了日期信息后，我们的月累计算是完成了，手工。 总结简单总结下，通过上面的例子，我们要掌握什么呢？首先是对业务的理解，比如上面的月累计的统计方法；然后根据统计方法，使用SQL去实现，一步步完善；还有对日期维度表的一个综合使用。年累计的实现也是一样的，同学们可以自行练习下，有问题可以反馈。]]></content>
      <categories>
        <category>笔试题</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>笔试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础（10）- 时间函数小记]]></title>
    <url>%2F2017%2F08%2F23%2Fpython-handbook-12%2F</url>
    <content type="text"><![CDATA[这里整理下时间类函数的使用方法，主要是整理下，防止老是忘记。参考官方网站：https://docs.python.org/3/library/datetime.html?highlight=strftime#strftime-strptime-behavior 1. strftime datetime.strftime(format)Return a string representing the date and time, controlled by an explicit format string.返回将datetime根据指定format格式化后的字符串 123456789101112from datetime import datetimenow = datetime.now()nowOut[3]: datetime.datetime(2017, 8, 23, 10, 21, 41, 802905)now.strftime('%Y-%m-%d %H:%M:%S')Out[5]: '2017-08-23 10:21:41' now.strftime('%Y-%m-%d %H:%M:%S %f')Out[6]: '2017-08-23 10:21:41 802905' 这里主要是了解format格式就可以了常用格式：1234567891011121314151617181920212223参考官方网站：http://www.runoob.com/python/att-time-strftime.html %y 两位数的年份表示（00-99） %Y 四位数的年份表示（000-9999） %m 月份（01-12） %d 月内中的一天（0-31） %H 24小时制小时数（0-23） %I 12小时制小时数（01-12） %M 分钟数（00=59） %S 秒（00-59） %a 本地简化星期名称 %A 本地完整星期名称 %b 本地简化的月份名称 %B 本地完整的月份名称 %c 本地相应的日期表示和时间表示 %j 年内的一天（001-366） %p 本地A.M.或P.M.的等价符 %U 一年中的星期数（00-53）星期天为星期的开始 %w 星期（0-6），星期天为星期的开始 %W 一年中的星期数（00-53）星期一为星期的开始 %x 本地相应的日期表示 %X 本地相应的时间表示 %Z 当前时区的名称 %% %号本身 2. strptime datetime.strptime(date_string, format)Return a datetime corresponding to date_string, parsed according to format根据format，将字符串转换为datetime 1234datetime.strptime('2017-08-23 10:21:41 802905','%Y-%m-%d %H:%M:%S %f')Out[8]: datetime.datetime(2017, 8, 23, 10, 21, 41, 802905)#就是strftime的一个逆过程，一个是日期到字符串，一个是字符串到日期]]></content>
      <categories>
        <category>Python-基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[词云图之《战狼2》影评]]></title>
    <url>%2F2017%2F08%2F22%2Fvisual-handbook-03%2F</url>
    <content type="text"><![CDATA[下午看了社区里的一篇文章《Python 爬虫实践：《战狼2》豆瓣影评分析》，感谢分享。最近也是在学习爬虫，周末刚好看了词云图，这里就自己也来实现下。 周末的词云图介绍《word_cloud-用Python之作个性化词云图》 豆瓣影评页面分析我们到豆瓣电影模块，选择《战狼2》，找到下面的短评 页面地址：https://movie.douban.com/subject/26363254/comments?status=P 通过FireBug，观察页面，可以发现，评论信息还是很好拿的 然后，我们看看下一页数据是怎么获取的 这里是直接用参数传的，多点几次观察，就会发现规律 这里有个小疑问，他这个参数start，短评每页20条没有问题，但是这个start，并不是0，20,40开始的，会跳跃，不知道为啥，而且，这个limit貌似是假的，我改成100都没用，还是显示20条而且，不登录的话，并不能看完所有的短评，后面会报错，说没有权限。1234567891011121314151617#解析当前页面 def parseCurrentPage(html): soup = BeautifulSoup(html, "html.parser") #获取评论信息 p_comments = soup.select('div#comments div.comment p') p_lines=[] for com in p_comments: p_lines.append(com.contents[0].strip()+'\n') #获取下一页信息，可以通过这个获取数据 p_next = soup.select_one('div#paginator a.next') #print(p_next) print(p_next['href']) return p_lines 生成词云图这里的方法还是和周末的那一篇类似，这里多了一个stopwords的概念，就是剔除了一些没有用的词语，貌似网上可以找到通用的一些，我这里直接根据测试，手动剔除的。 原文是自己使用pandas统计的词频，我这里直接就传给Wordcloud了，后面再试试12345#指定需要提出的词语 stopwords = &#123;u'个',u'一个',u'这个',u'个人',u'不是',u'就是',u'一部',u'这部' ,u'我们',u'所以',u'不会',u'这种',u'没有',u'各种',u'觉得' ,u'真的',u'知道',u'还是',u'但是',u'可以',u'这么',u'因为',u'很多'&#125; print('stopwords',stopwords) 实例代码刚刚看了下导出的评论文件，发现有重复数据，一定是哪里有问题 刚试了下，这个影评的返回结果有毒啊1234https://movie.douban.com/subject/26363254/comments?start=20&amp;limit=20&amp;sort=new_score&amp;status=P和https://movie.douban.com/subject/26363254/comments?start=26&amp;limit=20&amp;sort=new_score&amp;status=P这2个显示的内容居然是一样的。。看来还是得通过每次下一页的href属性去获取下一页地址 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788# -*- coding: utf-8 -*-"""Created on Tue Aug 22 16:13:24 2017@author: yuguiyang"""import osimport urllibfrom bs4 import BeautifulSoupimport jiebafrom wordcloud import WordCloudimport matplotlib.pyplot as pltfile_name = 'douban_movie_zhanlang.txt'#根据URL，获取页面源码def getHtml(url): req = urllib.request.Request(url) page = urllib.request.urlopen(req).read().decode('utf-8') return page#删除文件def clearFile(targetFile): if os.path.exists(targetFile): os.remove(targetFile) #将数据保存到文件def save2file(lines,targetFile): with open(targetFile,'a') as file: file.writelines(lines)#解析当前页面 def parseCurrentPage(html): soup = BeautifulSoup(html, "html.parser") #获取评论信息 p_comments = soup.select('div#comments div.comment p') p_lines=[] for com in p_comments: p_lines.append(com.contents[0].strip()+'\n') #获取下一页信息，可以通过这个获取数据 p_next = soup.select_one('div#paginator a.next') #print(p_next) print(p_next['href']) return p_lines def showWordCloud(targetFile): #指定字体，是为了显示中文 font = r'C:\Windows\Fonts\simsun.ttc' with open(targetFile) as file: comments = file.read() text_cut = jieba.cut(comments , cut_all=False) #指定需要提出的词语 stopwords = &#123;u'个',u'一个',u'这个',u'个人',u'不是',u'就是',u'一部',u'这部' ,u'我们',u'所以',u'不会',u'这种',u'没有',u'各种',u'觉得' ,u'真的',u'知道',u'还是',u'但是',u'可以',u'这么',u'因为',u'很多'&#125; print('stopwords',stopwords) wordcloud = WordCloud(font_path=font,width=800,height=400, background_color='white',stopwords=stopwords ).generate(' '.join(text_cut)) plt.imshow(wordcloud, interpolation='bilinear') plt.axis("off") def main(): url = 'https://movie.douban.com/subject/26363254/comments?start=&#123;0&#125;&amp;limit=20&amp;sort=new_score&amp;status=P' clearFile(file_name) #直接遍历200条评论，这里要注意，超过多少页后，需要登录才可以，这里暂时还没有做，就到200 for i in range(0,200,20): print(url.format(i)) html = getHtml(url.format(i)) movie_comments = parseCurrentPage(html) save2file(movie_comments,file_name) #显示词云图 showWordCloud(file_name)if __name__=='__main__': main()]]></content>
      <categories>
        <category>数据可视化-Python&amp;R</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>可视化</tag>
        <tag>词云图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫小实例-拉勾网数据分析岗位]]></title>
    <url>%2F2017%2F08%2F19%2Fcrawler-demo-03%2F</url>
    <content type="text"><![CDATA[周末没事，想看看最近的职位招聘情况，就用拉勾网为例，练习下爬虫 分析首先去拉勾网的主页去观察下，当前的搜索页面URL： 观察下源码，发现结构很清晰，解析起来应该很方便，再通过下一页去遍历即可 通过上面的，我们直接去解析应该是足够了，我们再看下他的请求信息，会找到这样一个post，很明显，这个pn应该就是页码（多查询几次观察下可以确认） 这个应该就是查询的请求，看看结果集，就是我们想要的数据 所以，如果我们直接去请求json数据，然后解析下就行了 要注意的小问题写的时候，主要遇到几个问题， 2.1 直接请求的话，不行，会被拦截，关于拉勾网，网上的例子很多，这里测试的时候，只要加上header就行了，其他的不需要headers = {‘User-Agent’:r’Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:55.0) Gecko/20100101 Firefox/55.0’,’Referer’:r’https://www.lagou.com/jobs/list_%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90?labelWords=&amp;fromSearch=true&amp;suginput&#39;,&#39;Host&#39;:&#39;www.lagou.com&#39;} 2.2 json的结果集中，有总条数和每页条数，根据这2点，我们可以算出总页数 2.3 解析json就行了 2.4 在 循环的时候，sleep了5秒，因为一直查询偶尔会报错，估计是被拦截了 代码下面分享下，第一版代码，后面再优化：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495# -*- coding: utf-8 -*-"""Created on Sat Aug 19 15:22:20 2017@author: yuguiyang"""import urllibimport jsonimport mathimport csvimport timeimport ostotal_page = 1current_page = 1#更新结果集的总页数def update_total_page(page_size,totalCount): global total_page total_page = math.ceil(totalCount/page_size) #解析HTML页面def parse_html(url,headers,post_data): post_data = urllib.parse.urlencode(post_data).encode('utf-8') req = urllib.request.Request(url,headers=headers,data=post_data) page = urllib.request.urlopen(req) return page.read().decode('utf-8') #解析返回的json数据def parse_result(html): data_json = json.loads(html) print(data_json['content']['pageNo']) #第一页的话，更新下总页数 if data_json['content']['pageNo'] == 1 : update_total_page(data_json['content']['pageSize'],data_json['content']['positionResult']['totalCount']) #将结果保存到csv文件 with open('lagou_shujufenxi_data.csv','a',newline='',encoding='utf-8') as csvfile: csv_writer = csv.writer(csvfile,quotechar='"',quoting=csv.QUOTE_MINIMAL) #在第一页的时候，写入标题 if data_json['content']['pageNo'] == 1 : csv_writer.writerow(['公司简称','公司全称','所属行业','工作地点','职位特点','薪资','经验','职位名称','firstType','secondType']) #遍历招聘信息 for job in data_json['content']['positionResult']['result']: line = [] line.append(job['companyShortName']) line.append(job['companyFullName']) line.append(job['industryField']) line.append(job['district']) line.append(job['positionAdvantage']) line.append(job['salary']) line.append(job['workYear']) line.append(job['positionName']) line.append(job['firstType']) line.append(job['secondType']) csv_writer.writerow(line)def main(): headers = &#123; 'User-Agent':r'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:55.0) Gecko/20100101 Firefox/55.0', 'Referer':r'https://www.lagou.com/jobs/list_%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90?labelWords=&amp;fromSearch=true&amp;suginput', 'Host':'www.lagou.com' &#125; url = 'https://www.lagou.com/jobs/positionAjax.json' #判断要导出的文件是否存在 if os.path.exists(r'lagou_shujufenxi_data.csv'): print('hey,delete the file') os.remove(r'lagou_shujufenxi_data.csv') #循环所有页数，取导出数据 while current_page &lt;= total_page: #post参数,每次修改页码就行了 post_data = &#123;'city':'上海','isSchoolJob':'0','needAddtionalResult':'false','kd':'数据分析','pn':current_page&#125; #解析每一个页面 html = parse_html(url ,headers, post_data) #处理获取的json数据 parse_result(html) #当前页码加1 current_page += 1 #这里sleep5秒，不停一下，偶尔会报错 time.sleep(5) if __name__=='__main__': main()]]></content>
      <categories>
        <category>Python-爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中文分词-jieba]]></title>
    <url>%2F2017%2F08%2F19%2Fpython-handbook-11%2F</url>
    <content type="text"><![CDATA[刚刚在看Python的词云图，想要显示中文的时候，需要做一个分词，这里我们学习下jieba分词。 1. jieba中文分词jieba是Python中文分词的一个组件 github地址：[https://github.com/fxsjy/jieba](https://github.com/fxsjy/jieba) 支持三种分词模式：精确模式，试图将句子最精确地切开，适合文本分析；全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义；搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。支持繁体分词支持自定义词典MIT 授权协议 安装：1pip install jieba 官网上的小实例12345678910111213import jiebaseg_list = jieba.cut("我来到北京清华大学", cut_all=True)print("Full Mode: " + "/ ".join(seg_list)) # 全模式seg_list = jieba.cut("我来到北京清华大学", cut_all=False)print("Default Mode: " + "/ ".join(seg_list)) # 精确模式seg_list = jieba.cut("他来到了网易杭研大厦") # 默认是精确模式print(", ".join(seg_list))seg_list = jieba.cut_for_search("小明硕士毕业于中国科学院计算所，后在日本京都大学深造") # 搜索引擎模式print(", ".join(seg_list)) 12345678Building prefix dict from the default dictionary ...Dumping model to file cache C:\Users\YUGUIY~1\AppData\Local\Temp\jieba.cacheLoading model cost 0.884 seconds.Prefix dict has been built succesfully.Full Mode: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学Default Mode: 我/ 来到/ 北京/ 清华大学他, 来到, 了, 网易, 杭研, 大厦小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, ，, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造 到这里，想要的功能其实已经足够了，官网还有很多其他的介绍内容，很强大，后面需要的时候，再来补充，接着回去改那个词云的了先。]]></content>
      <categories>
        <category>Python-基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>分词</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python异常（1）- module 'urllib' has no attribute 'urlopen']]></title>
    <url>%2F2017%2F08%2F19%2Fpython-handbook-10%2F</url>
    <content type="text"><![CDATA[今天想复习下BeautifulSoup，就把之前的代码拿过来测试，发现报错了 12345678910111213141516import urllibfrom bs4 import BeautifulSoup#加载网址，获取当前页面def getHTML(url) : page = urllib.urlopen(url) html = page.read() return htmlhtml = getHTML('https://movie.douban.com/top250')soup = BeautifulSoup(html, "html.parser")for img in soup.find_all('img'): print(img.get('src')) 查了下，发下是Python3中，需要引入的模块变了改一下就可以了1urllib.request.urlopen(url)]]></content>
      <categories>
        <category>Python-基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>异常</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[word_cloud-用Python之作个性化词云图]]></title>
    <url>%2F2017%2F08%2F19%2Fvisual-handbook-02%2F</url>
    <content type="text"><![CDATA[网上有很多制作词云的网站，我们使用Python也可以很方便的制作，这里，我们就简单学习下。 1. word_cloudGitHub地址：https://github.com/amueller/word_cloud首先我们需要安装，正常来说，直接就执行1pip install wordcloud 即可，但是，我这个是在Windows平台，安装的时候，提示什么少了，需要去下载个编译器的，报错信息后面有URL，这里忘记记下来了，遇到的同学，直接去下载下应该就行了 error: Microsoft Visual C++ 14.0 is required 这里，我从网上找了另一个方法解决去这个网站：http://www.lfd.uci.edu/~gohlke/pythonlibs/#wordcloud找到指定的版本下载就行了 这里，我就下载了这个cp36的，然后直接安装就行了1pip install wordcloud-1.3.2-cp36-cp36m-win_amd64.whl 官网上有个小栗子,我们可以测试下，代码和使用到的数据，github上都有123456789101112131415161718192021222324252627from os import pathfrom wordcloud import WordCloudd = path.dirname(__file__)# Read the whole text.text = open(path.join(d, 'constitution.txt')).read()# Generate a word cloud imagewordcloud = WordCloud().generate(text)# Display the generated image:# the matplotlib way:import matplotlib.pyplot as pltplt.imshow(wordcloud, interpolation='bilinear')plt.axis("off")# lower max_font_sizewordcloud = WordCloud(max_font_size=40).generate(text)plt.figure()plt.imshow(wordcloud, interpolation="bilinear")plt.axis("off")plt.show()# The pil way (if you don't have matplotlib)# image = wordcloud.to_image()# image.show() 我们通过wordcloud生成了一个图片，然后使用matplotlib将图片展示出来，我们分析下上面的代码 12345matplotlib.pyplot.imshow(X, cmap=None, norm=None, aspect=None, interpolation=None, alpha=None, vmin=None, vmax=None, origin=None, extent=None, shape=None, filternorm=1, filterrad=4.0, imlim=None, resample=None, url=None, hold=None, data=None, **kwargs)Display an image on the axes.class wordcloud.WordCloud(font_path=None, width=400, height=200, margin=2, ranks_only=None, prefer_horizontal=0.9, mask=None, scale=1, color_func=None, max_words=200, min_font_size=4, stopwords=None, random_state=None, background_color='black', max_font_size=None, font_step=1, mode='RGB', relative_scaling=0.5, regexp=None, collocations=True, colormap=None, normalize_plurals=True) 这里有很多例子：https://amueller.github.io/word_cloud/auto_examples/index.html 2. 中文词云上面，我们试试中文，我们换一个中文的文件试试即可 东方网记者王佳妮8月19日报道：8月18日晚，一则重磅消息在沪发布：即日起，上海暂停新增投放共享单车。上海市交通委表示，将采取有效措施，逐步解决共享单车快速发展带来的无序和不平衡问题，促进共享单车行业的持续健康发展。东方网记者从各平台获悉，将积极配合，同时考虑将中心城区部分车辆向郊区转移。 执行后，发现，中文都是乱码我们观察上面wordcloud类，他有一个参数123font_path : string Font path to the font that will be used (OTF or TTF). Defaults to DroidSansMono path on a Linux machine. If you are on another OS or don’t have this font, you need to adjust this path. 这个可以指定为中文字体，123font = r'C:\Windows\Fonts\simsun.ttc'# Generate a word cloud imagewordcloud = WordCloud(font_path=font).generate(text) 再次执行，中文可以正常显示了 但是，显示的并不是我们想要的，正常词云的话，显示的是关键词，和他的出现频率有关，这里的话，并没有对文本内容做很好的分词刚刚去简单整理了一个Python的分词：Python中文分词-jieba 这样，我们继续修改下上面的代码12345678910111213141516171819from os import pathfrom wordcloud import WordCloudimport jiebad = path.dirname(__file__)# Read the whole text.text = open(path.join(d, 'hello.txt'),encoding='utf-8').read()text_cut = jieba.cut(text , cut_all=False)font = r'C:\Windows\Fonts\simsun.ttc'# Generate a word cloud imagewordcloud = WordCloud(font_path=font).generate(' '.join(text_cut))# Display the generated image:# the matplotlib way:import matplotlib.pyplot as pltplt.imshow(wordcloud, interpolation='bilinear')plt.axis("off") 再次执行下 好嘞，这下差不多了，我们的中文词云图就搞好了 3. 自定义形状的词云图 下面，我们看看自定义形状的词云图，这里用官方的例子，就是需要多传入一个mask图片就行了12345678910111213141516171819202122232425262728293031323334353637383940414243# -*- coding: utf-8 -*-"""Created on Sun Aug 20 14:04:48 2017@author: yuguiyang"""from os import pathfrom PIL import Imageimport numpy as npimport matplotlib.pyplot as pltfrom wordcloud import WordCloud, STOPWORDSd = path.dirname(__file__)# Read the whole text.text = open(path.join(d, 'alice.txt'),encoding='utf-8').read()# read the mask image# taken from# http://www.stencilry.org/stencils/movies/alice%20in%20wonderland/255fk.jpgalice_mask = np.array(Image.open(path.join(d, "alice_mask.png")))stopwords = set(STOPWORDS)stopwords.add("said")wc = WordCloud(background_color="white", max_words=8000, mask=alice_mask, stopwords=stopwords)# generate word cloudwc.generate(text)# store to filewc.to_file(path.join(d, "alice.png"))# showplt.imshow(wc, interpolation='bilinear')plt.axis("off")plt.figure()plt.imshow(alice_mask, cmap=plt.cm.gray, interpolation='bilinear')plt.axis("off")plt.show() 刚试了下，就是要找一个喜欢的图形图片，]]></content>
      <categories>
        <category>数据可视化-Python&amp;R</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>词云</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础（9）- 排序技巧]]></title>
    <url>%2F2017%2F08%2F18%2Fpython-handbook-09%2F</url>
    <content type="text"><![CDATA[通常，我们排序的时候，我们可以使用系统内置的sorted函数或list自带的sort函数。参考文章：http://www.jb51.net/article/57678.htmhttps://docs.python.org/3/library/stdtypes.html?highlight=sort#list.sort 1. list.sort sort(*, key=None, reverse=False)This method sorts the list in place, using only &lt; comparisons between items. Exceptions are not suppressed - if any comparison operations fail, the entire sort operation will fail (and the list will likely be left in a partially modified state). sort是就地排序123456789101112131415list1 = [66, 48, 28, 69, 86, 96, 89, 37, 56, 33]list1Out[93]: [66, 48, 28, 69, 86, 96, 89, 37, 56, 33]list1.sort()list1Out[95]: [28, 33, 37, 48, 56, 66, 69, 86, 89, 96]#默认是升序，可以使用reverse降序list1.sort(reverse=True)list1Out[97]: [96, 89, 86, 69, 66, 56, 48, 37, 33, 28] key可以传入一个函数，会作用于每一个element，返回值会作为排序键 key specifies a function of one argument that is used to extract a comparison key from each list element (for example, key=str.lower). The key corresponding to each item in the list is calculated once and then used for the entire sorting process. The default value of None means that list items are sorted directly without calculating a separate key value. 1234567891011121314list2=list('AckjehLhct')list2Out[99]: ['A', 'c', 'k', 'j', 'e', 'h', 'L', 'h', 'c', 't']list2.sort()list2Out[101]: ['A', 'L', 'c', 'c', 'e', 'h', 'h', 'j', 'k', 't']list2.sort(key=str.lower)list2Out[103]: ['A', 'c', 'c', 'e', 'h', 'h', 'j', 'k', 'L', 't'] 2. sorted sorted(iterable, *, key=None, reverse=False)Return a new sorted list from the items in iterable sorted是返回一个排序后的新list，使用方式基本一样，只是12345678910list3 = [84, 57, 15, 84, 40, 37, 66, 45, 11, 95]list3Out[107]: [84, 57, 15, 84, 40, 37, 66, 45, 11, 95]sorted(list3)Out[108]: [11, 15, 37, 40, 45, 57, 66, 84, 84, 95]sorted(list3,reverse=True)Out[109]: [95, 84, 84, 66, 57, 45, 40, 37, 15, 11] sorted方法对所有的可迭代对象都有效12sorted(&#123;3:'a',1:'z',5:'b',2:'c'&#125;)Out[110]: [1, 2, 3, 5] 使用sorted还能对复杂对象进行排序12345678910111213141516171819202122l_op=[('namei',20,'a'),('lufei',19,'c'),('qiaoba',15,'b')]l_opOut[112]: [('namei', 20, 'a'), ('lufei', 19, 'c'), ('qiaoba', 15, 'b')]#默认是按照每一个tuple的第一个元素来排序的sorted(l_op)Out[113]: [('lufei', 19, 'c'), ('namei', 20, 'a'), ('qiaoba', 15, 'b')]sorted(l_op,key=lambda x: x[0])Out[114]: [('lufei', 19, 'c'), ('namei', 20, 'a'), ('qiaoba', 15, 'b')]#我们可以通过key函数，来指定要使用的排序键，sorted(l_op,key=lambda x: x[1])Out[115]: [('qiaoba', 15, 'b'), ('lufei', 19, 'c'), ('namei', 20, 'a')]sorted(l_op,key=lambda x: x[2],reverse=True)Out[116]: [('lufei', 19, 'c'), ('qiaoba', 15, 'b'), ('namei', 20, 'a')]sorted(l_op,key=lambda x: x[2])Out[117]: [('namei', 20, 'a'), ('qiaoba', 15, 'b'), ('lufei', 19, 'c')] 对于自定义的对象也是可以的123456789101112131415161718192021222324252627class People: def __init__(self,id,name): self.id=id self.name=name p1 = People(10,'lufei')p2 = People(5,'namei')p3 = People(15,'qiaoba')#指定我们要排序的字段就行了for p in sorted([p1,p2,p3],key=lambda p: p.id): print(p.id,p.name)#out:5 namei10 lufei15 qiaobafor p in sorted([p1,p2,p3],key=lambda p: p.name): print(p.id,p.name)#out:10 lufei5 namei15 qiaoba]]></content>
      <categories>
        <category>Python-基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib手册(10)-用pyplot实现“房间里100个人玩游戏的例子”]]></title>
    <url>%2F2017%2F08%2F18%2Fmatplotlib-handbook-10%2F</url>
    <content type="text"><![CDATA[之前有篇文章，说房间里有100个人，每人100块钱，的那个原文介绍：用数据分析告诉你这个世界很有意思 觉得挺有意思的，昨天发现pyplot也可以绘制动画，就来试试，主要是实现动画效果，其他的暂时先不考虑了目前跑起来是可以，就是比较慢，还没找到原因整体想法，就是x轴表示玩家的序号，财富值用y轴来表示 存款为0后，不在支付给别人，但可以收到别人给的钱1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import numpy as np import matplotlib.pyplot as plt import matplotlib.animation as animation #初始状态，共100人total_people = 100 #每个人的x轴坐标x = np.arange(100)#每人初始100块people = [100]*total_people#局数game_round = 0game_max_round = 17000#初始绘图fig,axes = plt.subplots()axes.bar(x,people,facecolor='green') axes.set_title(u'Round: '+str(game_round))axes.grid(True,axis='y')#根据下标i,随机返回另一个下标def give_to(i): i_to = i while i == i_to: i_to = np.random.randint(0,100) #print('from',i,'to',i_to) return i_to#重新绘制图形#1.当拥有的钱为0，则不再支出，但可以收入def game(obj): global people global game_round #还不知道咋让循环停止，就在这判断下 if game_round &lt; game_max_round: #清空当前轴 plt.cla() #遍历100个人 for i in range(total_people): #判断，当前人是否有钱 if people[i] &gt; 0 : #每个人拿出1块钱，给另一个人 people[i] = people[i] - 1 people_to = give_to(i) people[people_to] = people[people_to] + 1 #print(people) else : pass game_round += 1 #重新绘图 axes.set_title(u'Round: '+str(game_round)) axes.bar(x,sorted(people),facecolor='green') axes.grid(True,axis='y') else : pass#循环调用游戏ani = animation.FuncAnimation(fig, game, interval=0.1) plt.show() 不知道是不是代码有问题，最后有1个人那么高代码跑的很慢，随机那块不知道有没有问题，等再研究下看看优化下 可以贷款，即存款为负数12345678910111213141516171819202122232425#2.允许借贷的情况，及拥有的钱可以为负def game2(obj): global people global game_round #还不知道咋让循环停止，就在这判断下 if game_round &lt; game_max_round: #清空当前轴 plt.cla() #遍历100个人 for i in range(total_people): #每个人拿出1块钱，给另一个人 people[i] = people[i] - 1 people_to = give_to(i) people[people_to] = people[people_to] + 1 game_round += 1 #重新绘图 axes.set_title(u'Round: '+str(game_round)) axes.bar(x,sorted(people),facecolor='green') axes.grid(True,axis='y') else : pass 35岁破产后，人生的走势这里就以第6500轮游戏为基准， # -*- coding: utf-8 -*- """ Created on Fri Aug 18 11:08:44 2017 @author: yuguiyang """ import numpy as np import matplotlib.pyplot as plt import matplotlib.animation as animation #定义一个people类 class People: __data = 100 __color = 'green' def __init__(self,data=100,color='green'): self.__data = data self.__color = color def __str__(self): return str(self.__data)+','+self.__color def set_data(self,data): self.__data = data def set_color(self,color): self.__color = color def get_data(self): return self.__data def get_color(self): return self.__color def give_money(self,money=1): self.__data = self.__data - money def rcv_money(self,money=1): self.__data = self.__data + money #对数据进行排序后返回数据 def parse_people_data(): global peoples people_data = [] people_color = [] for p in sorted(peoples, key=lambda p: p.get_data()): people_data.append(p.get_data()) people_color.append(p.get_color()) return people_data,people_color #################################################### #初始状态，共100人 total_people = 100 #每个人的x轴坐标 x = np.arange(total_people) #每人初始100块,颜色是绿色 peoples = [] for i in range(total_people): peoples.append(People()) #局数 game_round = 0 game_max_round = 17000 #初始绘图 fig,axes = plt.subplots() axes.bar(x,parse_people_data()[0],color=parse_people_data()[1]) axes.set_title(u'Round: '+str(game_round)) axes.grid(True,axis='y') #根据下标i,随机返回另一个下标 def give_to(i): i_to = i while i == i_to: i_to = np.random.randint(0,100) return i_to #重新绘制图形 #1.当拥有的钱为0，则不再支出，但可以收入 #参考plt_flash_demo2 #2.允许借贷的情况，及拥有的钱可以为负 def game2(obj): global peoples global game_round #还不知道咋让循环停止，就在这判断下 if game_round &lt; game_max_round: #清空当前轴 plt.cla() #遍历100个人 for i in range(total_people): #每个人拿出1块钱，给另一个人 peoples[i].give_money() people_to = give_to(i) peoples[people_to].rcv_money() #在第6500次游戏，修改负债者的颜色 if game_round == 6500: for p in peoples : if p.get_data()&lt;0: p.set_color('red') else : pass game_round += 1 #重新绘图 axes.set_title(u'Round: '+str(game_round)) axes.bar(x,parse_people_data()[0],color=parse_people_data()[1]) axes.grid(True,axis='y') else : pass #循环调用游戏 ani = animation.FuncAnimation(fig, game2, interval=1) plt.show() 6500忘记截图了， 挺好玩儿的，还是有2个人逆袭的这里，定义了一个People类来绑定财富值和颜色，感觉可以直接用pandas DataFrame就可以解决，一会儿找时间改下看看后续再补充下其他情况]]></content>
      <categories>
        <category>数据可视化-Python&amp;R</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib手册(9) - 绘制动画]]></title>
    <url>%2F2017%2F08%2F17%2Fmatplotlib-handbook-09%2F</url>
    <content type="text"><![CDATA[matplotlib手册(9) 绘制动画 前面，我们介绍了很多绘图的方法，matplotlib不单单可以绘制静态的图，还可以制作动态的图，下面，我们就来学习下。 我们主要参考matplotlib官网的例子http://matplotlib.org/api/animation_api.html 创建动画最简单的方式，就是使用Animation的子类,就是下面的这2个 1. FuncAnimation函数介绍及主要参数1234567891011class matplotlib.animation.FuncAnimation(fig, func, frames=None, init_func=None, fargs=None, save_count=None, **kwargs)fig : matplotlib.figure.Figurefunc : callableframes : iterable, int, generator function, or None, optionalinit_func : callable, optionalfargs : tuple or None, optionalinterval : number, optionalrepeat_delay : number, optionalrepeat : bool, optional 小栗子1234567891011121314151617181920212223242526272829# -*- coding: utf-8 -*-"""Created on Thu Aug 17 18:19:08 2017@author: yuguiyang"""import numpy as np import matplotlib.pyplot as plt import matplotlib.animation as animation from datetime import datetime fig,axes = plt.subplots() axes.plot(np.random.rand(10)) #重新绘制图形def update_line(data): print(datetime.now(),'--',data) #清空当前轴 plt.cla() #重新绘图 axes.plot(np.random.rand(10))#传入的fig中，调用update_line函数，将range(3)作为参数传给update_line，1秒调用一次ani = animation.FuncAnimation(fig, update_line, 3, interval=1000) plt.show() 上面的代码，我们定义了一个函数update_line，他会清空axes，并重新绘图；FuncAnimation会每个1秒调用一次这个函数 这里记录一个小问题，暂时还没有解决 frames 参数的问题上面的例子里，我们给的是一个常量3，按照官方的介绍，是按照range(3),来一次传给函数的，但实际测试下来，发现他的调用会有问题。我们看下上面的那个输出 刚刚发现了导致这个问题的原因，注意看这个：123init_func : callable, optional A function used to draw a clear frame. If not given, the results of drawing from the first item in the frames sequence will be used. This function will be called once before the first frame. 上面，因为我们没有指定初始化函数，所以导致，会调用一次update_line，用它返回值作为初始状态，我们改下脚本再看123456def init(): print('init') #清空当前轴 plt.cla() ani = animation.FuncAnimation(fig, update_line, 3, repeat=False, interval=3000,init_func=init) 这回输出就正常了， repeat、repeat_delay这2个参数一般会配合使用，repeat默认是true，所以上面的例子会一直循环下去，如果我们改为false,第一次循环完之后就会停止。 2. ArtistAnimation12345class matplotlib.animation.ArtistAnimation(fig, artists, *args, **kwargs)artists : list Each list entry a collection of artists that represent what needs to be enabled on each frame. These will be disabled for other frames. 使用起来和上面的差不多，这里不会调用函数，而是传入一个list，123456789101112131415import numpy as np import matplotlib.pyplot as plt import matplotlib.animation as animation fig,axes = plt.subplots() ims= []for i in range(5): ims.append(axes.plot(np.random.rand(10)))im_ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=3000, blit=True)print(ims)plt.show()]]></content>
      <categories>
        <category>数据可视化-Python&amp;R</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>数据可视化</tag>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas手册（12）- 时间序列]]></title>
    <url>%2F2017%2F08%2F17%2Fpandas-handbook-12%2F</url>
    <content type="text"><![CDATA[PythonPandas 在数据分析中，时间序列应该很常见，这里，我们看看在pandas里面的使用 1. 日期和时间数据类型经常使用的datetime，time，及calendar模块123456789101112131415from datetime import datetimenow = datetime.now()nowOut[33]: datetime.datetime(2017, 8, 18, 9, 43, 46, 360886)now.yearOut[34]: 2017now.monthOut[35]: 8now.dayOut[36]: 18 datetime.timedelta表示2个datetime对象的时间差12345678910a = datetime(2018,8,18) - datetime(2018,8,17)aOut[38]: datetime.timedelta(1)a.daysOut[39]: 1a.secondsOut[40]: 0 12class datetime.timedelta(days=0, seconds=0, microseconds=0, milliseconds=0, minutes=0, hours=0, weeks=0)A timedelta object represents a duration, the difference between two dates or times. 我们可以给datetime加上或者减去一个或多个timedelta123456789from datetime import timedeltastart = datetime(2017,8,18)start + timedelta(days=-12)Out[44]: datetime.datetime(2017, 8, 6, 0, 0)start + timedelta(seconds=600)Out[45]: datetime.datetime(2017, 8, 18, 0, 10) 2. 日期转换1pandas.to_datetime(arg, errors='raise', dayfirst=False, yearfirst=False, utc=None, box=True, format=None, exact=True, unit=None, infer_datetime_format=False, origin='unix') dayfirst参数1234567#比如有时候，我们的格式，可能是day在前面，默认的话，会转换为 月/日/年pd.to_datetime(['06/08/2017','07/08/2017'])Out[29]: DatetimeIndex(['2017-06-08', '2017-07-08'], dtype='datetime64[ns]', freq=None)#使用dayfirst，指定日期，日/月/年pd.to_datetime(['06/08/2017','07/08/2017'],dayfirst=True)Out[30]: DatetimeIndex(['2017-08-06', '2017-08-07'], dtype='datetime64[ns]', freq=None) 我们可以将一个包含日期信息的DataFrame封装成datetime123456789101112131415df = pd.DataFrame(&#123;'year': [2015, 2016], 'month': [2, 3], 'day': [4, 5]&#125;)dfOut[32]: day month year0 4 2 20151 5 3 2016pd.to_datetime(df)Out[33]: 0 2015-02-041 2016-03-05dtype: datetime64[ns] 这样转换的时候，是一定需要，最少满足年月日3列，不然会报错 ValueError: to assemble mappings requires at least that [year, month, day] be specified: [day,month] is missing errors参数12345errors : &#123;‘ignore’, ‘raise’, ‘coerce’&#125;, default ‘raise’ If ‘raise’, then invalid parsing will raise an exception If ‘coerce’, then invalid parsing will be set as NaT If ‘ignore’, then invalid parsing will return the input 如果一个日期不符合规范，默认是会报错的，我们可以通过errors参数来控制12345678pd.to_datetime('13/13/2017')ValueError: month must be in 1..12pd.to_datetime('13/13/2017',errors='ignore')Out[38]: '13/13/2017'pd.to_datetime('13/13/2017',errors='coerce')Out[39]: NaT 这个NaT，就是Not a Time，在时间序列里面表示NA值123456789101112131415161718192021222324252627282930313233pd.to_datetime(1)Out[21]: Timestamp('1970-01-01 00:00:00.000000001')pd.to_datetime(10)Out[22]: Timestamp('1970-01-01 00:00:00.000000010')s1 = pd.Series(['2017-01-01','2017-02-01','2017-03-01']*3)s1Out[24]: 0 2017-01-011 2017-02-012 2017-03-013 2017-01-014 2017-02-015 2017-03-016 2017-01-017 2017-02-018 2017-03-01dtype: objectpd.to_datetime(s1)Out[25]: 0 2017-01-011 2017-02-012 2017-03-013 2017-01-014 2017-02-015 2017-03-016 2017-01-017 2017-02-018 2017-03-01dtype: datetime64[ns] 3. 时间序列基础最基本的时间序列类型，就是以时间戳为索引的Series1234567891011121314s = pd.Series(np.random.randn(3),index=[datetime(2018,8,1),datetime(2018,8,2),datetime(2018,8,3)])sOut[52]: 2018-08-01 0.3601372018-08-02 1.4225332018-08-03 -1.079172dtype: float64s.indexOut[53]: DatetimeIndex(['2018-08-01', '2018-08-02', '2018-08-03'], dtype='datetime64[ns]', freq=None)type(s)Out[54]: pandas.core.series.Series 不同索引的时间序列之间的算术运算会按时间自动对齐12345678910111213141516171819s+s[1:]Out[56]: 2018-08-01 NaN2018-08-02 2.8450662018-08-03 -2.158345dtype: float64sOut[57]: 2018-08-01 0.3601372018-08-02 1.4225332018-08-03 -1.079172dtype: float64s[1:]Out[58]: 2018-08-02 1.4225332018-08-03 -1.079172dtype: float64 4. 索引、选取、子集构造对于时间序列的处理，有很方便的方式123456789101112131415161718192021222324252627longer_ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))longer_ts.head()Out[76]: 2000-01-01 -0.6176522000-01-02 1.3312102000-01-03 0.3510122000-01-04 -1.6861232000-01-05 -0.426614Freq: D, dtype: float64longer_ts['2000-01-01':'2000-01-04']Out[77]: 2000-01-01 -0.6176522000-01-02 1.3312102000-01-03 0.3510122000-01-04 -1.686123Freq: D, dtype: float64longer_ts['01/01/2000':'01/04/2000']Out[78]: 2000-01-01 -0.6176522000-01-02 1.3312102000-01-03 0.3510122000-01-04 -1.686123Freq: D, dtype: float64 对于DataFrame来说，也是一样的1234pandas.date_range(start=None, end=None, periods=None, freq='D', tz=None, normalize=False, name=None, closed=None, **kwargs)Return a fixed frequency datetime index, with day (calendar) as the default frequencyfreq参数可以参考网址：http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases 1234567891011121314151617181920212223#从2017-01-01开始，每个周三，生成100个dates = pd.date_range('1/1/2017', periods=100, freq='W-WED')long_df = pd.DataFrame(np.random.randn(100, 4), index=dates, columns=['Colorado', 'Texas', 'New York', 'Ohio'])long_df.head()Out[92]: Colorado Texas New York Ohio2017-01-04 -1.061954 0.324256 1.532633 -0.3105992017-01-11 1.211990 -0.241169 -0.488018 -0.4730852017-01-18 0.112726 -0.021406 -0.903449 -1.1918922017-01-25 0.247537 -0.086301 1.252001 1.1196872017-02-01 -0.058160 -0.856956 -0.783473 0.081420#选取1月份所有数据long_df.loc['2017-01']Out[94]: Colorado Texas New York Ohio2017-01-04 -1.061954 0.324256 1.532633 -0.3105992017-01-11 1.211990 -0.241169 -0.488018 -0.4730852017-01-18 0.112726 -0.021406 -0.903449 -1.1918922017-01-25 0.247537 -0.086301 1.252001 1.119687]]></content>
      <categories>
        <category>Python-Pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas手册（11）- groupby]]></title>
    <url>%2F2017%2F08%2F16%2Fpandas-handbook-11%2F</url>
    <content type="text"><![CDATA[PythonPandas 这里，我们整理下pandas中关于groupby的使用，和SQL中一样，就是对数据进行聚合可以参考官方：http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.htmlhttp://pandas.pydata.org/pandas-docs/stable/groupby.html 1. groupby基本使用12DataFrame.groupby(by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=False, **kwargs)Group series using mapper (dict or key function, apply given function to group, return result as series) or by a series of columns. 12345678910111213141516import pandas as pdimport numpy as npdf = pd.DataFrame(&#123;'key1' : ['a', 'a', 'b', 'b', 'a'], 'key2' : ['one', 'two', 'one', 'two', 'one'], 'data1' : np.random.randint(0,10,5), 'data2' : np.random.randint(0,10,5)&#125;)dfOut[158]: data1 data2 key1 key20 2 4 a one1 3 8 a two2 6 5 b one3 7 3 b two4 7 6 a one 我们现在，根据key1来groupby1234a = df.groupby(by=['key1'])aOut[170]: &lt;pandas.core.groupby.DataFrameGroupBy object at 0x000000000B8317F0&gt; 我们可以看到，返回值是一个DataFrameGroupBy对象，这只是一个中间数据，还没有进行真正的聚合这里有一个概念”split-apply-combine”，拆分-应用-合并，感觉和MapReduce的概念差不多，这个的groupby就是做了拆分我们可以遍历DataFrameGroupBy，12345678910111213for k,v in a: print('k:',k) print('v:',v) k: av: data1 data2 key1 key20 2 4 a one1 3 8 a two4 7 6 a onek: bv: data1 data2 key1 key22 6 5 b one3 7 3 b two 这个就是将内容进行了拆分,当我们在调用统计函数时，才会执行应用和合并12345678910111213141516171819202122232425262728293031323334a.sum()Out[172]: data1 data2key1 a 12 18b 13 8a.size()Out[173]: key1a 3b 2dtype: int64a.count()Out[174]: data1 data2 key2key1 a 3 3 3b 2 2 2a.max()Out[175]: data1 data2 key2key1 a 7 8 twob 7 5 twoa.mean()Out[176]: data1 data2key1 a 4.0 6.0b 6.5 4.0 我们可以按2个值进行聚合1234567891011121314151617dfOut[188]: data1 data2 key1 key20 2 4 a one1 3 8 a two2 6 5 b one3 7 3 b two4 7 6 a onedf.groupby(by=['key1','key2']).sum()Out[189]: data1 data2key1 key2 a one 9 10 two 3 8b one 6 5 two 7 3 默认的话，会将数值类型的字段做聚合，我们也可以选择1234567891011121314151617df.groupby(by=['key1','key2'])['data1'].sum()Out[190]: key1 key2a one 9 two 3b one 6 two 7Name: data1, dtype: int32df.groupby(by=['key1','key2'])['data1','data2'].sum()Out[191]: data1 data2key1 key2 a one 9 10 two 3 8b one 6 5 two 7 3 下面的写法也是同样的，前面我们是直接传入的列名，这里我们传入series也可以123456789101112131415df.groupby(by=df['key1']).sum()Out[197]: data1 data2key1 a 12 18b 13 8df.groupby(by=[df['key1'],df['key2']]).sum()Out[198]: data1 data2key1 key2 a one 9 10 two 3 8b one 6 5 two 7 3 上面我们传入的都是当前df的序列，这里也可以传入新的，这里只要长度符合就行了，感觉就是把它当成新列来处理12345df.groupby(by=['lufei','lufei','lufei','lufei','namei']).sum()Out[199]: data1 data2lufei 18 20namei 7 6 123by : mapping, function, str, or iterable Used to determine the groups for the groupby. If by is a function, it’s called on each value of the object’s index. If a dict or Series is passed, the Series or dict VALUES will be used to determine the groups (the Series’ values are first aligned; see .align() method). If an ndarray is passed, the values are used as-is determine the groups. A str or list of strs may be passed to group by the columns in self 这个by参数，还可以接收一个dict，像这样：1234567891011121314151617181920212223242526dfOut[204]: data1 data2 key1 key20 2 4 a one1 3 8 a two2 6 5 b one3 7 3 b two4 7 6 a onedf.indexOut[205]: RangeIndex(start=0, stop=5, step=1)dfOut[206]: data1 data2 key1 key20 2 4 a one1 3 8 a two2 6 5 b one3 7 3 b two4 7 6 a one#默认是根据啊axis=0，所以groupby之前会先将index和dict进行映射，df.groupby(&#123;0:'a',1:'a',2:'a',3:'a',4:'a'&#125;).sum()Out[207]: data1 data2a 25 26 这里，对于series也是一样的，series也有index，更厉害的是，这里还可以使用函数进行分组，函数会在各个索引值上调用一次，然后根据返回值来用作分组名称123456789101112131415161718dfOut[216]: data1 data2 key1 key20 2 4 a one1 3 8 a two2 6 5 b one3 7 3 b two4 7 6 a one#会把每一个index的值加10，然后再聚合df.groupby(lambda x:x+10).sum()Out[217]: data1 data210 2 411 3 812 6 513 7 314 7 6 —————update at 2017-08-23这里继续整理下pandas中groupby的使用 2. 面向列的多函数应用 上面，我们再对列做聚合的时候，都是使用使用统一的函数，比如sum(),count(),都是一起的，在pandas中，我们可以同时调用多个函数，主要是使用agg12345678910111213DataFrameGroupBy.agg(arg, *args, **kwargs)Aggregate using callable, string, dict, or list of string/callablesfunc : callable, string, dictionary, or list of string/callables Function to use for aggregating the data. If a function, must either work when passed a DataFrame or when passed to DataFrame.apply. For a DataFrame, can pass a dict, if the keys are DataFrame column names. Accepted Combinations are: string function name function list of functions dict of column names -&gt; functions (or list of functions) 小栗子12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152df = pd.DataFrame(&#123;'A': [1, 1, 2, 2], 'B': [1, 2, 3, 4], 'C': np.random.randn(4)&#125;)dfOut[64]: A B C0 1 1 0.4330761 1 2 0.5097642 2 3 -1.0913183 2 4 -0.696079df.groupby(by=['A']).min()Out[65]: B CA 1 1 0.4330762 3 -1.091318#使用agg，调用min函数，和直接调用时等价的df.groupby(by=['A']).agg('min')Out[66]: B CA 1 1 0.4330762 3 -1.091318#还可以传入一个函数数组，同时调用min和maxdf.groupby(by=['A']).agg(['min','max'])Out[67]: B C min max min maxA 1 1 2 0.433076 0.5097642 3 4 -1.091318 -0.696079df.groupby(by=['A'])['B'].agg(['min','max'])Out[68]: min maxA 1 1 22 3 4#还可以通过传入一个dict，来对不同的列做不同的操作，列名为key，func为valuedf.groupby(by=['A']).agg(&#123;'B':['min','max'],'C':['sum']&#125;)Out[69]: B C min max sumA 1 1 2 0.9428402 3 4 -1.787397 上面，我们传入函数，默认会用我们的函数名来做列名，但，有时我们想要自定义，我们通过传入一个（name,function）的列表12345678910111213141516df.groupby(by=['A']).agg([('the_min_data','min'),('the_max_data','max')])Out[73]: B C the_min_data the_max_data the_min_data the_max_dataA 1 1 2 0.433076 0.5097642 3 4 -1.091318 -0.696079#可以随意组合df.groupby(by=['A']).agg(&#123;'B':['min','max'],'C':[('hey_sum','sum')]&#125;)Out[74]: B C min max hey_sumA 1 1 2 0.9428402 3 4 -1.787397 3. 已无索引形式返回聚合数据前面，我们groupby之后，都是用聚合建来当做index，我们可以通过参数as_index=False,来取消123456789101112131415161718#会默认生成一个新indexdf.groupby(by=['A','B'],as_index=False).max()Out[80]: A B C0 1 1 0.4330761 1 2 0.5097642 2 3 -1.0913183 2 4 -0.696079df.groupby(by=['A','B'],as_index=True).max()Out[81]: CA B 1 1 0.433076 2 0.5097642 3 -1.091318 4 -0.696079 做了练习之后，这里发现，直接调用函数是好用的，但是，如果使用agg来调用，是不好用的这个参数123456789df.groupby(by=['A','B'],as_index=False).agg(['min','max'])Out[79]: C min maxA B 1 1 0.433076 0.433076 2 0.509764 0.5097642 3 -1.091318 -1.091318 4 -0.696079 -0.696079]]></content>
      <categories>
        <category>Python-Pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫小实例-1688物流信息之发货地信息获取]]></title>
    <url>%2F2017%2F08%2F15%2Fcrawler-demo-02%2F</url>
    <content type="text"><![CDATA[这里写个例子，公司的一个大神之前爬过1688上面的物流信息，这里也来试一下，顺便分享下学习过程。 背景介绍目标网页：https://56.1688.com/order/price/estimate_price.htm 目的：抓取网站上所有的线路信息，保存到文件或数据库中。 实践步骤先观察下网站的特征，随便查询一下看看，主要看看URL是怎么传递参数的，通过FireBug，下面几个参数是不为空的 通过查看网页源码，发现上面的参数，就是发货地和收获地的省份、城市、区县的编码 既然我们要获取所有的线路数据，那第一步就是获取这里所有的发货地、收货地信息 一开始以为这个编码是1688上自定义的，后来查了一下，发现是官方的编码，这也是为啥，前面会写一篇《最新行政区信息获取》 所以，这里最方便的办法就是直接使用官方的编码，第一步就解决了；但是前面呢，既然要学习爬虫，就用爬虫来试试，正好前面学了selenium，这里就熟练掌握一下吧。实践的时候呢，还是遇到了一些问题比如：默认省份信息是隐藏的（display: none）,城市和区县信息的动态的切换的 这个应该使用JS来控制的，但是一直没找到是哪段JS还好selenium中可以模拟单击事件，很方便下面是代码，该有的注释都在，效率比较差，也就3000多条记录，还好跑一次就可以了 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697# -*- coding: utf-8 -*-"""Created on Mon Aug 14 21:20:56 2017@author: yuguiyang"""from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.common.exceptions import TimeoutException#打开浏览器browser = webdriver.Firefox()#设置等待时长，最长等待10swait = WebDriverWait(browser,10)#单击省份标签def click_province(): #这里不点击的话，下面可以通过属性获取省份名称，但是直接使用text是不行的 li_labels = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR,'#source-area-select ul.h li span.inner'))) li_labels[1].click()#单击城市标签 def click_city(): li_labels = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR,'#source-area-select ul.h li span.inner'))) li_labels[2].click() def main(): #打开URL browser.get('https://56.1688.com/order/price/estimate_price.htm') #输出浏览器标题 print('browser title: ',browser.title) #单击发货输入框，显示城市选择的标签 input_start = wait.until(EC.presence_of_element_located((By.ID,'source-area'))) input_start.click() #单击省份标签 click_province() #区域信息 div_tabs = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR,'#source-area-select div.s-tab-b'))) #只使用省份 li_provinces = div_tabs[1].find_elements(By.CSS_SELECTOR ,'a.panel-item') file = open('1688_line.txt','w') #遍历每一个省份 for pro in li_provinces: #单击当前的省份，页面会跳转到该省份的城市列表 pro.click() #获取城市信息 div_tabs = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR,'#source-area-select div.s-tab-b'))) li_citys = div_tabs[2].find_elements(By.CSS_SELECTOR ,'a.panel-item') data = [] #遍历每一个城市 for city in li_citys: #单击当前城市标签，页面会跳转到该城市下的区县列表 city.click() #获取区县信息 div_tabs = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR,'#source-area-select div.s-tab-b'))) li_areas = div_tabs[3].find_elements(By.CSS_SELECTOR ,'a.panel-item') #遍历每一个区县 for area in li_areas: data.append(pro.get_attribute('code')+','+pro.get_attribute('panel-item') +','+city.get_attribute('code')+','+city.get_attribute('panel-item') +','+area.get_attribute('code')+','+area.get_attribute('panel-item') +'\n' ) #跳转回到城市标签，为了遍历下一个城市 click_city() #将数据写入文件 file.writelines(data) #跳转回省份标签，为了遍历下一个省份 click_province() file.close() browser.quit()if __name__=='__main__': main()]]></content>
      <categories>
        <category>Python-爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas手册（10）- 数据转换]]></title>
    <url>%2F2017%2F08%2F15%2Fpandas-handbook-10%2F</url>
    <content type="text"><![CDATA[PythonPandas 这里接着上一篇，继续记录下pandas中数据处理方面的函数 1. 重复数据结果集中，可能会有重复数据，有函数可以做去重操作 1234567#判断数据是否重复DataFrame.duplicated(subset=None, keep='first')Return boolean Series denoting duplicate rows, optionally only considering certain columns#删除重复记录DataFrame.drop_duplicates(subset=None, keep='first', inplace=False)Return DataFrame with duplicate rows removed, optionally only considering certain columns 1234567891011121314151617181920212223242526272829303132333435363738394041424344data = pd.DataFrame(&#123;'K1':['one']*3+['two']*4,'k2':[1,1,2,3,3,4,4]&#125;)dataOut[100]: K1 k20 one 11 one 12 one 23 two 34 two 35 two 46 two 4#keep参数，默认是first，会以第一个为准，后面再出现就是Truedata.duplicated()Out[103]: 0 False1 True2 False3 False4 True5 False6 Truedtype: booldata.duplicated(keep='last')Out[116]: 0 True1 False2 False3 True4 False5 True6 Falsedtype: bool#这里drop也是一样的，可以用keep控制保留第一次出现的，还是最后1次出现的data.drop_duplicates()Out[104]: K1 k20 one 12 one 23 two 35 two 4 这2个函数，默认都是判断全部的列，也可以指定列12345678910111213141516171819202122232425262728293031323334353637383940data['v1']=range(7)dataOut[118]: K1 k2 v10 one 1 01 one 1 12 one 2 23 two 3 34 two 3 45 two 4 56 two 4 6data.duplicated()Out[119]: 0 False1 False2 False3 False4 False5 False6 Falsedtype: booldata.duplicated(subset=['K1'])Out[120]: 0 False1 True2 True3 False4 True5 True6 Truedtype: booldata.drop_duplicates(subset=['K1'])Out[121]: K1 k2 v10 one 1 03 two 3 3 2. 利用函数或映射进行数据转换有的时候，我们想要对Series和dataframe中的每个元素做些转换，就用到了这个函数，和Python内置的map函数类似123456Series.map(arg, na_action=None)Map values of Series using input correspondence (which can be a dict, Series, or function)arg : function, dict, or SeriesDataFrame.applymap(func)Apply a function to a DataFrame that is intended to operate elementwise, i.e. like doing map(func, series) for each series in the DataFrame 12345678910111213141516171819202122232425262728293031323334353637383940414243s1 = pd.Series(range(10))s1Out[133]: 0 01 12 23 34 45 56 67 78 89 9dtype: int32s1.map(lambda x:x+10)Out[134]: 0 101 112 123 134 145 156 167 178 189 19dtype: int64s1.map(lambda x:x*-1)Out[135]: 0 01 -12 -23 -34 -45 -56 -67 -78 -89 -9dtype: int64 这个map，不单单可以是函数，还可以是 Series或dict123456789101112131415161718192021222324252627282930313233x = pd.Series([1,2,3], index=['one', 'two', 'three'])y = pd.Series(['foo', 'bar', 'baz'], index=[1,2,3])xOut[142]: one 1two 2three 3dtype: int64yOut[143]: 1 foo2 bar3 bazdtype: objectx.map(y)Out[144]: one footwo barthree bazdtype: objectz = &#123;1: 'A', 2: 'B', 3: 'C'&#125;x.map(z)Out[146]: one Atwo Bthree Cdtype: object 3.替换值在字符串中，我们经常会用到replace函数，用来替换指定的值，pandas中也有123456DataFrame.replace(to_replace=None, value=None, inplace=False, limit=None, regex=False, method='pad', axis=None)Replace values given in ‘to_replace’ with ‘value’.to_replace : str, regex, list, dict, Series, numeric, or NoneSeries.replace(to_replace=None, value=None, inplace=False, limit=None, regex=False, method='pad', axis=None)Replace values given in ‘to_replace’ with ‘value’. 12345678910111213141516171819202122232425262728293031323334353637383940414243data = pd.Series([1., -999., 2., -999., -1000., 3.])dataOut[150]: 0 1.01 -999.02 2.03 -999.04 -1000.05 3.0dtype: float64#我们将-999这个异常值替换掉data.replace(-999,np.nan)Out[151]: 0 1.01 NaN2 2.03 NaN4 -1000.05 3.0dtype: float64#同时替换多个值，也可以data.replace([-999,-1000],np.nan)Out[152]: 0 1.01 NaN2 2.03 NaN4 NaN5 3.0dtype: float64data.replace([-999,-1000],[np.nan,0])Out[153]: 0 1.01 NaN2 2.03 NaN4 0.05 3.0dtype: float64]]></content>
      <categories>
        <category>Python-Pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最新行政区信息获取]]></title>
    <url>%2F2017%2F08%2F14%2Fthe-newest-area-info-01%2F</url>
    <content type="text"><![CDATA[这是之前记录的，这里顺便分享下。 之前想要获取官方的省市区的代码code，就找了下。 官方地址：http://www.stats.gov.cn/tjsj/tjbz/xzqhdm/ 我们可以直接将数据复制到Excel中，简单处理下，导入到数据库中 里面会有换行，先去个重，然后trim一下，再分列就可以了 最后，我们可以将数据组织成我们想要的维度表格式 ————–update at 2017-08-15 上面呢，我是直接在Excel里面生成insert脚本插入数据库的，但是目前的表使用起来应该不是很方便，这里我们改造下。 这个区域信息呢，是很常用的一张维度表，通常他可能会是这个样子的123456789101112131415CREATE TABLE base.dm_area( area_id integer, area_name character varying(50), province_id integer, province_name character varying(50), city_id integer, city_name character varying(50), country_id integer, country_name character varying(50), flevel integer)WITH ( OIDS=FALSE); 这里我们就简单处理下，变成这种格式。 其实一开始的时候，我们可以通过数据的格式区分他是省份、城市、还是区县，下面我们用的是一个套路 主要是观察法，因为这个code都是有规律的，所以我们处理起来方便很多 code一共是6位，前两位是省份、中间2位是城市，后2位是区县1select *from public.b_area where area_id||'' like '00' 下面，我们来初始化数据12345678910111213141516171819202122232425262728293031323334353637INSERT INTO base.dm_area( area_id, area_name)select *from public.b_area; -- 初始化flevel 1:省份,2:城市,3:区县update base.dm_area set flevel=1 where area_id||'' like '00';update base.dm_area set flevel=2 where area_id||'' like '' and flevel is null;update base.dm_area set flevel=3 where flevel is null;-- 更新省份信息update base.dm_area a set province_id = b.area_id, province_name = b.area_namefrom base.dm_area bwhere left(a.area_id||'',2)=left(b.area_id||'',2) and b.flevel=1;-- 更新城市信息update base.dm_area a set city_id = c.area_id, city_name = c.area_name from base.dm_area c where left(a.area_id||'',4)=left(c.area_id||'',4) and c.flevel=2;-- 更新区县信息update base.dm_area a set country_id = area_id, country_name = area_name where a.flevel=3; 到这里，flevel=3的数据已经没有问题了，再对省份、城市做些优化就行了12345678910111213-- 补全数据update base.dm_area set city_id=-area_id,city_name=area_name||'XT', country_id=-area_id,country_name=area_name||'XT'where flevel=1;update base.dm_area set country_id=-area_id,country_name=area_name||'XT'where flevel=2; 好了，我们就处理好这个区域维度了，正常使用的话，估计就用flevel=3的就够了]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas手册（9）- 数据合并与连接]]></title>
    <url>%2F2017%2F08%2F14%2Fpandas-handbook-09%2F</url>
    <content type="text"><![CDATA[PythonPandas 这里来看一下，pandas中数据转换与合并的使用方法，刚刚学习了一下，很好用，就跟SQL里面一样。 #1. 合并数据集就是说，我们有2个数据集，想要将他们合并一下，就是SQL里面的关联查询，pandas里面用一个函数就行了12345DataFrame.merge(right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=True, indicator=False)Merge DataFrame objects by performing a database-style join operation by columns or indexes.If joining columns on columns, the DataFrame indexes will be ignored. Otherwise if joining indexes on indexes or indexes on a column or columns, the index will be passed on. 熟练掌握几个参数就足够了，下面会依次介绍下小例子123456789101112131415161718192021222324import pandas as pdimport numpy as npa = pd.DataFrame(&#123;'key':list('bbccaa'),'data1':np.random.randint(0,10,size=6)&#125;)b = pd.DataFrame(&#123;'key':list('abc'),'data2':np.random.randint(0,10,size=3)&#125;)aOut[18]: data1 key0 8 b1 0 b2 5 c3 2 c4 3 a5 6 abOut[19]: data2 key0 0 a1 3 b2 2 c 上面是我们的原始数据集，一个a，一个b，key是相同的字段，可以用来关联，123456789a.merge(b)Out[20]: data1 key data20 8 b 31 0 b 32 5 c 23 2 c 24 3 a 05 6 a 0 这个翻译成SQL，就是a join b on a.key=b.key（因为我们没有指定根据什么字段去关联，所以会使用a、b中名字一样的字段去关联）我们当然可以手动指定关联的字段1234567891011on : label or list Field names to join on. Must be found in both DataFrames. If on is None and not merging on indexes, then it merges on the intersection of the columns by default.left_on : label or list, or array-like Field names to join on in left DataFrame. Can be a vector or list of vectors of the length of the DataFrame to use a particular vector as the join key instead of columnsright_on : label or list, or array-like Field names to join on in right DataFrame or vector/list of vectors per left_on docs 如果，数据集中，关联字段名称一样，直接使用on就行了，如果不一样，就可以分别使用left_on 和right_on12345678910111213141516171819a.merge(b,on='key')Out[21]: data1 key data20 8 b 31 0 b 32 5 c 23 2 c 24 3 a 05 6 a 0a.merge(b,left_on='key',right_on='key')Out[22]: data1 key data20 8 b 31 0 b 32 5 c 23 2 c 24 3 a 05 6 a 0 如果关联字段又多个，就指定为数组就行了123456789101112131415161718192021222324252627282930313233343536373839404142c = pd.DataFrame(&#123;'lkey1':list('ab'),'lkey2':list('xy'),'ldata1':np.random.randint(0,10,size=2)&#125;)d = pd.DataFrame(&#123;'rkey1':list('ab'),'rkey2':list('xy'),'ldata1':np.random.randint(0,10,size=2)&#125;)cOut[25]: ldata1 lkey1 lkey20 8 a x1 2 b ydOut[26]: ldata1 rkey1 rkey20 5 a x1 7 b y#因为找不到名字一样的字段做关联，所以是空的结果（有一个字段是一样的，ldata1，但是都一样，刚刚d的名字忘改了......）#正常找不到关联字段，是会报错的c.merge(d)Out[27]: Empty DataFrameColumns: [ldata1, lkey1, lkey2, rkey1, rkey2]Index: []#我们将组合建传入，即可c.merge(d,left_on=['lkey1','lkey2'],right_on=['rkey1','rkey2'])Out[28]: ldata1_x lkey1 lkey2 ldata1_y rkey1 rkey20 8 a x 5 a x1 2 b y 7 b y#看下结果集，我们会发现，c和d中有一个同名的字段ldata1，这里默认加了后缀 _x，_y来表示区分#我们也可以通过参数来指定后缀名称suffixes : 2-length sequence (tuple, list, ...) Suffix to apply to overlapping column names in the left and right side, respectivelyc.merge(d,left_on=['lkey1','lkey2'],right_on=['rkey1','rkey2'],suffixes=['_left','_right'])Out[29]: ldata1_left lkey1 lkey2 ldata1_right rkey1 rkey20 8 a x 5 a x1 2 b y 7 b y 我们再看下一个例子123456789101112131415161718192021222324252627282930313233e = pd.DataFrame(&#123;'key':list('bbccaadd'),'data':np.random.randint(0,10,size=8)&#125;)f = pd.DataFrame(&#123;'key':list('abcx'),'data':np.random.randint(0,10,size=4)&#125;)eOut[32]: data key0 4 b1 5 b2 1 c3 2 c4 7 a5 7 a6 7 d7 9 dfOut[33]: data key0 6 a1 9 b2 2 c3 8 xe.merge(f,on='key')Out[34]: data_x key data_y0 4 b 91 5 b 92 1 c 23 2 c 24 7 a 65 7 a 6 熟悉SQL的同学，会发现，上面的结果集市inner join之后的结果，SQL中，还有什么left join、right join之类的，pandas中也有的123456how : &#123;‘left’, ‘right’, ‘outer’, ‘inner’&#125;, default ‘inner’ left: use only keys from left frame, similar to a SQL left outer join; preserve key order right: use only keys from right frame, similar to a SQL right outer join; preserve key order outer: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically inner: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys 123456789101112131415161718192021222324252627282930313233343536373839#左关联，e为主即包含全部数据，不管是否可以和f关联上e.merge(f,on='key',how='left')Out[35]: data_x key data_y0 4 b 9.01 5 b 9.02 1 c 2.03 2 c 2.04 7 a 6.05 7 a 6.06 7 d NaN7 9 d NaN#右为主，即f为主，e.merge(f,on='key',how='right')Out[36]: data_x key data_y0 4.0 b 91 5.0 b 92 1.0 c 23 2.0 c 24 7.0 a 65 7.0 a 66 NaN x 8#所有数据都包含e.merge(f,on='key',how='outer')Out[37]: data_x key data_y0 4.0 b 9.01 5.0 b 9.02 1.0 c 2.03 2.0 c 2.04 7.0 a 6.05 7.0 a 6.06 7.0 d NaN7 9.0 d NaN8 NaN x 8.0 前面，我们的例子，都是通过columns来关联的，有的时候，我们可能需要使用index来关联，者就用到了另2个参数1234567left_index : boolean, default False Use the index from the left DataFrame as the join key(s). If it is a MultiIndex, the number of keys in the other DataFrame (either the index or a number of columns) must match the number of levelsright_index : boolean, default False Use the index from the right DataFrame as the join key. Same caveats as left_index 12345678910111213141516171819202122232425262728293031eOut[38]: data key0 4 b1 5 b2 1 c3 2 c4 7 a5 7 a6 7 d7 9 dg = pd.DataFrame(&#123;'data':np.random.randint(0,10,size=3)&#125;,index=list('abc'))gOut[40]: dataa 9b 4c 0#指定right_index=True，使用index去关联e.merge(g,left_on='key',right_index=True)Out[41]: data_x key data_y0 4 b 41 5 b 42 1 c 03 2 c 04 7 a 95 7 a 9 2. 轴向连接这里主要是介绍pandas中另一个函数的使用，pd.concat，concat一看上去，感觉是做拼接用的12345pandas.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, copy=True)Concatenate pandas objects along a particular axis with optional set logic along the other axes.Can also add a layer of hierarchical indexing on the concatenation axis, which may be useful if the labels are the same (or overlapping) on the passed axis number. 我们先来看例子1234567891011121314151617181920212223s1 = pd.Series(['a', 'b'])s2 = pd.Series(['c', 'd'])s1Out[51]: 0 a1 bdtype: objects2Out[52]: 0 c1 ddtype: objectpd.concat([s1,s2])Out[53]: 0 a1 b0 c1 ddtype: object 默认，是按纵轴进行拼接的，我们可以设置 123456#横轴进行拼接pd.concat([s1,s2],axis=1)Out[54]: 0 10 a c1 b d 这里要注意下axis=1时，，如果index不一样，拼接的时候，是会合并的，如下面的例子12345678910111213141516171819202122232425262728293031323334353637383940414243444546s1 = pd.Series([0,1],index=['a','b'])s2 = pd.Series([2,3,4],index=['c','d','e'])s3 = pd.Series([5,6],index=['f','g'])pd.concat([s1,s2,s3])Out[58]: a 0b 1c 2d 3e 4f 5g 6dtype: int64s1Out[60]: a 0b 1dtype: int64s2Out[61]: c 2d 3e 4dtype: int64s3Out[62]: f 5g 6dtype: int64pd.concat([s1,s2,s3],axis=1)Out[59]: 0 1 2a 0.0 NaN NaNb 1.0 NaN NaNc NaN 2.0 NaNd NaN 3.0 NaNe NaN 4.0 NaNf NaN NaN 5.0g NaN NaN 6.0 这里，我们看个常用的参数，join，可以选择是取交集还是并集123join : &#123;‘inner’, ‘outer’&#125;, default ‘outer’ How to handle indexes on other axis(es) 12345678910111213141516171819202122232425262728293031323334353637#他们并没哟并集，所以是空pd.concat([s1,s2,s3],axis=1,join='inner')Out[73]: Empty DataFrameColumns: [0, 1, 2]Index: []s4 = pd.concat([s1*5,s3])s1Out[75]: a 0b 1dtype: int64s4Out[76]: a 0b 5f 5g 6dtype: int64pd.concat([s1,s4],axis=1)Out[77]: 0 1a 0.0 0b 1.0 5f NaN 5g NaN 6#只返回了交集部分pd.concat([s1,s4],axis=1,join='inner')Out[78]: 0 1a 0 0b 1 5 我们也可以指明其他轴要使用的索引，要显示的index1234567891011join_axes : list of Index objects Specific indexes to use for the other n - 1 axes instead of performing inner/outer set logicpd.concat([s1,s4],axis=1,join_axes=[['a','b','c','d']])Out[81]: 0 1a 0.0 0.0b 1.0 5.0c NaN NaNd NaN NaN 3. 合并重叠数据这里是另一个函数的使用介绍 combine_first，类似于numpy中where，123DataFrame.combine_first(other)Combine two DataFrame objects and default to non-null values in frame calling the method. Result index columns will be the union of the respective indexes and columns 12345678910111213141516171819202122232425262728293031323334353637383940414243a = pd.Series([np.nan,2.5,np.nan,3.5,4.5,np.nan] ,index=['f','e','d','c','b','a'])b = pd.Series(np.arange(len(a),dtype=np.float64) ,index=['f','e','d','c','b','a'])b[-1]=np.nanaOut[85]: f NaNe 2.5d NaNc 3.5b 4.5a NaNdtype: float64bOut[88]: f 0.0e 1.0d 2.0c 3.0b 4.0a NaNdtype: float64#如果a的字段为nan，则用b的数据np.where(a.isnull(),b,a)Out[92]: array([ 0. , 2.5, 2. , 3.5, 4.5, nan])a.combine_first(b)Out[93]: f 0.0e 2.5d 2.0c 3.5b 4.5a NaNdtype: float64 combine_first还会做数据对齐的操作12345678910111213141516171819202122232425b[:-2]Out[95]: f 0.0e 1.0d 2.0c 3.0dtype: float64a[2:]Out[96]: d NaNc 3.5b 4.5a NaNdtype: float64b[:-2].combine_first(a[2:])Out[94]: a NaNb 4.5c 3.0d 2.0e 1.0f 0.0dtype: float64]]></content>
      <categories>
        <category>Python-Pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib手册(8) - 绘图知识点杂记]]></title>
    <url>%2F2017%2F08%2F12%2Fmatplotlib-handbook-08%2F</url>
    <content type="text"><![CDATA[matplotlib手册(8) 这里会整理些matplotlib绘图的小知识点，可能比较杂。 1. subplot前面，我们有简单的说过subplot，通过他，我们可以在一个figure上，分割多个区域分别去绘图，这里说一个最常用的方法1234matplotlib.pyplot.subplots(nrows=1, ncols=1, sharex=False, sharey=False, squeeze=True, subplot_kw=None, gridspec_kw=None, **fig_kw)Create a figure and a set of subplotsThis utility wrapper makes it convenient to create common layouts of subplots, including the enclosing figure object, in a single call 12345678910import numpy as npimport matplotlib.pyplot as plt#初始化2行2列的figuref,axes = plt.subplots(2,2)#这样通过下标可以快速的获取axesaxes[0,1].hist(np.random.randn(100),bins=20)plt.show() 我们还可以设置，是否共享x轴y轴 调整subplot周围的间距12345678910111213matplotlib.pyplot.subplots_adjust(*args, **kwargs)subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)left = 0.125 # the left side of the subplots of the figureright = 0.9 # the right side of the subplots of the figurebottom = 0.1 # the bottom of the subplots of the figuretop = 0.9 # the top of the subplots of the figurewspace = 0.2 # the amount of width reserved for blank space between subplots, # expressed as a fraction of the average axis widthhspace = 0.2 # the amount of height reserved for white space between subplots, # expressed as a fraction of the average axis height 1234567891011121314151617181920212223242526import numpy as npimport matplotlib.pyplot as plt#初始化2行2列的figuref,axes = plt.subplots(2,2)data=np.random.randn(10).cumsum()axes[0,0].plot(data,marker='o')axes[0,0].set_title('default')#drawstyle [‘default’ | ‘steps’ | ‘steps-pre’ | ‘steps-mid’ | ‘steps-post’]axes[0,1].plot(data,drawstyle='steps-pre',marker='o')axes[0,1].set_title('steps-pre')axes[1,0].plot(data,drawstyle='steps-mid',marker='o')axes[1,0].set_title('steps-mid')axes[1,1].plot(data,drawstyle='steps-post',marker='o')axes[1,1].set_title('steps-post')#plt.subplots_adjust(hspace=0)plt.show() 我们这里，可以设置轴要显示的刻度比如，上面显示的是0，2，4，6，81234#设置显示的x轴的值axes[0,0].set_xticks([0,5,10])#将上面的值，替换为标签axes[0,0].set_xticklabels(['one','two','three']) 好了，这里先补充这些，后面有的话继续，下一篇整理下pandas中绘图的方法]]></content>
      <categories>
        <category>数据可视化-Python&amp;R</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>数据可视化</tag>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib手册(7) - 折线图和曲线图]]></title>
    <url>%2F2017%2F08%2F12%2Fmatplotlib-handbook-07%2F</url>
    <content type="text"><![CDATA[matplotlib手册(7) 这回，我么来看看折线图和曲线图，其实一开始的时候，我们掌握的就是折线图来回忆下之前的内容，先看个小例子123456789101112131415161718192021import numpy as npimport matplotlib.pyplot as pltplt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签 plt.rcParams['axes.unicode_minus']=False #用来正常显示负号x = np.arange('2017-08-01','2017-08-10',dtype=np.datetime64)y = np.random.randint(10,100,size=9)y2 = np.random.randint(10,100,size=9)plt.plot(x,y,color='red',label='APP')plt.plot(x,y2,color='blue',label='PC')plt.title(u'每日登录用户数')plt.xlabel(u'日期')plt.ylabel(u'登录人数')plt.legend()plt.show() 就是使用plot直接绘制就行了这次，我们使用了稍微有点儿意义的数据，下面，我们再看个曲线图，以前数学里面，最常见的曲线是啥呢？可能就是那个正弦函数之类的（回忆一下，有点儿懵，忘了都）1234567891011121314import numpy as npimport matplotlib.pyplot as pltplt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签 plt.rcParams['axes.unicode_minus']=False #用来正常显示负号x = np.linspace(-2*np.pi,2*np.pi,1000)y1,y2 = np.sin(x),np.cos(x)plt.plot(x,y1,color='red',label='sin(x)')plt.plot(x,y2,color='blue',label='cos(x)')plt.legend()plt.show() 就线条和标记，有很多常用的参数 哈，刚刚想到一个好玩儿的功能点，最近一直没看到，但是可视化的时候应该很常见，就是在每一个点上，显示他的数值试了下，还可以的1234567891011121314151617181920212223242526import numpy as npimport matplotlib.pyplot as pltplt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签 plt.rcParams['axes.unicode_minus']=False #用来正常显示负号x = np.arange('2017-08-01','2017-08-10',dtype=np.datetime64)y = np.random.randint(10,100,size=9)y2 = np.random.randint(10,100,size=9)plt.plot(x,y,color='red',label='APP')plt.plot(x,y2,color='blue',label='PC')plt.title(u'每日登录用户数')plt.xlabel(u'日期')plt.ylabel(u'登录人数')#遍历每一个点，使用text将y值显示for i,j in list(zip(x,y)): plt.text(i,j+1,j,fontsize=12)plt.legend()plt.show() 这样可以实现，但是展示起来不是特别的友好，不知道有没有其他的好办法，后面再看看的。]]></content>
      <categories>
        <category>数据可视化-Python&amp;R</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>数据可视化</tag>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas手册（8）- 常见绘图]]></title>
    <url>%2F2017%2F08%2F12%2Fpandas-handbook-08%2F</url>
    <content type="text"><![CDATA[PythonPandas 前面，我们大概了解了matplotlib中基本的绘图方式，现在，我们来看看在pandas中绘图的方式，pandas做好了封装，我们用起来会很方便的。123456789101112131415Series.plot(kind='line', ax=None, figsize=None, use_index=True, title=None, grid=None, legend=False, style=None, logx=False, logy=False, loglog=False, xticks=None, yticks=None, xlim=None, ylim=None, rot=None, fontsize=None, colormap=None, table=False, yerr=None, xerr=None, label=None, secondary_y=False, **kwds)#这个kind可以指定图表类型 ‘line’ : line plot (default) ‘bar’ : vertical bar plot ‘barh’ : horizontal bar plot ‘hist’ : histogram ‘box’ : boxplot ‘kde’ : Kernel Density Estimation plot ‘density’ : same as ‘kde’ ‘area’ : area plot ‘pie’ : pie plotDataFrame.plot(x=None, y=None, kind='line', ax=None, subplots=False, sharex=None, sharey=False, layout=None, figsize=None, use_index=True, title=None, grid=None, legend=True, style=None, logx=False, logy=False, loglog=False, xticks=None, yticks=None, xlim=None, ylim=None, rot=None, fontsize=None, colormap=None, table=False, yerr=None, xerr=None, secondary_y=False, sort_columns=False, **kwds) 1. 线形图12345678import pandas as pdimport numpy as nps = pd.Series(np.random.randint(0,100,size=10))print(s)s.plot(title='demo-series',label='count',legend=True) 123456789import pandas as pdimport numpy as npdf = pd.DataFrame(np.random.randn(10,4)*100,index=np.arange(0,100,10), columns=list('ABCD'))print(df)df.plot() DataFrame绘图的时候，会把每一列单独绘制 2. 柱状图123456789import pandas as pdimport numpy as nps = pd.Series(np.random.randint(0,100,size=10))print(s)s.plot(title='demo-series',label='line',legend=True)s.plot(kind='bar',colormap='Oranges_r',label='bar',legend=True) 我们设置kind=’bar’，就可以画柱状图了 123456789101112import pandas as pdimport numpy as npimport matplotlib.pyplot as pltdf = pd.DataFrame(np.random.randn(10,4)*100,index=np.arange(0,100,10), columns=list('ABCD'))print(df)f,axes = plt.subplots(2,1)df.plot(kind='bar',ax=axes[0])df.plot(kind='barh',ax=axes[1]) 在pandas里画图非常容易，很多都可以是默认转换，index、columns可以自动转换为x轴、y轴标签12345678910111213import pandas as pdimport numpy as npimport matplotlib.pyplot as pltdf = pd.DataFrame(np.random.rand(6,4)*10,index=['one','two','three','four','five','six'], columns=list('ABCD'))print(df)#通过ax参数，可以在不同的subplot上绘图f,axes = plt.subplots(2,1)df.plot(kind='bar',ax=axes[0])df.plot(kind='barh',ax=axes[1]) 在DataFrame中，另一个好用的参数，就是stacked，可以很方便的绘制堆叠图1df.plot(kind='bar',ax=axes[0],stacked=True)]]></content>
      <categories>
        <category>Python-Pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib手册(6) - 水平条形图]]></title>
    <url>%2F2017%2F08%2F11%2Fmatplotlib-handbook-06%2F</url>
    <content type="text"><![CDATA[matplotlib手册(6) 这个其实和上一篇柱状图差不多，只是用了另一个函数，这里主要介绍下上一篇没有说到的东西。函数介绍12345678matplotlib.pyplot.barh(bottom, width, height=0.8, left=None, hold=None, **kwargs)Make a horizontal bar plot.Make a horizontal bar plot with rectangles bounded by: left, left + width, bottom, bottom + height (left, right, bottom and top edges) 由于是横向的条形图，所有参数会有些变化，比如这里，默认height是0.8来一个基本小例子123456789101112import matplotlib.pyplot as pltimport numpy as npplt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签 plt.rcParams['axes.unicode_minus']=False #用来正常显示负号data = np.random.randint(10,100,size=10)plt.barh(np.arange(data.size), data, label='夜宵')plt.legend()plt.show() 下面，我们来看几个之前没有用过的函数12345matplotlib.pyplot.xticks(*args, **kwargs)Get or set the x-limits of the current tick locations and labels.matplotlib.pyplot.yticks(*args, **kwargs)Get or set the y-limits of the current tick locations and labels. 这2个函数，可以设置x轴和y轴的标签和坐标长度及间距，还是很好玩儿的1234#x轴从0到100，间距10plt.xticks(np.arange(0,100,10))#y轴使用中文替换plt.yticks(np.arange(data.size),('愿','有','人','陪','你','颠','沛','流','离','心')) 下面说2个暂时还不知道咋用的参数，12345xerr : scalar or array-like, optional, default: None if not None, will be used to generate errorbar(s) on the bar chartyerr : scalar or array-like, optional, default: Noneif not None, will be used to generate errorbar(s) on the bar chart 在网上查了一圈，应该是叫“误差棒”，说实在散布图中，注明所测量数据的不确定度的大小。这里先混个眼熟吧，后面等明白咋用了，再说下1plt.barh(np.arange(data.size), data, label='夜宵', xerr=2, yerr=0.5,ecolor='red') 这些十字线，就是绘制的“误差棒”下面，我们试一个对照图，我们将x轴变成负的123plt.barh(np.arange(data.size), data, label='夜宵',color='orange')plt.barh(np.arange(data.size), -data, label='早餐',color='green') 先到这吧，网有点儿卡，看看明天再加些其他内容]]></content>
      <categories>
        <category>数据可视化-Python&amp;R</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>数据可视化</tag>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib手册(5) - 柱状图]]></title>
    <url>%2F2017%2F08%2F11%2Fmatplotlib-handbook-05%2F</url>
    <content type="text"><![CDATA[matplotlib手册(5) 这里主要整理下matplotlib中绘制柱状图的方法，参考了几篇文章：matplotlib绘图——柱状图官网介绍感谢上面的分享，博客写的很好很全，帮助很大，下面我们开始。我们要绘制柱形图，要使用pyplot的bar方法12345678matplotlib.pyplot.bar(left, height, width=0.8, bottom=None, hold=None, data=None, **kwargs)Make a bar plot.Make a bar plot with rectangles bounded by: left, left + width, bottom, bottom + height (left, right, bottom and top edges) 做了一些练习之后，发现通过left、height、width、bottom这几个参数可以实现很多不同的图表，后面我们会说到。因为是柱形图，所以四个点的坐标很重要，我们初始化，必须指定的是left和height，width默认是0.8，bottom默认是None，就是0下面，开始我们的小例子 1.基本的柱形图1234567import matplotlib.pyplot as plt#plt.bar(left=[10,11],height=[10,15])plt.bar([10,11],[10,15])plt.grid(True)plt.show() 上面就是一个简单的柱状图，我们来分析一下，这个图 width&amp;bottom我们传入的参数是left=[10,11]，height=[10,15]，以第一个柱状图为例，他四个点的坐标为：图展示的时候，默认是居中的左下(left-width/2,bottom)，右下(left+width/2,bottom)，左上(left-width/2,bottom+height)，右上(left+width/2,bottom+height)即(9.6,0)，(10.4,0)，(9.6,10)，(10.4,10)，理解这个位置很重要，后面会用的到。下面，我们就修改下参数，加深下这个位置的理解12#我们把width改为1，看起来会更方便plt.bar([10,12],[10,15],width=1) 我们可以修改对齐方式1234#align : &#123;‘center’, ‘edge’&#125;, optional#If ‘edge’, aligns bars by their left edges (for vertical bars) and by their bottom edges (for horizontal bars).plt.bar([10,12],[10,15],width=1,align='edge') 我们看看width和bottom的参数介绍，他们都可以是一个常量，或数组的，什么意思呢？就是说，我们可以给不同的柱状图设置不同的宽度和y轴起始高度 width : scalar or array-like, optionalbottom : scalar or array-like, optional 像这样：1plt.bar([10,12],[10,15],width=[0.5,1],bottom=[-5,0]) 好了，下面，我们就开始看看一些常用的参数 颜色 facecolor or fc: mpl color spec, or None for default, or ‘none’ for no colorcolor : Set both the edgecolor and the facecolor 123456import matplotlib.pyplot as plt#设置填充颜色和边框颜色plt.bar([10,12,14],[10,15,20],facecolor='yellow',edgecolor='blue')plt.show() 那个color也可以设置填充颜色和边框颜色，但是他比较好玩儿，他可以接受一个颜色数组，让不同的柱状图颜色不一样1plt.bar([10,12,14],[10,15,20],color=['red','green','blue']) 边框的样式和宽度 linestyle or ls ： [‘solid’ | ‘dashed’, ‘dashdot’, ‘dotted’ | (offset, on-off-dash-seq) | ‘-‘ | ‘–’ | ‘-.’ | ‘:’ | ‘None’ | ‘ ‘ | ‘’]linewidth or lw ： float or None for default 12345import matplotlib.pyplot as pltplt.bar([10,12,14],[10,15,20],facecolor='yellow',edgecolor='blue',linestyle='--',linewidth=3)plt.show() 填充12345678910111213hatch： / - diagonal hatching\ - back diagonal| - vertical- - horizontal+ - crossedx - crossed diagonalo - small circleO - large circle. - dots* - starsLetters can be combined, in which case all the specified hatchings are done. If same letter repeats, it increases the density of hatching of that pattern. 12plt.bar([10,12,14],[10,15,20],facecolor='yellow',edgecolor='blue' ,linestyle='--',linewidth=3,hatch='/') 这个hatch是可以组合的12plt.bar([10,12,14],[10,15,20],facecolor='yellow',edgecolor='blue' ,linestyle='--',linewidth=3,hatch='/\\') 标签下面还有一个参数，我们平时使用时，x轴可能并不是数字，而是文字，这个也是可以的， tick_label : string or array-like, optionalthe tick labels of the bars default: None 123456import matplotlib.pyplot as pltplt.bar([10,12,14],[10,15,20],facecolor='yellow',edgecolor='blue' ,linestyle='--',linewidth=3,hatch='/\\',tick_label=['one','two','three'])plt.show() 这个中文标签，暂时还不行，会显示乱码，这个bar貌似没有字体参数，等我研究下，再回来分享。好辣，中文是可以处理的，加上参数就行了，详情参考：matplotlib手册(4)-中文乱码 中最下面的更新的方法 2. 堆叠柱状图就是根据坐标，将2个柱状图上下放置即可，就是灵活运用left、bottom、width、height参数import matplotlib.pyplot as pltplt.bar([10,12,14],[10,15,20],facecolor=’green’,tick_label=[‘one’,’two’,’three’],label=’green’)#横坐标一样就行了，plt.bar([10,12,14],[5,30,10],bottom=[10,15,20],facecolor=’orange’,label=’orange’)plt.legend()plt.show() 3. 并列柱状图 就是同时显示过个柱状图，同样是利用坐标，计算好位置就行了，将右边的left加上一个width就行了，12345678910import matplotlib.pyplot as pltplt.bar([10,12,14],[10,15,20],facecolor='green',tick_label=['one','two','three'],label='green')#横坐标一样就行了，plt.bar([10,12,14],[5,30,10],bottom=[10,15,20],facecolor='orange',label='orange')plt.legend()plt.show()]]></content>
      <categories>
        <category>数据可视化-Python&amp;R</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>数据可视化</tag>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy手册(5)-random模块]]></title>
    <url>%2F2017%2F08%2F10%2Fnumpy-handbook-05%2F</url>
    <content type="text"><![CDATA[PythonNumpy知识总结 numpy的random模块应该很常用，这里整理一下，参考文章：http://www.mamicode.com/info-detail-507676.htmlhttps://docs.scipy.org/doc/numpy/reference/routines.random.html 简单随机数据123456789101112131415161718192021222324252627numpy.random.rand(d0, d1, ..., dn)Random values in a given shape.Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1)生成给定形状的随机值，随机值在[0,1)import numpy as npnp.random.rand(10)Out[2]: array([ 0.41103285, 0.19043225, 0.30385602, 0.19330136, 0.09727556, 0.96518049, 0.29930132, 0.00633969, 0.64269577, 0.79953589])np.random.rand(2,3)Out[3]: array([[ 0.86213038, 0.56657202, 0.83083843], [ 0.48660386, 0.20508572, 0.4927877 ]])np.random.rand(2,3,1)Out[4]: array([[[ 0.06676746], [ 0.55548283], [ 0.04411342]], [[ 0.18659571], [ 0.02209355], [ 0.83529269]]]) 12345678910111213141516171819202122numpy.random.randn(d0, d1, ..., dn)返回指定形状的状态分布样本For random samples from N(\mu, \sigma^2), use:sigma * np.random.randn(...) + munp.random.randn(2,1)Out[6]: array([[-0.29088142], [ 1.29634911]])np.random.randn(2,2)Out[7]: array([[ 0.24125164, 1.62201226], [ 0.10129715, -1.62001598]])Two-by-four array of samples from N(3, 6.25):2.5 * np.random.randn(2, 4) + 3Out[8]: array([[ 5.09295036, 2.2706219 , 3.26392307, 0.86550482], [ 7.59911261, 5.22543816, 2.0441248 , 1.03322082]]) 1234567891011121314numpy.random.randint(low, high=None, size=None, dtype='l')返回随机整数，左闭右开[low,high)np.random.randint(low=1,high=10,size=8)Out[10]: array([4, 2, 6, 7, 2, 4, 3, 8])#high为空的话，直接[0,low)np.random.randint(10,size=5)Out[11]: array([1, 3, 7, 9, 9])np.random.randint(low=1,high=10,size=(2,3))Out[12]: array([[8, 3, 6], [4, 1, 9]]) 12345678numpy.random.random_integers(low, high=None, size=None)返回随机整数，闭区间[low,high]这个和random.randint类似，已经不推荐使用了np.random.random_integers(low=1,high=5,size=5)__main__:1: DeprecationWarning: This function is deprecated. Please call randint(1, 5 + 1) insteadOut[13]: array([3, 2, 2, 4, 5]) 123456789101112131415numpy.random.random_sample(size=None)numpy.random.random(size=None)numpy.random.ranf(size=None)numpy.random.sample(size=None)返回随机的浮点值，左闭右开区间[0.0, 1.0)np.random.random_sample(8)Out[14]: array([ 0.70353035, 0.79018004, 0.50390916, 0.46261548, 0.85556642, 0.68129238, 0.07098945, 0.65927063])np.random.random_sample([2,3])Out[16]: array([[ 0.37546444, 0.50352846, 0.3496647 ], [ 0.02849239, 0.6035842 , 0.32514876]]) 排列123456789101112numpy.random.shuffle(x)就地修改序列的顺序，类似于洗牌，打乱顺序a = np.random.randint(low=1,high=10,size=10)aOut[18]: array([6, 2, 5, 5, 2, 2, 4, 9, 7, 8])np.random.shuffle(a)aOut[20]: array([2, 8, 4, 7, 5, 2, 5, 6, 2, 9]) 123456789numpy.random.permutation(x)返回一个随机排列If x is an integer, randomly permute np.arange(x). If x is an array, make a copy and shuffle the elements randomly.np.random.permutation(10)Out[21]: array([8, 9, 7, 0, 6, 1, 2, 5, 3, 4])np.random.permutation([1, 4, 9, 12, 15])Out[22]: array([ 4, 12, 1, 9, 15])]]></content>
      <categories>
        <category>Python-Numpy</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib手册(4)-中文乱码]]></title>
    <url>%2F2017%2F08%2F10%2Fmatplotlib-handbook-04%2F</url>
    <content type="text"><![CDATA[matplotlib手册(4) 前面的练习中，我们基本上没有使用中文，但是在实际使用时，中文肯定是需要的，刚刚试了下，发现会有乱码的，就找了找，这里记录下。 1.问题描述1234567891011import matplotlib.pyplot as pltplt.figure(u'中文')plt.title(u'今天')plt.xlabel(u'明天')plt.ylabel(u'昨天')plt.show() 通过上面的例子，我们可以看到，除了figure的title没有问题，其他的title、label中文都显示为乱码 2. 解决办法首先参考的是一篇博客：http://blog.csdn.net/garfielder007/article/details/51405139 就是手动指定中文的字体，就可以了，就是每次都要指定可能会麻烦些1234567891011121314151617import matplotlib.pyplot as pltfrom matplotlib.font_manager import FontProperties #引用Windows中的字体font_set = FontProperties(fname=r'C:\Windows\Fonts\simsun.ttc', size=15)plt.figure(u'中文')plt.plot([1,2,3,4],[-2,-1,0,1])plt.title(u'今天',fontproperties=font_set)plt.xlabel(u'明天',fontproperties=font_set)plt.ylabel(u'昨天',fontproperties=font_set)plt.show() 上面，我们引入了一个Windows中的字体，然后再每次使用时，都指定这个字体就行了 我看网上说还有修改一次就可以的，暂时还没有测试，后面我试一下，再回来补充。 —–update at 2017-08-11刚发现了另一种方法，貌似简单些，不需要每次都设置字体，感谢原作者分享：https://segmentfault.com/a/119000000514427512345678910111213import matplotlib.pyplot as plt#设置这个就可以了，plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签 plt.rcParams['axes.unicode_minus']=False #用来正常显示负号plt.bar([10,12,14],[10,15,20],facecolor='green',tick_label=[u'苹果',u'桃子',u'荔枝'],label='绿色')plt.bar([10,12,14],[5,30,10],bottom=[10,15,20],facecolor='orange',label='橙色')plt.legend()plt.show()]]></content>
      <categories>
        <category>数据可视化-Python&amp;R</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>数据可视化</tag>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础（8）- 关键字yield]]></title>
    <url>%2F2017%2F08%2F10%2Fpython-handbook-08%2F</url>
    <content type="text"><![CDATA[前几天遇到了这个yield，不知道是干嘛的，这里学习整理下，主要参考了：如何理解Python关键字yield Python高级特性 上面介绍的都很好，这里就根据自己的理解，简单整理下。 #1. 什么是迭代 常见的list、tuple等集合，我们会通过遍历，比如for循环来获取每一个元素，这就是迭代。这些可以遍历的对象，也叫做可迭代对象 小例子123456789101112131415161718192021222324252627282930313233343536a = [1,2,3]print(a)for i in a: print(i) b = 'abcd'print(b)for i in b: print(i) c = &#123;'name':'lufei','age':20&#125;print(c)for k in c: print(k)for v in c.values(): print(v)for k,v in c.items(): print(k,v)#out[1, 2, 3]123abcdabcd&#123;'name': 'lufei', 'age': 20&#125;nameagelufei20name lufeiage 20 我们怎样判断一个对象是否可以去迭代呢？可以使用collections模块的Iterable12345678910print(type(a),isinstance(a,Iterable))print(type(b),isinstance(b,Iterable))print(type(c),isinstance(c,Iterable))print(type(123),isinstance(123,Iterable))#out&lt;class 'list'&gt; True&lt;class 'str'&gt; True&lt;class 'dict'&gt; True&lt;class 'int'&gt; False 2. 列表生成式(List Comprehensions)一个非常简单的方式来生成list，像这样：12345678910111213141516171819202122range(10)Out[56]: range(0, 10)list(range(10))Out[57]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]list(x for x in range(10))Out[58]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]list(x*x for x in range(10))Out[59]: [0, 1, 4, 9, 16, 25, 36, 49, 64, 81][x*x for x in range(3)]Out[60]: [0, 1, 4]#上面的for后面还可以加上if判断[x for x in range(10) if x&gt;5]Out[61]: [6, 7, 8, 9]#for循环也可以嵌套[x+y for x in 'abc' for y in 'xyz']Out[62]: ['ax', 'ay', 'az', 'bx', 'by', 'bz', 'cx', 'cy', 'cz'] 3.迭代器 前面，我们说了for循环和可迭代对象，像这种可以使用for循环不断取出先一个元素的对象，就叫做迭代器（Iterator）。迭代器不单单可以使用for循环来遍历，还可以使用next()函数不断获取下一个元素，当没有下一个元素时，会抛出StopIteration异常。我们可以使用collections模块的Iterator来判断一个对象是否为迭代器 12345678910from collections import Iteratorisinstance([1,2,3],Iterator)Out[2]: Falseisinstance('abc',Iterator)Out[3]: Falseisinstance(&#123;'name':'lufei','age':20&#125;,Iterator)Out[4]: False 创建一个迭代器有3中方式： 为对象创建 iter()和next()方法 内置的iter()可以将可迭代对象转换为迭代器 生成器 1234567a = [1,2,3]type(iter(a))Out[6]: list_iteratorisinstance(iter(a),Iterator)Out[7]: True 4. 生成器上面，我们使用list()或者[]，很简单方便的生成了一个列表，只要我们将[]替换为()，就创建一个一个generator。生成器可以一边循环，一边计算生成下一个元素，而不是像list一样，一下生成所有的数据。12345678910(x+y for x in 'abc' for y in 'xyz')Out[63]: &lt;generator object &lt;genexpr&gt; at 0x0000021AE0FC9D58&gt;a = (x+y for x in 'abc' for y in 'xyz')aOut[66]: &lt;generator object &lt;genexpr&gt; at 0x0000021AE0FC9518&gt;isinstance(a,Iterator)Out[67]: True 通常，我们使用yield语句可以返回一个生成器，很多例子，这里都是举一个斐波那契数列yield类似return，只不过他返回的是生成器，调用了next()之后，1234567891011121314151617181920212223242526def fab(n): a,b,i = 0,1,0 while i&lt;n: print(b) a,b = b,a+b i+=1fab(5)#out11235#使用yield改造def fab(n): a,b,i = 0,1,0 while i&lt;n: yield b a,b = b,a+b i+=1for i in fab(5): print(i) 我们调用fab的时候，执行到yield，会返回一个生成器，当调用next()后，程序会回到yield停止的地方继续往下执行这样，就是每次只生成当前元素，而不是一下子生成所有的元素；当然，for循环替我们调用了next()，并处理了StopIteration异常。好了，梳理好上面这些概念，到yield这里，其实还好，平时在理解下，多用用，好，就到这。]]></content>
      <categories>
        <category>Python-基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib手册(3)-pyplot文本相关函数使用]]></title>
    <url>%2F2017%2F08%2F10%2Fmatplotlib-handbook-03%2F</url>
    <content type="text"><![CDATA[matplotlib手册(3) 昨天学习了figure、subplot，最后还说了些常用的方法。这里先简单总结下文本相关的函数使用，算是昨天最后一个部分的补充。 1.pyplot.plot我们先来看下，这个plot函数，之前呢，绘图的时候，一直都是使用这个函数， 貌似没说这个函数到底干啥的。123matplotlib.pyplot.plot(*args, **kwargs)Plot lines and/or markers to the Axes. args is a variable length argument, allowing for multiple x, y pairs with an optional format string. For example, each of the following is legal: 就是说，这个plot函数呢，可以在axes上绘制线条或者标记；我们呢，可以直接使用pyplot.plot函数去在当前的Axes上绘图（我们之前应该主要使用这种方法），也可以获取当前Axes，然后通过Axes去绘图123456789101112131415import matplotlib.pyplot as plt#手动创建一个figure，标题为"one"f1 = plt.figure('one')#获取当前axescurrent_axes = plt.gca()#使用axes绘图current_axes.plot([1,2,3],[3,2,1],color='red',label='lable1',linewidth=3)current_axes.plot([1,2,3],[1,2,3],color='green',label='label2',linewidth=3)print(type(current_axes))plt.show() 2. 常用文本函数下面，我们就介绍下Text相关的函数，参考文档：http://matplotlib.org/users/text_intro.html 123456789text() #在任意坐标轴位置，添加文字 matplotlib.axes.Axes.text() figtext() #matplotlib.figure.Figure.text() Add text to figure at location x, y (relative 0-1 coords)Axes.text(x, y, s, fontdict=None, withdash=False, **kwargs)#这里记录几个常用的参数#这2个主要是控制文本的位置，大家试一下知道啥意思了horizontalalignment #[ ‘center’ | ‘right’ | ‘left’ ],水平位置verticalalignment #[ ‘center’ | ‘top’ | ‘bottom’ | ‘baseline’ ]，垂直位置 一个小例子 current_axes.text(2,1,’hahaha’,fontsize=20) 默认位置， 我们修改下参数 12#设置水平居中，文字会在指定的坐标出，进行居中current_axes.text(2,1,'hahaha',fontsize=20,horizontalalignment='center') 还有一个参数，他可以将文字用框框起来，bbox的value是dic，指定矩形的属性就行， bbox #FancyBboxPatch prop dict bbox is a dictionary of Rectangle properties. 小例子1234current_axes.text(2,3,'hello world.',bbox=dict(facecolor='blue', alpha=0.1))current_axes.text(2,1,'hahaha',fontsize=20,horizontalalignment='center', verticalalignment='center', bbox=dict(facecolor='blue', alpha=0.5)) 上面，还有一个figtext()函数，和text()差不多，只不过，他是相对于figure来说的，主要是相对位置，x，y是0-1的值 plt.figtext(0.5,0.6,’figtext’,fontsize=20) 下面，我们继续说常用的文本函数，有些，我们已经用过了123xlabel() #x轴标签 matplotlib.axes.Axes.set_xlabel()ylabel() #y轴标签 matplotlib.axes.Axes.set_ylabel()title() #给轴添加标题 matplotlib.axes.Axes.set_title() 小例子123current_axes.set_xlabel('x-label')current_axes.set_ylabel('y-label')current_axes.set_title('op-title') 我们继续，还有一个suptitle()，他会在figure的中间位置，添加一个标题1suptitle() #add a title to the Figure; matplotlib.figure.Figure.suptitle() 他有很多的默认参数 plt.suptitle(‘hey,boy’) 再看最后一个函数1234567#add an annotation, with optional arrow, to the Axes ; matplotlib.axes.Axes.annotate()annotate() #xy指向的坐标，xytext文本坐标，arrowprops定义箭头格式current_axes.annotate('look!!',xy=(2,2),xytext=(2.5,1.75), arrowprops=dict(facecolor='black', shrink=0.05)) 好了，文本相关的函数就介绍到这，整理完以后，感觉还是有太多太多内容，还是得多练习。完整练习代码：1234567891011121314151617181920212223242526272829303132import matplotlib.pyplot as plt#手动创建一个figure，标题为"one"f1 = plt.figure('one')#获取当前axescurrent_axes = plt.gca()plt.figtext(0.5,0.6,'figtext',fontsize=20)plt.suptitle('hey,boy')#使用axes绘图current_axes.plot([1,2,3],[3,2,1],color='red',label='lable1',linewidth=3)current_axes.plot([1,2,3],[1,2,3],color='green',label='label2',linewidth=3)current_axes.plot([1,2,3],[2,2,2],'bs')current_axes.text(2,3,'hello world.',bbox=dict(facecolor='blue', alpha=0.1))current_axes.text(2,1,'hahaha',fontsize=20,horizontalalignment='center', verticalalignment='center', bbox=dict(facecolor='blue', alpha=0.5))current_axes.set_xlabel('x-label')current_axes.set_ylabel('y-label')current_axes.set_title('op-title')#xy指向的坐标，xytext文本坐标，arrowprops定义箭头格式current_axes.annotate('look!!',xy=(2,2),xytext=(2.5,1.75), arrowprops=dict(facecolor='black', shrink=0.05))print(type(current_axes))plt.show() 官网上还有一个例子代码，这里也复制一下，同学们可以去官网看详细的介绍：123456789101112131415161718192021222324252627282930313233# -*- coding: utf-8 -*-import matplotlib.pyplot as pltfig = plt.figure()fig.suptitle('bold figure suptitle', fontsize=14, fontweight='bold')ax = fig.add_subplot(111)fig.subplots_adjust(top=0.85)ax.set_title('axes title')ax.set_xlabel('xlabel')ax.set_ylabel('ylabel')ax.text(3, 8, 'boxed italics text in data coords', style='italic', bbox=&#123;'facecolor':'red', 'alpha':0.5, 'pad':10&#125;)ax.text(2, 6, r'an equation: $E=mc^2$', fontsize=15)ax.text(3, 2, u'unicode: Institut f\374r Festk\366rperphysik')ax.text(0.95, 0.01, 'colored text in axes coords', verticalalignment='bottom', horizontalalignment='right', transform=ax.transAxes, color='green', fontsize=15)ax.plot([2], [1], 'o')ax.annotate('annotate', xy=(2, 1), xytext=(3, 4), arrowprops=dict(facecolor='black', shrink=0.05))ax.axis([0, 10, 0, 10])plt.show()]]></content>
      <categories>
        <category>数据可视化-Python&amp;R</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>数据可视化</tag>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫小实例学习篇-猫眼电影]]></title>
    <url>%2F2017%2F08%2F09%2Fcrawler-demo-01%2F</url>
    <content type="text"><![CDATA[这里参考了论坛里一位同学分享的博客：猫眼电影TOP100爬取练习，感谢分享。 学习要从模仿开始，学习了上面的博客之后，自己做下练习，正好最近看了selenium，就用了这个。 原作者的正则用的太溜了，等后面有时间再研究下，这里就简单的使用CSS Selector来实现了。 原文代码很精彩，我这个代码就粗糙很多了，先来个初始版，后面再慢慢优化。 大体思路和Python基础（7）- Selenium使用 里面的豆瓣读书例子差不多， 代码（2017-08-09版）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123import csvfrom selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.common.exceptions import TimeoutException#打开浏览器browser = webdriver.Firefox()#设置等待时长，最长等待10swait = WebDriverWait(browser,10)#定义电影排名comment_index = 0#定义一个movieclass Movie: __movie_id = 0 __movie_name = '' __movie_star = '' __movie_releasetime = '' __movie_score = '' def __init__(self , movie_id,movie_name,movie_star,movie_releasetime,movie_score): self.__movie_id = movie_id self.__movie_name = movie_name self.__movie_star = movie_star self.__movie_releasetime = movie_releasetime self.__movie_score = movie_score def show(self): print('影片排名: ', self.__movie_id) print('影片名称: ', self.__movie_name) print(self.__movie_star) print(self.__movie_releasetime) print('影片评分', self.__movie_score) print('') def simple_list(self): return [self.__movie_id, self.__movie_name, self.__movie_star, self.__movie_releasetime, self.__movie_score] def save2csv(movie_list): with open('movie.csv', 'a', newline='',encoding='utf-8') as csvfile: csv_writer = csv.writer(csvfile) for m in movie_list: csv_writer.writerow(m.simple_list()) csvfile.close() def main(): #打开URL browser.get('http://maoyan.com/board/4') #输出浏览器标题 print('browser title: ',browser.title) #数据更新信息 p_update_info = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,'div.main p.update-time'))) print('更新信息: ',p_update_info.text) #输出当前页信息 show_current_page() def show_current_page(): print('-----------------------------------') print('current url: ',browser.current_url) #获取当前页信息 div_page = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,'div.pager-main li.active'))) print('current page: ',div_page.text) #当前页总数量 div_items = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR,'div.main div.board-item-main div.board-item-content'))) print('total count: ',len(div_items)) movie_list = [] #结果集有很多结果，我们获取 for item in div_items: #调用全局变量 global comment_index comment_index += 1 #获取我们需要的信息 p_name = item.find_element_by_css_selector('div.movie-item-info p.name') p_star = item.find_element_by_css_selector('div.movie-item-info p.star') p_releasetime = item.find_element_by_css_selector('div.movie-item-info p.releasetime') p_score_integer = item.find_element_by_css_selector('div.movie-item-number p.score i.integer') p_score_fraction = item.find_element_by_css_selector('div.movie-item-number p.score i.fraction') #初始化movie对象 m = Movie(comment_index , p_name.text , p_star.text, p_releasetime.text, p_score_integer.text+p_score_fraction.text) movie_list.append(m) save2csv(movie_list) #获取下一页 show_next_page()def show_next_page(): try: #获取下一页的标签 #最后1页，没有下一页标签 a_next = wait.until(EC.presence_of_element_located((By.PARTIAL_LINK_TEXT,'下一页'))) a_next.click() show_current_page() except TimeoutException: #找不到下一页 print('get all movies.') #也有可能真是网络异常 finally: browser.quit() if __name__=='__main__': main() 这里就简单记录遇到的一些问题和后面需要优化的地方： 获取影片信息的时候，数据没有清洗好，像这个“主演”，“上映时间”还没有剔除掉；那个地点也可以拆分出来单独一个字段 csv编码问题，一开始默认使用gbk（在Windows下开发的），会报错，说是有异常的字符无法保存，改为UTF-8后，就可以了，但是使用CSV打开前，先用notepad++转了编码，才用CSV打开 使用正则去获取元素 4.异常问题的处理]]></content>
      <categories>
        <category>Python-爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib手册(2)-pyplot.figure]]></title>
    <url>%2F2017%2F08%2F09%2Fmatplotlib-handbook-02%2F</url>
    <content type="text"><![CDATA[matplotlib手册(2) 这里，我们继续上一节，继续学习下pyplot参考官方文档：http://matplotlib.org/2.0.2/users/pyplot_tutorial.html 1. pyplot.figure 我暂时是把他理解成画板的意思，前面我们虽然没有使用figure，但是默认会创建一个figure12345678matplotlib.pyplot.figure(num=None, figsize=None, dpi=None, facecolor=None, edgecolor=None, frameon=True, FigureClass=&lt;class 'matplotlib.figure.Figure'&gt;, **kwargs)#这里有几个常用的参数，比如num，这就好比是一个id，来表示每一个figurenum : integer or string, optional, default: none#画板的大小figsize : tuple of integers, optional, default: None#表示画板的背景颜色facecolor :the background color. If not provided, defaults to rc figure.facecolor 我们来个小例子，我们就创建3个figure，修改背景颜色，不做其他的12345678910import matplotlib.pyplot as pltplt.figure('one' ,facecolor='green')plt.figure('two' ,facecolor='red')plt.figure('three' ,facecolor='blue')plt.show() 上面，我们新建3个figure，名字分别是“one”，“two”，“three”，执行后，会看到这3个figure 不同的画板，我们可以在上面画不同的图12345678910111213import matplotlib.pyplot as pltplt.figure('one')plt.plot([1,2,3] , [3,5,1])plt.figure('two')plt.plot([1,2,3] , [3,5,1],'ro')#plt.figure('three' ,facecolor='blue')plt.show() 有的时候，我们只是想在一个画板上，显示多个图，比如上面这个，我们想在一个figure上显示这时候，我们需要用到另一个函数 2. pyplot.subplot1234matplotlib.pyplot.subplot(*args, **kwargs)#plot_number starts at 1subplot(nrows, ncols, plot_number) 这个函数不知道翻译成什么，大概的意思，就是说把figure分割成了几块，nrows:多少行，ncols：多少列，plot_number：编号；这里面有一个轴的概念，因为每个图都有自己的横纵坐标，这里也说是把figure分成了nrows*ncols 个子轴12345678910111213import matplotlib.pyplot as pltplt.figure('one')#2行1列，第1个plt.subplot(2,1,1)plt.plot([1,2,3] , [3,5,1])#2行1列，第2个plt.subplot(2,1,2)plt.plot([1,2,3] , [3,5,1],'ro')plt.show() 3. pyplot常用方法下面，我们补充下pyplot的一些常用方法12345678910111213141516171819202122232425262728import matplotlib.pyplot as pltplt.figure('one')#2行1列，第1个plt.subplot(2,1,1)plt.plot([1,2,3] , [3,5,1],label='label1')#添加x轴标题plt.xlabel('name',fontsize=14, color='green')#添加y轴标题plt.ylabel('age')#添加标题plt.title('hello world')#添加文本注释plt.text(2, 2, 'hey,look at me')#显示图例plt.legend()#显示网格线plt.grid(True)#2行1列，第2个plt.subplot(2,1,2)plt.plot([1,2,3] , [3,5,1],'ro')plt.show() 上面的text函数，可以添加文本标记，还有一个更好用的函数annotate, plt.annotate(‘local max’, xy=(1.25, 3), xytext=(3, 1.5), arrowprops=dict(facecolor=’black’, shrink=0.05), ) 这些函数，实在太多，功能太强大了，得慢慢来消化，多使用。]]></content>
      <categories>
        <category>数据可视化-Python&amp;R</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>数据可视化</tag>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy手册(4)-ufunc]]></title>
    <url>%2F2017%2F08%2F08%2Fnumpy-handbook-04%2F</url>
    <content type="text"><![CDATA[PythonNumpy知识总结 这里，我们说下对数组操作的常用函数 常用函数我们先说下接收一个参数的一元函数，比如 np.sqrt 开方函数1234567891011a = np.arange(10)np.sqrt(a)Out[45]: array([ 0. , 1. , 1.41421356, 1.73205081, 2. , 2.23606798, 2.44948974, 2.64575131, 2.82842712, 3. ])a**0.5Out[46]: array([ 0. , 1. , 1.41421356, 1.73205081, 2. , 2.23606798, 2.44948974, 2.64575131, 2.82842712, 3. ]) 常用的一元函数 还有些常用的二元函数，比如 add,subtract123456789101112131415a = np.array([1,2,3])b = np.array([4,5,6])aOut[49]: array([1, 2, 3])bOut[50]: array([4, 5, 6])np.add(a,b)Out[51]: array([5, 7, 9])np.subtract(a,b)Out[52]: array([-3, -3, -3]) np.wherenp.where 是三元表达式 x if condition else y 的矢量化版本12345numpy.where(condition[, x, y])Return elements, either from x or y, depending on condition.If only condition is given, return condition.nonzero(). 我们根据condition的值，来确定是返回x的值，还是y的值12345678x = np.array([1.1,1.2,1.3,1.4,1.5])y = np.array([2.1,2.2,2.3,2.4,2.5])cond = np.array([True,False,True,True,False])np.where(cond,x,y)Out[56]: array([ 1.1, 2.2, 1.3, 1.4, 2.5]) np.where的第2个，第3个参数不一定是数组，也可以是标量值；比如，有一个矩阵，我们想要将所有正值替换为2，负值替换为-21234567891011121314151617181920212223a = np.random.randn(4,4)aOut[58]: array([[-0.62737481, -0.69252389, 0.34290602, -0.54297339], [ 0.57164788, -0.74841413, 1.02406934, 0.3089722 ], [-0.46170713, 2.17671732, -0.51607955, -0.44006653], [-0.13365017, 0.67350363, -0.51877754, -0.382468 ]])np.where(a&gt;0,2,-2)Out[59]: array([[-2, -2, 2, -2], [ 2, -2, 2, 2], [-2, 2, -2, -2], [-2, 2, -2, -2]])#负值的话，我们使用原来的值np.where(a&gt;0,2,a)Out[60]: array([[-0.62737481, -0.69252389, 2. , -0.54297339], [ 2. , -0.74841413, 2. , 2. ], [-0.46170713, 2. , -0.51607955, -0.44006653], [-0.13365017, 2. , -0.51877754, -0.382468 ]]) 数组统计方法我们可以统计数组或某个轴上的数据进行统计计算123456789101112131415a = np.array([1,2,3])a.sum()Out[68]: 6a.max()Out[69]: 3a.min()Out[70]: 1a.mean()Out[71]: 2.0numpy.sum(a, axis=None, dtype=None, out=None, keepdims=&lt;class numpy._globals._NoValue at 0x40ba726c&gt;) 这类聚合函数，可以接收一个axis参数，指定要聚合的轴123456789101112131415a = np.arange(15).reshape(3,5)aOut[74]: array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]])#在横轴上聚合a.sum(axis=1)Out[75]: array([10, 35, 60])#在列上聚合a.sum(axis=0)Out[76]: array([15, 18, 21, 24, 27]) 常用的统计函数 用于布尔数组的方法 对于布尔数组来说，执行上面的统计函数，会是将True转成1，False转成0123456789101112b = np.array([True,False,True,True])b.sum()Out[78]: 3#b中有Trueb.any()Out[79]: True#b中不都为Trueb.all()Out[80]: False 还有2个函数any和all，可以判断数组中是否存在一个或多个True 排序我们可以对数组进行排序12345678910111213141516ndarray.sort(axis=-1, kind='quicksort', order=None)Sort an array, in-place.a = np.random.randn(10)aOut[82]: array([ 0.02418202, -1.86975588, 0.00273745, 0.22470742, 1.10362729, 0.75344308, -0.89005284, -0.94833805, 1.37111527, 1.22149417])a.sort()aOut[84]: array([-1.86975588, -0.94833805, -0.89005284, 0.00273745, 0.02418202, 0.22470742, 0.75344308, 1.10362729, 1.22149417, 1.37111527]) ndarray是就地排序，直接排序原数组；np.sort则是返回一个排序后的数组 其他集合函数比如unique，可以获取数组的唯一值1234567a = np.array([1,3,3,3,5,5,2,1])aOut[90]: array([1, 3, 3, 3, 5, 5, 2, 1])np.unique(a)Out[92]: array([1, 2, 3, 5])]]></content>
      <categories>
        <category>Python-Numpy</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib手册(1)-pyplot使用]]></title>
    <url>%2F2017%2F08%2F08%2Fmatplotlib-handbook-01%2F</url>
    <content type="text"><![CDATA[matplotlib手册(1) 最近开始看看Pandas中可视化的部分，主要是使用matplotlib，这里，我们先从pyplot开始。主要参考官方教程：Pyplot tutorial我们从一个最简单的例子开始12345678import matplotlib.pyplot as plt#plt.plot([1,2,3,4])#指定y轴标签plt.ylabel('some numbers')#展示图片plt.show() 只要几行代码，就显示了一个图形，我们观察一下这个图，会发现，y轴是1到4，而x轴是1到3，这是由于我们传入的参数导致的plot([1,2,3,4])，这里呢，我们只传了一个数组，matplotlib会默认当做y轴的值，x轴是默认分配的，Python中ranges默认是从0开始的，所以x轴从0-3我们也可以指定x轴的值12345678910import matplotlib.pyplot as plt#默认是y轴的值#plt.plot([1,2,3,4])#传入x轴，y轴plt.plot([1, 2, 3, 4], [1, 4, 9, 16])#指定y轴标签plt.ylabel('some numbers')#展示图片plt.show() matplotlib.pyplot.plot(args, *kwargs) 这个plot呢，还可以接收一个参数，来控制绘制图形的颜色和类型，这里默认是’’，’b’表示颜色是blue，’-‘表示solid line详细的见下面的介绍，从官网copy来的http://matplotlib.org/2.0.2/api/pyplot_api.html#matplotlib.pyplot.legend12345678910111213141516171819202122232425262728The following format string characters are accepted to control the line style or marker:character description'-' solid line style'--' dashed line style'-.' dash-dot line style':' dotted line style'.' point marker',' pixel marker'o' circle marker'v' triangle_down marker'^' triangle_up marker'&lt;' triangle_left marker'&gt;' triangle_right marker'1' tri_down marker'2' tri_up marker'3' tri_left marker'4' tri_right marker's' square marker'p' pentagon marker'*' star marker'h' hexagon1 marker'H' hexagon2 marker'+' plus marker'x' x marker'D' diamond marker'd' thin_diamond marker'|' vline marker'_' hline marker 12345678910The following color abbreviations are supported:character color‘b’ blue‘g’ green‘r’ red‘c’ cyan‘m’ magenta‘y’ yellow‘k’ black‘w’ white 我们按照上面的介绍，这里我们改一下展示效果，我们改为’ro’，红色，圆圈标记1plt.plot([1, 2, 3, 4], [1, 4, 9, 16] ,'ro') 就颜色来说，上面只介绍了几种颜色的缩写，那我们想用其他颜色咋办呢？我们可以直接用参数指定，详细参数介绍，可以看官网介绍：http://matplotlib.org/2.0.2/api/pyplot_api.html12plt.plot([1, 2, 3, 4], [1, 4, 9, 16] ,color='green', marker='^',markersize=15,markerfacecolor='red') 我们再观察这个图，前面我们说过，只传一个list，会初始化y轴，传入2个，会初始化x，y；这里继续观察x轴和y轴会发现他们的起始点不一样，同样，我们是可以修改这个轴的1234567matplotlib.pyplot.axis(*v, **kwargs)plt.plot([1, 2, 3, 4], [1, 4, 9, 16] ,color='green', marker='^',markersize=15,markerfacecolor='red')#设置轴坐标，[xmin, xmax, ymin, ymax]plt.axis([1,8,1,32]) plot这个函数很有意思，我们最后来看个例子12345678910111213141516import numpy as npimport matplotlib.pyplot as plt# evenly sampled time at 200ms intervalst = np.arange(0., 5., 0.2)print(t)#他直接传入了好多组参数，绘制了3条线，这里，我们也可以拆开分别绘制# red dashes, blue squares and green trianglesplt.plot(t, t, 'r--', t, t**2, 'bs', t, t**3, 'g^')plt.show()#分别绘制#plt.plot(t, t, 'r--')#plt.plot(t, t**2, 'bs')#plt.plot(t, t**3, 'g^')]]></content>
      <categories>
        <category>数据可视化-Python&amp;R</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>数据可视化</tag>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Spyder不弹出画图面板]]></title>
    <url>%2F2017%2F08%2F08%2Fvisual-handbook-01%2F</url>
    <content type="text"><![CDATA[最近练习Python都是在Spyder中开发，很方便，今天练习matplotlib的时候，发现，绘制的图片显示在out中1234import matplotlib.pyplot as pltplt.plot([1,2,3,4])plt.ylabel('some numbers')plt.show() 依稀记得之前在IPython中执行的时候，是单独弹出一个面板的，就百度了下，找到了解决方法我们在Tools-&gt;Preferences-&gt; 修改完以后，我们需要重启下Spyder重启后，再次执行，就可以看到那个单独的窗口了。]]></content>
      <categories>
        <category>数据可视化-Python&amp;R</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Spyder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas手册（7）- pandas数据加载]]></title>
    <url>%2F2017%2F08%2F08%2Fpandas-handbook-07%2F</url>
    <content type="text"><![CDATA[PythonPandas 这里整理下，pandas中数据加载的几个方法，前面，我们也有用过，read_csv，下面，我们整理下 1.pandas读取数据方法1pandas.read_csv(filepath_or_buffer, sep=', ', delimiter=None, header='infer', names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, iterator=False, chunksize=None, compression='infer', thousands=None, decimal=b'.', lineterminator=None, quotechar='"', quoting=0, escapechar=None, comment=None, encoding=None, dialect=None, tupleize_cols=False, error_bad_lines=True, warn_bad_lines=True, skipfooter=0, skip_footer=0, doublequote=True, delim_whitespace=False, as_recarray=False, compact_ints=False, use_unsigned=False, low_memory=True, buffer_lines=None, memory_map=False, float_precision=None) 我们可以看到这个参数非常多，基本上可以解决我们文件读取时的常见问题。 下面就是小例子csv文件内容是这样的：数据以逗号分隔，没有其他特殊的情况 123456789101112131415161718import pandas as pddf = pd.read_csv(r'D:\document\python_demo\pydata-book-master\ch06\ex1.csv')#使用read_table需要，手动指定下分隔符#df = pd.read_table(r'D:\document\python_demo\pydata-book-master\ch06\ex1.csv',sep=',')print(df)print(df.index)print(df.columns)runfile('D:/document/python_demo/pandas_csv.py', wdir='D:/document/python_demo') a b c d message0 1 2 3 4 hello1 5 6 7 8 world2 9 10 11 12 fooRangeIndex(start=0, stop=3, step=1)Index(['a', 'b', 'c', 'd', 'message'], dtype='object') 我们可以看到，这里默认把第一行当做columns了，我们可以通过header=None，来自动指定标题1234567891011121314151617181920df = pd.read_csv(r'D:\document\python_demo\pydata-book-master\ch06\ex1.csv',header=None) 0 1 2 3 40 a b c d message1 1 2 3 4 hello2 5 6 7 8 world3 9 10 11 12 fooRangeIndex(start=0, stop=4, step=1)Int64Index([0, 1, 2, 3, 4], dtype='int64')#通过names，来自定义columnsdf = pd.read_csv(r'D:\document\python_demo\pydata-book-master\ch06\ex1.csv',header=None,names=list('opqrxy')) o p q r x y0 a b c d message NaN1 1 2 3 4 hello NaN2 5 6 7 8 world NaN3 9 10 11 12 foo NaNRangeIndex(start=0, stop=4, step=1)Index(['o', 'p', 'q', 'r', 'x', 'y'], dtype='object') 现在的行索引是自动初始化的，我们可以指定存在的列为索引12345678910111213path = r'D:\document\python_demo\pydata-book-master\ch06\ex1.csv'df = pd.read_csv(path,header=None, names=list('opqrxy'), index_col='x') o p q r yx message a b c d NaNhello 1 2 3 4 NaNworld 5 6 7 8 NaNfoo 9 10 11 12 NaNIndex(['message', 'hello', 'world', 'foo'], dtype='object', name='x')Index(['o', 'p', 'q', 'r', 'y'], dtype='object') 比如，有这样一份数据，字段间是通过一个或多个空格来分隔的 我们直接使用read_csv去读取，会发现，列索引有些不友好，我们可以使用正则表达式去分隔123456789101112131415161718df = pd.read_csv(path) A B C0 aaa -0.264438 -1.026059 -0.6195001 bbb 0.927272 0.302904 -0.0323992 ccc -0.264273 -0.386314 -0.2176013 ddd -0.871858 -0.348382 1.100491RangeIndex(start=0, stop=4, step=1)Index([' A B C'], dtype='object')#\s表示空格，+表示1个或多个，就是说分隔符是1个或多个空格df = pd.read_table(path,sep='\s+') A B Caaa -0.264438 -1.026059 -0.619500bbb 0.927272 0.302904 -0.032399ccc -0.264273 -0.386314 -0.217601ddd -0.871858 -0.348382 1.100491Index(['aaa', 'bbb', 'ccc', 'ddd'], dtype='object')Index(['A', 'B', 'C'], dtype='object') 还有很多其他常用的参数，比如skiprows，可以跳过指定行下面，再说个填充缺失值的方法，na_values可以将其他我们指定的值也当成NaN处理12345678910111213141516171819202122232425262728na_values : scalar, str, list-like, or dict, default None Additional strings to recognize as NA/NaN. If dict passed, specific per-column NA values. By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, ‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, ‘nan’`.#原始数据是这样的， something a b c d message0 one 1 2 3.0 4 NaN1 two 5 6 NaN 8 world2 three 9 10 11.0 12 foo#这里，我们将1，2，4也当成NaN值处理df = pd.read_csv(path,na_values=[1,2,4])#加载后，数据就会变成这样 something a b c d message0 one NaN NaN 3.0 NaN NaN1 two 5.0 6.0 NaN 8.0 world2 three 9.0 10.0 11.0 12.0 foo#我们还可以用一个dict来对不同的列指定NaN值df = pd.read_csv(path,na_values=&#123;'something':['one','three'],'d':[8,12]&#125;) something a b c d message0 NaN 1 2 3.0 4.0 NaN1 two 5 6 NaN NaN world2 NaN 9 10 11.0 NaN foo #2.pandas导出数据我们使用read_csv读取数据，处理完之后，我们可能还需要将数据存储起来，还有一个to_csv的函数1234567891011121314151617181920212223242526272829DataFrame.to_csv(path_or_buf=None, sep=', ', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, mode='w', encoding=None, compression=None, quoting=None, quotechar='"', line_terminator='\n', chunksize=None, tupleize_cols=False, date_format=None, doublequote=True, escapechar=None, decimal='.')##我们就使用这份数据，先读取后，再保存something,a,b,c,d,messageone,1,2,3,4,NAtwo,5,6,,8,worldthree,9,10,11,12,foodf.to_csv(sys.stdout,sep='^')^something^a^b^c^d^message0^one^1^2^3.0^4^1^two^5^6^^8^world2^three^9^10^11.0^12^foo#我们看到这个输出，发现，NaN值，导出时，转为了空字符串，这里呢，我们可以手工指定na_rep : string, default ‘’df.to_csv(sys.stdout,sep='^',na_rep='NaN')^something^a^b^c^d^message0^one^1^2^3.0^4^NaN1^two^5^6^NaN^8^world2^three^9^10^11.0^12^foo#有时候，行列标签可能也不需要，我们也可以禁用df.to_csv(sys.stdout,sep='^',na_rep='NaN',index=False,header=False)one^1^2^3.0^4^NaNtwo^5^6^NaN^8^worldthree^9^10^11.0^12^foo #3.附录pandas中还有其他的加载数据方式，像读取HTML，JSON，xml等等，常用的可能还是和数据库取连接，这块后面会再补充，这里就先到这里。]]></content>
      <categories>
        <category>Python-Pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy手册(3)-Datetimes and Timedeltas]]></title>
    <url>%2F2017%2F08%2F07%2Fnumpy-handbook-03%2F</url>
    <content type="text"><![CDATA[PythonNumpy知识总结 这里说下numpy中日期、时间相关的使用。主要参考：https://docs.scipy.org/doc/numpy-dev/reference/arrays.datetime.html 基本使用在numpy中，我们很方便的讲字符串转换成日期类型123456789101112131415161718192021222324252627import numpy as npnp.datetime64('2017-08-06')Out[3]: numpy.datetime64('2017-08-06')np.datetime64('2017-08')Out[4]: numpy.datetime64('2017-08')#我们可以通过参数，强制将数据格式化为我们想要的粒度np.datetime64('2017-08' , 'D')Out[5]: numpy.datetime64('2017-08-01')np.datetime64('2017-08' , 'Y')Out[6]: numpy.datetime64('2017')a = np.array(['2017-07-01','2017-07-15','2017-08-01'] , dtype=np.datetime64)aOut[10]: array(['2017-07-01', '2017-07-15', '2017-08-01'], dtype='datetime64[D]')#我们也可以使用arange函数初始化数组b = np.arange('2017-08-01','2017-09-01',dtype=np.datetime64)bOut[12]: array(['2017-08-01', '2017-08-02', '2017-08-03', ..., '2017-08-29', '2017-08-30', '2017-08-31'], dtype='datetime64[D]') 日期计算在numpy中，我们可以进行简单的日期计算 1234567891011121314151617181920#2个日期相减，会得到相差的天数np.datetime64('2017-08-03') - np.datetime64('2017-07-15')Out[27]: numpy.timedelta64(19,'D')#这里日期可以直接减去对应的天数np.datetime64('2017-08-03') - np.timedelta64(20,'D')Out[28]: numpy.datetime64('2017-07-14')#这里粒度要一样，一个是D,不可以和M相减np.datetime64('2017-08-03') - np.timedelta64(1,'M')Traceback (most recent call last): File "&lt;ipython-input-29-61beff16fb05&gt;", line 1, in &lt;module&gt; np.datetime64('2017-08-03') - np.timedelta64(1,'M')TypeError: Cannot get a common metadata divisor for NumPy datetime metadata [D] and [M] because they have incompatible nonlinear base time unitsnp.datetime64('2017-08') - np.timedelta64(1,'M')Out[30]: numpy.datetime64('2017-07') 工作日判断numpy中提供了一个一些工作日判断的函数，比如，通常周一到周五是工作日123456789101112numpy.busday_offset(dates, offsets, roll='raise', weekmask='1111100', holidays=None, busdaycal=None, out=None)First adjusts the date to fall on a valid day according to the roll rule, then applies offsets to the given dates counted in valid days.#2017-08-01是周二#2017-08-01的下一个工作日是2017-08-02np.busday_offset('2017-08-01',1)Out[32]: numpy.datetime64('2017-08-02')#2017-08-01的下2个工作日是2017-08-03np.busday_offset('2017-08-01',2)Out[33]: numpy.datetime64('2017-08-03') 这时候，如果传入的日期是周末，就会报错了12345678#2017-08-05是周六np.busday_offset('2017-08-05',2)Traceback (most recent call last): File "&lt;ipython-input-34-9f767204127b&gt;", line 1, in &lt;module&gt; np.busday_offset('2017-08-05',2)ValueError: Non-business day date in busday_offset 当然，我们可以通过参数来避免错误123456789101112roll : &#123;‘raise’, ‘nat’, ‘forward’, ‘following’, ‘backward’, ‘preceding’, ‘modifiedfollowing’, ‘modifiedpreceding’&#125;, optional How to treat dates that do not fall on a valid day. The default is ‘raise’. ‘raise’ means to raise an exception for an invalid day. ‘nat’ means to return a NaT (not-a-time) for an invalid day. ‘forward’ and ‘following’ mean to take the first valid day later in time. ‘backward’ and ‘preceding’ mean to take the first valid day earlier in time. ‘modifiedfollowing’ means to take the first valid day later in time unless it is across a Month boundary, in which case to take the first valid day earlier in time. ‘modifiedpreceding’ means to take the first valid day earlier in time unless it is across a Month boundary, in which case to take the first valid day later in time. 常用的可能是这个forward和backward一个是向前取第一个有效的工作日，一个是向后取第一个有效的工作日 123456789101112131415161718192021222324252627282930np.busday_offset('2017-08-05',2,roll='forward')Out[35]: numpy.datetime64('2017-08-09')np.busday_offset('2017-08-05',2,roll='backward')Out[36]: numpy.datetime64('2017-08-08')np.busday_offset('2017-08-05',0,roll='forward')Out[37]: numpy.datetime64('2017-08-07')np.busday_offset('2017-08-05',0,roll='backward')Out[38]: numpy.datetime64('2017-08-04')#判断是否为工作日np.is_busday()np.is_busday(np.datetime64('2017-08-05'))Out[39]: Falsenp.is_busday(np.datetime64('2017-08-01'))Out[40]: True#判断时间段内，工作日天数np.busday_count()np.busday_count(np.datetime64('2017-08-01'),np.datetime64('2017-08-06'))Out[41]: 4np.busday_count(np.datetime64('2017-08-06'),np.datetime64('2017-08-01'))Out[42]: -4]]></content>
      <categories>
        <category>Python-Numpy</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础（7）- Selenium使用]]></title>
    <url>%2F2017%2F08%2F07%2Fpython-handbook-07%2F</url>
    <content type="text"><![CDATA[以前搞Java的时候，知道Selenium是做自动化测试的，后来发现搞爬虫也会用到Selenium，这里就和Python整合方面，简单学习下。 1. 什么是Selenium Selenium是一个用于Web应用程序测试的工具。Selenium测试直接运行在浏览器中，就像真正的用户在操作一样。支持的浏览器包括IE（7, 8, 9, 10, 11），Mozilla Firefox，Safari，Google Chrome，Opera等。这个工具的主要功能包括：测试与浏览器的兼容性——测试你的应用程序看是否能够很好得工作在不同浏览器和操作系统之上。测试系统功能——创建回归测试检验软件功能和用户需求。支持自动录制动作和自动生成 .Net、Java、Perl等不同语言的测试脚本。–百度百科 官网地址：http://www.seleniumhq.org/因为Selenium可以录制不同的动作，模拟操作，和Python结合以后，在爬取数据的时候就很方便，所以下面我们来看看需要怎样配置。 #2. 安装配置 2.1火狐浏览器插件安装插件地址：https://addons.mozilla.org/en-US/firefox/addon/selenium-ide/ 安装之后，我们可以直接在浏览器中使用Selenium 2.2火狐浏览器driver下载这个再后面会用到，地址：https://github.com/mozilla/geckodriver/releasesWebDriver是Selenium的API，我们用Python集成开发会很方便，这个下载后，需要放到Path路径下 2.3Python安装Selenium模块1pip install selenium 3.示例代码我们先来看一个简单的例子，我们打开火狐浏览器，然后让浏览器加载一个URL123456789101112from selenium import webdriverbrowser = webdriver.Firefox()browser.get('http://www.baidu.com/')error 信息： File "D:\Users\yugui\Anaconda3\lib\site-packages\selenium\webdriver\common\service.py", line 81, in start os.path.basename(self.path), self.start_error_message)WebDriverException: 'geckodriver' executable needs to be in PATH. 刚开始可能会报错，这是因为我们上面的那个driver没有加到PATH。加到PATH后，再次执行，就可打开浏览器了 再来看一个略复杂的例子：这是官方中的例子：http://www.seleniumhq.org/docs/03_webdriver.jsp#selenium-webdriver12345678910111213141516171819202122232425262728293031from selenium import webdriverfrom selenium.common.exceptions import TimeoutExceptionfrom selenium.webdriver.support.ui import WebDriverWait # available since 2.4.0from selenium.webdriver.support import expected_conditions as EC # available since 2.26.0#1.打开浏览器browser = webdriver.Firefox()#2.浏览器加载地址browser.get('https://www.baidu.com/')#3.输出浏览器标题print(browser.title)#4.一个断言，浏览器标题中包含'百度'关键字assert '百度' in browser.title#5.根据ID，获取搜索框elem = browser.find_element_by_id('kw')#模拟输入Python，进行搜索elem.send_keys('python')elem.submit()try: # 等待浏览器刷新，返回我们的搜索结果 WebDriverWait(browser, 10).until(EC.title_contains("python")) # 输出当前浏览器的标题 print(browser.title)finally: # 关闭浏览器 browser.quit() 执行后，我们会看到，打开浏览器，然后搜索‘Python’，返回搜索结果后会关闭。 4.获取控件的方式在上面的例子中，我们使用了find_element_by_id，就是通过标签的id来获取元素这个和JS里面的差不多，还有很多其他的方法1234567891011#by id#&lt;div id="coolestWidgetEvah"&gt;...&lt;/div&gt;find_element_by_id(id_)element = driver.find_element_by_id("coolestWidgetEvah")#by class name#&lt;div class="cheese"&gt;&lt;span&gt;Cheddar&lt;/span&gt;&lt;/div&gt;&lt;div class="cheese"&gt;&lt;span&gt;Gouda&lt;/span&gt;&lt;/div&gt;find_element_by_class_name(name)cheeses = driver.find_elements_by_class_name("cheese") 论坛里这篇博客写的很好：Python3中Selenium使用方法 5. 实例练习这里顺便写个小例子，练习下，我们方位豆瓣读书，然后搜索关键字，将所有的书都打印出来1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import timefrom selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.common.exceptions import NoSuchElementException#打开浏览器browser = webdriver.Firefox()#设置等待时长，最长等待10swait = WebDriverWait(browser,10)#打开URLbrowser.get('https://book.douban.com/')#输出浏览器标题print('browser title: ',browser.title)#获取搜索框inp_query = wait.until(EC.presence_of_element_located((By.ID,'inp-query')))inp_query.send_keys('onepiece')#可以直接提交，或者获取提交按钮再提交inp_query.submit()'''#获取提交按钮inp_btn = browser.find_element_by_class_name('inp-btn').find_element_by_tag_name('input')inp_btn.click()'''def show_current_page(): print('-----------------------------------') print('current url: ',browser.current_url) #&lt;ul class="subject-list"&gt; rs = wait.until(EC.presence_of_element_located((By.CLASS_NAME,'subject-list'))) book_list = rs.find_elements_by_tag_name('li') print('current page total book: ',len(book_list)) print('info:',browser.find_element_by_class_name('trr').text) #输出书的名字 for book in book_list: print(book.find_element_by_tag_name('h2').text) print('-----------------------------------') #进入下一页 show_next_page()def show_next_page(): #获取下一页的标签 #&lt;span class="next"&gt; span_next = wait.until(EC.presence_of_element_located((By.CLASS_NAME,'next'))) try: #获取span中的a标签 a_next = span_next.find_element_by_tag_name('a') a_next.click() show_current_page() except NoSuchElementException: #已经没有下一页，可以退出了 print('now print all pages.') finally: browser.quit() show_current_page() #time.sleep(10)#退出browser.quit() 这里的元素获取方式，不是最佳的，代码还得继续优化，这个下一页挺好玩儿的， 刚刚简单优化了下代码，主要改了些元素选择的方式，更直接了：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.common.exceptions import TimeoutException#打开浏览器browser = webdriver.Firefox()#设置等待时长，最长等待10swait = WebDriverWait(browser,10) def main(): #打开URL browser.get('https://book.douban.com/') #输出浏览器标题 print('browser title: ',browser.title) #获取搜索框 inp_query = wait.until(EC.presence_of_element_located((By.ID,'inp-query'))) inp_query.send_keys('onepiece') #可以直接提交，或者获取提交按钮再提交 inp_query.submit() show_current_page()def show_current_page(): print('-----------------------------------') print('current url: ',browser.current_url) #搜索结果-汇总信息 page_info = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,'div.article div.trr'))) print('info: ',page_info.text) #搜索结果-书籍列表 book_list = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR,'div.article ul.subject-list li.subject-item'))) print('current page total book: ',len(book_list)) for book in book_list: print(book.find_element_by_tag_name('h2').text) print('-----------------------------------') #进入下一页 show_next_page()def show_next_page(): try: #获取下一页的标签 a_next = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,'div.article div.paginator span.next a'))) a_next.click() show_current_page() except TimeoutException: print('超时了.') span_next = EC.invisibility_of_element_located((By.CSS_SELECTOR,'div.article div.paginator span.next')) if span_next: print('now print all pages.') else : #真的超时，重试一次 show_next_page() finally: browser.quit()if __name__=='__main__': main()]]></content>
      <categories>
        <category>Python-基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas手册（6）- 用pandas完成excel中常见任务]]></title>
    <url>%2F2017%2F08%2F07%2Fpandas-handbook-06%2F</url>
    <content type="text"><![CDATA[PythonPandas 这里整理下pandas常用的操作，为什么要写这个呢？有本书《利用Python进行数据分析》一边看一遍记录下。 1. 重新索引(reindex)就是重构一下索引，在重构的同时，我们可以做一些其他操作12345DataFrame.reindex(index=None, columns=None, **kwargs)Conform DataFrame to new index with optional filling logic, placing NA/NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=FalseSeries.reindex(index=None, **kwargs)Conform Series to new index with optional filling logic, placing NA/NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False 一个小例子123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172obj = pd.Series([4.5, 7.2, -5.3, 3.6], index=['d', 'b', 'a', 'c'])objOut[156]: d 4.5b 7.2a -5.3c 3.6dtype: float64#reindex后，没有的值，默认会用NaN填充obj.reindex(['a','b','c','d','e'])Out[157]: a -5.3b 7.2c 3.6d 4.5e NaNdtype: float64#fill_value，常用的参数，表示没有数据时默认填充的值obj.reindex(['a','b','c','d','e'] , fill_value=9.9)Out[159]: a -5.3b 7.2c 3.6d 4.5e 9.9dtype: float64#method,常用参数，在递增或递减index中，填充空值的方法obj3 = pd.Series(['blue', 'purple', 'yellow'], index=[0, 2, 4])obj3Out[165]: 0 blue2 purple4 yellowdtype: objectobj3.reindex(range(6))Out[170]: 0 blue1 NaN2 purple3 NaN4 yellow5 NaNdtype: object#ffill，前向填充obj3.reindex(range(6),method='ffill')Out[167]: 0 blue1 blue2 purple3 purple4 yellow5 yellowdtype: object#bfill，后向填充obj3.reindex(range(6),method='bfill')Out[171]: 0 blue1 purple2 purple3 yellow4 yellow5 NaNdtype: object 对于DataFrame来说，用起来也是差不多的 2. 丢弃指定轴上的项主要就是drop方法的使用12DataFrame.drop(labels, axis=0, level=None, inplace=False, errors='raise')Return new object with labels in requested axis removed. 小例子12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455obj = pd.Series(np.arange(5.), index=['a', 'b', 'c', 'd', 'e'])objOut[174]: a 0.0b 1.0c 2.0d 3.0e 4.0dtype: float64obj.drop('c')Out[175]: a 0.0b 1.0d 3.0e 4.0dtype: float64obj.drop(['b','d'])Out[176]: a 0.0c 2.0e 4.0dtype: float64#DataFramedata = pd.DataFrame(np.arange(16).reshape((4, 4)), index=['Ohio', 'Colorado', 'Utah', 'New York'], columns=['one', 'two', 'three', 'four'])dataOut[178]: one two three fourOhio 0 1 2 3Colorado 4 5 6 7Utah 8 9 10 11New York 12 13 14 15#默认是横轴，data.drop(['Ohio','Utah'])Out[179]: one two three fourColorado 4 5 6 7New York 12 13 14 15#我们可以指定axis，在columns上删除data.drop(['two','four'],axis=1)Out[180]: one threeOhio 0 2Colorado 4 6Utah 8 10New York 12 14 3. 算术运算和数据对齐在numpy和pandas中好像都会看到这个词，数据对齐，就是说2个对象在运算的时候，会取一个并集，然后在自动对齐的时候，不重叠的部分就会填充NaN 小例子先看看123456789101112131415161718192021222324252627s1 = pd.Series([7.3, -2.5, 3.4, 1.5], index=['a', 'c', 'd', 'e'])s2 = pd.Series([-2.1, 3.6, -1.5, 4, 3.1], index=['a', 'c', 'e', 'f', 'g'])#index不重叠的地方，会填充NaNs1+s2Out[188]: a 5.2c 1.1d NaNe 0.0f NaNg NaNdtype: float64#使用自带的add方法，就可以填充默认值了，这个和我们上面reindex时的思想是一样的#Series.add(other, level=None, fill_value=None, axis=0)s1.add(s2,fill_value=0)Out[189]: a 5.2c 1.1d 3.4e 0.0f 4.0g 3.1dtype: float64 4.DataFrame和Series之间的运算这里用到了一个广播的思想，就是指不同形状的数组之间的算术运算的执行方式，很强大的功能，这里，我们先简单了解下。小例子1234567891011121314151617arr = np.arange(12.).reshape((3, 4))arrOut[191]: array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]])arr[0]Out[192]: array([ 0., 1., 2., 3.])#3行4列的数组，减1行4列的数组，这就是广播arr - arr[0]Out[193]: array([[ 0., 0., 0., 0.], [ 4., 4., 4., 4.], [ 8., 8., 8., 8.]]) DataFrame和Series之间的计算也是这样12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364frame = pd.DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])frameOut[195]: b d eUtah 0.0 1.0 2.0Ohio 3.0 4.0 5.0Texas 6.0 7.0 8.0Oregon 9.0 10.0 11.0s = frame.iloc[0]sOut[197]: b 0.0d 1.0e 2.0Name: Utah, dtype: float64frame - sOut[198]: b d eUtah 0.0 0.0 0.0Ohio 3.0 3.0 3.0Texas 6.0 6.0 6.0Oregon 9.0 9.0 9.0s = pd.Series(range(3),index=list('abc'))frameOut[223]: b d eUtah 0.0 1.0 2.0Ohio 3.0 4.0 5.0Texas 6.0 7.0 8.0Oregon 9.0 10.0 11.0sOut[224]: a 0b 1c 2dtype: int32frame.add(s)Out[225]: a b c d eUtah NaN 1.0 NaN NaN NaNOhio NaN 4.0 NaN NaN NaNTexas NaN 7.0 NaN NaN NaNOregon NaN 10.0 NaN NaN NaN#我们可以通过axis控制在哪个方向上去广播frame.add(s,axis=0)Out[227]: b d eOhio NaN NaN NaNOregon NaN NaN NaNTexas NaN NaN NaNUtah NaN NaN NaNa NaN NaN NaNb NaN NaN NaNc NaN NaN NaN 在这里，不能使用fill_value填充默认值，还不知道为啥，总是报错，说不支持 5. 函数应用和映射这里主要是介绍DataFrame中的一个函数使用，apply，就是对DataFrame中的每一个元素执行传入的函数12DataFrame.apply(func, axis=0, broadcast=False, raw=False, reduce=None, args=(), **kwds)Applies function along input axis of DataFrame. 小例子1234567891011121314151617181920212223242526272829f = lambda x: x+10#每一个单元格都会加10frame.apply(f)Out[230]: b d eUtah 10.0 11.0 12.0Ohio 13.0 14.0 15.0Texas 16.0 17.0 18.0Oregon 19.0 20.0 21.0f = lambda x: x.max() - x.min()frame.apply(f)Out[232]: b 9.0d 9.0e 9.0dtype: float64#我们可以指定轴，去执行函数frame.apply(f,axis=1)Out[233]: Utah 2.0Ohio 2.0Texas 2.0Oregon 2.0dtype: float64 这里还有一个applymap函数123DataFrame.applymap(func)Apply a function to a DataFrame that is intended to operate elementwise, i.e. like doing map(func, series) for each series in the DataFrame 这里得注意下，这2个函数的区别；目前的理解是，applymap是元素级的，apply在轴上进行操作（貌似不太顺，等明白了再记录下）123456789101112131415161718f = lambda x: '$&#123;:,.3f&#125;'.format(x)frameOut[237]: b d eUtah 0.0 1.0 2.0Ohio 3.0 4.0 5.0Texas 6.0 7.0 8.0Oregon 9.0 10.0 11.0#前面，我们有用过，格式化内容的frame.applymap(f)Out[238]: b d eUtah $0.000 $1.000 $2.000Ohio $3.000 $4.000 $5.000Texas $6.000 $7.000 $8.000Oregon $9.000 $10.000 $11.000 6.处理缺失数据在pandas中处理缺失数据非常容易，pandas使用浮点值NaN（Not a Number）表示缺失值。前面，我们说过使用isnull来判断是否有NaN值小例子12345678910111213141516171819202122232425262728293031323334353637383940414243444546a = pd.Series(['one','two',np.nan,'three'])aOut[240]: 0 one1 two2 NaN3 threedtype: objecta.isnull()Out[241]: 0 False1 False2 True3 Falsedtype: boola.notnull()Out[242]: 0 True1 True2 False3 Truedtype: bool#Python内置的None也会被当做NaN处理a[4]=NoneaOut[247]: 0 one1 two2 NaN3 three4 Nonedtype: objecta.isnull()Out[248]: 0 False1 False2 True3 False4 Truedtype: bool 对于这种数据，我们要怎样处理呢？有的时候，我们可能会初始化为默认值，或者直接剔除掉我们可以使用dropna函数来剔除掉，或者布尔类型索引1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)aOut[249]: 0 one1 two2 NaN3 three4 Nonedtype: objecta.dropna()Out[250]: 0 one1 two3 threedtype: objecta[a.notnull()]Out[251]: 0 one1 two3 threedtype: object##dataframedata = pd.DataFrame([[1., 6.5, 3.], [1., np.nan, np.nan], [np.nan, np.nan, np.nan], [np.nan, 6.5, 3.]])dataOut[254]: 0 1 20 1.0 6.5 3.01 1.0 NaN NaN2 NaN NaN NaN3 NaN 6.5 3.0#默认的话，会将行、列含有NaN的都剔除掉data.dropna()Out[255]: 0 1 20 1.0 6.5 3.0#我们可以使用参数how来控制how : &#123;‘any’, ‘all’&#125; any : if any NA values are present, drop that label all : if all values are NA, drop that labeldata.dropna(how='all')Out[257]: 0 1 20 1.0 6.5 3.01 1.0 NaN NaN3 NaN 6.5 3.0 有的时候，我们想要做填充而不是剔除，像我们前面使用的参数fill_value12345678910111213141516171819202122232425262728DataFrame.fillna(value=None, method=None, axis=None, inplace=False, limit=None, downcast=None, **kwargs)Fill NA/NaN values using the specified methodmethod : &#123;‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, None&#125;, default NonedataOut[261]: 0 1 20 1.0 6.5 3.01 1.0 NaN NaN2 NaN NaN NaN3 NaN 6.5 3.0data.fillna(9.9)Out[259]: 0 1 20 1.0 6.5 3.01 1.0 9.9 9.92 9.9 9.9 9.93 9.9 6.5 3.0#使用method，和前面reindex的时候是一个道理data.fillna(method='ffill')Out[262]: 0 1 20 1.0 6.5 3.01 1.0 6.5 3.02 1.0 6.5 3.03 1.0 6.5 3.0]]></content>
      <categories>
        <category>Python-Pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy手册(2)-常用操作杂记]]></title>
    <url>%2F2017%2F08%2F06%2Fnumpy-handbook-02%2F</url>
    <content type="text"><![CDATA[PythonNumpy知识总结 这里记录下numpy常用的一些操作，一些散乱的知识点。 数组和标量之间的运算就是对数组进行批量的运算123456789101112131415161718192021import numpy as npa = np.arange(15).reshape(3,5)aOut[3]: array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]])a+2Out[4]: array([[ 2, 3, 4, 5, 6], [ 7, 8, 9, 10, 11], [12, 13, 14, 15, 16]])a+aOut[5]: array([[ 0, 2, 4, 6, 8], [10, 12, 14, 16, 18], [20, 22, 24, 26, 28]]) 基本的索引和切片上一篇，我们大概介绍过数组的切片，这里的切片操作都是原数组的一个视图，即原数组变化后，视图也会变化123456789101112131415161718192021222324252627aOut[7]: array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]])a[1]Out[8]: array([5, 6, 7, 8, 9])a[1][2]Out[9]: 7a[1:2]Out[10]: array([[5, 6, 7, 8, 9]])b=a[1:2]a[1:2]=0bOut[13]: array([[0, 0, 0, 0, 0]])aOut[14]: array([[ 0, 1, 2, 3, 4], [ 0, 0, 0, 0, 0], [10, 11, 12, 13, 14]]) 花式索引花式索引，就是利用整数数组进行索引。123456789101112131415161718192021x = np.empty((8,4))for i in range(8): x[i]=i xOut[18]: array([[ 0., 0., 0., 0.], [ 1., 1., 1., 1.], [ 2., 2., 2., 2.], ..., [ 5., 5., 5., 5.], [ 6., 6., 6., 6.], [ 7., 7., 7., 7.]])x[[3,0,5]]Out[19]: array([[ 3., 3., 3., 3.], [ 0., 0., 0., 0.], [ 5., 5., 5., 5.]]) 如果使用负数，会从末尾开始选取1234x[[-1,-2]]Out[20]: array([[ 7., 7., 7., 7.], [ 6., 6., 6., 6.]]) 数组的转置12345678910111213141516xOut[21]: array([[ 0., 0., 0., 0.], [ 1., 1., 1., 1.], [ 2., 2., 2., 2.], ..., [ 5., 5., 5., 5.], [ 6., 6., 6., 6.], [ 7., 7., 7., 7.]])x.TOut[22]: array([[ 0., 1., 2., ..., 5., 6., 7.], [ 0., 1., 2., ..., 5., 6., 7.], [ 0., 1., 2., ..., 5., 6., 7.], [ 0., 1., 2., ..., 5., 6., 7.]]) 上面的主要是轴对换，转置的话，看下这个例子（大学学的矩阵都忘了，得回顾下…） 一开始真迷糊了，找了个解释，明天再研究下这个函数]]></content>
      <categories>
        <category>Python-Numpy</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础（6）- zip]]></title>
    <url>%2F2017%2F08%2F06%2Fpython-handbook-06%2F</url>
    <content type="text"><![CDATA[这里记录一个函数的使用，zip123456zip(iter1 [,iter2 [...]]) --&gt; zip objectReturn a zip object whose .__next__() method returns a tuple wherethe i-th element comes from the i-th iterable argument. The .__next__()method continues until the shortest iterable in the argument sequenceis exhausted and then it raises StopIteration. 我们可以传入一个或多个可迭代对象，然后将对应位置的元素封装成一个tuple，然后把所有tuple封装为list返回1234567891011121314151617181920x = [1,2,3]y = [4,5,6]z = zip(x,y)print(z)&lt;zip object at 0x000002D26251F208&gt;print(list(z))[(1, 4), (2, 5), (3, 6)]#在只有一个参数的时候a = zip(x)aOut[8]: &lt;zip at 0x2d261cab588&gt;list(a)Out[9]: [(1,), (2,), (3,)] 如果2个参数的长度不一样，会以较短的为主12345678a = [1,2,3]b = ['ONE', 'TWO', 'THREE', 'FOUR']c = list(zip(a,b))cOut[31]: [(1, 'ONE'), (2, 'TWO'), (3, 'THREE')] 我们使用*，可以看做是unzip的过程12345678x = [1,2,3]y = [4,5,6]z = zip(*zip(x,y))list(z)Out[27]: [(1, 2, 3), (4, 5, 6)] 关于zip，有几个常用的场景，比如矩阵行列转换1234567a = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]list(zip(*a))Out[38]: [(1, 4, 7), (2, 5, 8), (3, 6, 9)]list(map(list,zip(*a)))Out[39]: [[1, 4, 7], [2, 5, 8], [3, 6, 9]] 构造字典123456789a = [1,2,3]b = ['one','two','three']list(zip(a,b))Out[42]: [(1, 'one'), (2, 'two'), (3, 'three')]dict(list(zip(a,b)))Out[43]: &#123;1: 'one', 2: 'two', 3: 'three'&#125; 参考博客：http://blog.csdn.net/shomy_liu/article/details/46968651http://www.jb51.net/article/53051.htm]]></content>
      <categories>
        <category>Python-基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础（5）- csv]]></title>
    <url>%2F2017%2F08%2F05%2Fpython-handbook-05%2F</url>
    <content type="text"><![CDATA[这里简单介绍下Python中的csv模块，应该蛮常用的。和csv有关，一定要回合打开文件这类操作有关，这里先看下这个open函数官方文档：https://docs.python.org/3/library/functions.html#open 1.open open(file, mode=’r’, buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)Open file and return a corresponding file object. If the file cannot be opened, an OSError is raised. file，就是我们要打开的文件地址；mode，就是打开的方式，默认是只读文本（’rt’) newline，和换行符有关，在网上找了个资料：https://www.zhihu.com/question/19751023 换行符看起来有点儿乱，以后如果一段问题了再研究下。下面，我们先来看看csv 2. csv.reader csv.reader(csvfile, dialect=’excel’, **fmtparams)Return a reader object which will iterate over lines in the given csvfile. 我们的csv文件是这样的 1234567891011import csvwith open(r'D:\document\python_demo\employee_data.csv') as csvfile: emp_reader = csv.reader(csvfile) for row in emp_reader: print(row)##runfile('D:/document/python_demo/demo_open.py', wdir='D:/document/python_demo')['lufei', '20', 'leader', 'onepiece', '100']['namei', '19', 'teacher', 'onepiece', '999'] 就csv文件来说，会有几个特点，比如字段之间的分隔符，换行符等，我们使用上面的dialect来指定如果我们，现在将分隔符，替换为^ 我们再次执行，就无法正确分割数据了123runfile('D:/document/python_demo/demo_open.py', wdir='D:/document/python_demo')['lufei^20^leader^onepiece^100']['namei^19^teacher^onepiece^999'] 我们修改下代码，加上delimiter就行了，详情参考官网：https://docs.python.org/3/library/csv.html#csv-fmt-params 1234567891011import csvwith open(r'D:\document\python_demo\employee_data.csv') as csvfile: emp_reader = csv.reader(csvfile,delimiter='^') for row in emp_reader: print(row)##runfile('D:/document/python_demo/demo_open.py', wdir='D:/document/python_demo')['lufei', '20', 'leader', 'onepiece', '100']['namei', '19', 'teacher', 'onepiece', '999'] 这时候，如果我们的数据中，含有分隔符，我们需要再加上封闭符，一般都会使用双引号，这里使用参数quotechar指定，默认是双引号12345678910111213141516csv data:lufei^20^leader^$one^_^piece$^100namei^19^teacher^onepiece^999import csvwith open(r'D:\document\python_demo\employee_data.csv') as csvfile: emp_reader = csv.reader(csvfile,delimiter='^',quotechar='$') for row in emp_reader: print(row)result:runfile('D:/document/python_demo/demo_open.py', wdir='D:/document/python_demo')['lufei', '20', 'leader', 'one^_^piece', '100']['namei', '19', 'teacher', 'onepiece', '999'] 3.csv.writer csv.writer(csvfile, dialect=’excel’, **fmtparams)Return a writer object responsible for converting the user’s data into delimited strings on the given file-like object. csvfile can be any object with a write() method. 这里的用法都差不多，我们简单举个小例子，用官网的例子123456789with open('eggs.csv', 'w', newline='') as csvfile: spamwriter = csv.writer(csvfile, delimiter=' ', quotechar='|', quoting=csv.QUOTE_MINIMAL) spamwriter.writerow(['Spam'] * 5 + ['Baked Beans']) spamwriter.writerow(['Spam', 'Lovely |Spam', 'Wonderful Spam']) result：|Spam| |Spam| |Spam| |Spam| |Spam| |Baked Beans||Spam| |Lovely ||Spam| |Wonderful Spam| 这里用到了另一个参数：quoting，这个参数是针对quotechar来说的，quotechar在ETL工具中叫做封闭符，是为了防止字段内容中出现分割符，我们需要区分到底是分隔符，还是字段内容，所以需要根据quotechar去判断；quoting则是控制在什么情况下使用封闭符，他有几个选项1234csv.QUOTE_ALL #所有字段都添加封闭符csv.QUOTE_NONNUMERIC #在非数值字段加封闭符csv.QUOTE_NONE #所有字段都不加csv.QUOTE_MINIMAL #只在出现分隔符的字段旁加封闭符，默认 好了，csv的就简单分享到这里了。]]></content>
      <categories>
        <category>Python-基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础（4）- collections]]></title>
    <url>%2F2017%2F08%2F05%2Fpython-handbook-04%2F</url>
    <content type="text"><![CDATA[昨天用到了这个collections模块，挺好用的，这里记录下。官网介绍：https://docs.python.org/3/library/collections.html博客：廖雪峰的博客这里介绍些好玩儿的例子。 namedtuple collections.namedtuple(typename, field_names, *, verbose=False, rename=False, module=None)Returns a new tuple subclass named typename. The new subclass is used to create tuple-like objects that have fields accessible by attribute lookup as well as being indexable and iterable. Instances of the subclass also have a helpful docstring (with typename and field_names) and a helpful repr() method which lists the tuple contents in a name=value format. namedtuple是一个工厂函数，返回一个自定义的tuple类，可读性更强些。通常我们使用tuple的时候，像这样123456789101112131415point_a = 1,3point_b = 2,6point_aOut[37]: (1, 3)point_bOut[38]: (2, 6)point_a[0]Out[39]: 1point_a[1]Out[40]: 3 我们是那个namedtuple就可以这样了12345678910111213141516171819from collections import namedtuplePoint = namedtuple('Point',['x','y'])point_a = Point(2,2)point_b = Point(3,3)point_aOut[45]: Point(x=2, y=2)point_bOut[46]: Point(x=3, y=3)point_a.xOut[47]: 2point_b.yOut[48]: 3 这样使用一个坐标位置，是不是可读性更强呢，而且用起来也很方便我们可以看看这个Point是怎样定义的12345678910111213141516171819202122232425262728293031323334353637383940414243444546print(point_a._source)from builtins import property as _property, tuple as _tuplefrom operator import itemgetter as _itemgetterfrom collections import OrderedDictclass Point(tuple): 'Point(x, y)' __slots__ = () _fields = ('x', 'y') def __new__(_cls, x, y): 'Create new instance of Point(x, y)' return _tuple.__new__(_cls, (x, y)) @classmethod def _make(cls, iterable, new=tuple.__new__, len=len): 'Make a new Point object from a sequence or iterable' result = new(cls, iterable) if len(result) != 2: raise TypeError('Expected 2 arguments, got %d' % len(result)) return result def _replace(_self, **kwds): 'Return a new Point object replacing specified fields with new values' result = _self._make(map(kwds.pop, ('x', 'y'), _self)) if kwds: raise ValueError('Got unexpected field names: %r' % list(kwds)) return result def __repr__(self): 'Return a nicely formatted representation string' return self.__class__.__name__ + '(x=%r, y=%r)' % self def _asdict(self): 'Return a new OrderedDict which maps field names to their values.' return OrderedDict(zip(self._fields, self)) def __getnewargs__(self): 'Return self as a plain tuple. Used by copy and pickle.' return tuple(self) x = _property(_itemgetter(0), doc='Alias for field number 0') y = _property(_itemgetter(1), doc='Alias for field number 1') 下面还有个更好用的地方，我们再读取CSV或者数据库的时候，会返回结果集，这个时候用起来更方便，比如：1234567891011121314import csvfrom collections import namedtuple EmployeeRecord = namedtuple('EmployeeRecord', 'name, age, title, department, paygrade')for emp in map(EmployeeRecord._make, csv.reader(open(r'D:\document\python_demo\employee_data.csv'))): print(emp.name, emp.title) print('emp:',emp)runfile('D:/document/python_demo/demo_hi.py', wdir='D:/document/python_demo')lufei leaderemp: EmployeeRecord(name='lufei', age='20', title='leader', department='onepiece', paygrade='100')namei teacheremp: EmployeeRecord(name='namei', age='19', title='teacher', department='onepiece', paygrade='999') _make12somenamedtuple._make(iterable)Class method that makes a new instance from an existing sequence or iterable. deque我们使用list的时候，用下标查找很快，数据量大的时候，插入删除比较慢，deque是为了高效实现插入和删除的双向队列。 deque:double-ended queue class collections.deque([iterable[, maxlen]])Returns a new deque object initialized left-to-right (using append()) with data from iterable. If iterable is not specified, the new deque is empty. 123456789101112131415161718192021222324from collections import dequea = deque(list('abcdef'))aOut[80]: deque(['a', 'b', 'c', 'd', 'e', 'f'])a.append('x')a.append('y')aOut[83]: deque(['a', 'b', 'c', 'd', 'e', 'f', 'x', 'y'])a.appendleft('w')aOut[85]: deque(['w', 'a', 'b', 'c', 'd', 'e', 'f', 'x', 'y'])a.pop()Out[86]: 'y'a.popleft()Out[87]: 'w' 这里扩展了很多方便的函数，appendleft(),popleft()等等 defaultdict可以设置默认值的dict，平时我们使用dict的时候，如果key不存在，会报错 class collections.defaultdict([default_factory[, …]])Returns a new dictionary-like object. defaultdict is a subclass of the built-in dict class. It overrides one method and adds one writable instance variable. The remaining functionality is the same as for the dict class and is not documented here. 123456789101112131415161718a = &#123;'name':'lufe','age':20&#125;aOut[105]: &#123;'age': 20, 'name': 'lufe'&#125;a['name']Out[106]: 'lufe'a['age']Out[107]: 20a['score']Traceback (most recent call last): File "&lt;ipython-input-108-99f54e089332&gt;", line 1, in &lt;module&gt; a['score']KeyError: 'score' 我们使用defaultdict就可以避免这个错误1234567891011from collections import defaultdictb = defaultdict(int)b['name']='lufei'bOut[123]: defaultdict(int, &#123;'name': 'lufei'&#125;)b['age']Out[124]: 0 这里我们设置默认是int型，默认值为012345678910111213141516x = defaultdict(0)Traceback (most recent call last): File "&lt;ipython-input-125-dd2052e23af0&gt;", line 1, in &lt;module&gt; x = defaultdict(0)TypeError: first argument must be callable or Nonex = defaultdict(lambda : 100)xOut[127]: defaultdict(&lt;function __main__.&lt;lambda&gt;&gt;, &#123;&#125;)x['name']Out[128]: 100 Counter是一个简单的计数器， class collections.Counter([iterable-or-mapping])A Counter is a dict subclass for counting hashable objects. It is an unordered collection where elements are stored as dictionary keys and their counts are stored as dictionary values. Counts are allowed to be any integer value including zero or negative counts. The Counter class is similar to bags or multisets in other languages. 123456789101112131415161718from collections import Countercnt = Counter(['red', 'blue', 'red', 'green', 'blue', 'blue'])cntOut[131]: Counter(&#123;'blue': 3, 'green': 1, 'red': 2&#125;)cnt.most_common(1)Out[132]: [('blue', 3)]cnt.most_common(-1)Out[133]: []cnt.elementsOut[134]: &lt;bound method Counter.elements of Counter(&#123;'blue': 3, 'red': 2, 'green': 1&#125;)&gt;cnt.most_common(3)[:-2:-1]Out[137]: [('green', 1)] 这个most_common最好用了感觉，根据次数进行排名 当然，collections中还有很多其他的好用的类，我们可以参考官方文档。]]></content>
      <categories>
        <category>Python-基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy手册(1)-ndarray]]></title>
    <url>%2F2017%2F08%2F02%2Fnumpy-handbook-01%2F</url>
    <content type="text"><![CDATA[前面我们算是简单入门了Pandas，numpy也是数据分析中常用的，这里我们也来简单学习下。 1.numpy基本介绍numpy是Python的一种开源数值计算扩展，这种工具可以用来存储和处理大型矩阵。一个用Python实现的科学计算包。from 百度百科 numpy有2种基本对象，1ndarray（N-dimensional array object）和 ufunc（universal function object） ndarray是存储单一数据类型的多维数组，ufunc是能够对数组进行处理的函数。 2.ndarray我们先来看看这个数组首先，我们得引入numpy1import numpy as np 2.1 创建数组初始化的话有很多方式：Array creation routines我们可以直接使用list来初始化，array有很多的属性，比如大小，维度，元素个数123456789import numpy as npa = np.array([1,2,3])b = np.array([4,5,6])c = np.array([[1,2,3],[4,5,6],[7,8,9]])print(a,type(a),',shape:',a.shape,',ndim:',a.ndim,',size:',a.size)print(b,type(b),',shape:',b.shape,',ndim:',b.ndim,',size:',b.size)print(c,type(c),',shape:',c.shape,',ndim:',c.ndim,',size:',c.size) 这里呢，我们定义了一维数组和二维数组，比如c，是3行3列的2维数组，元素个数是9个1numpy.array(object, dtype=None, copy=True, order='K', subok=False, ndmin=0) 这里，我们再说下这个shape，这个属性可以修改123456789#原来是4行3列c = np.array([[1,2,3],[4,5,6],[7,8,9],[0,0,7]])print(c)#我们改为3行4列c.shape=(3,4)print(c)#改为2行6列c.shape=(2,6)print(c) 这里需要注意下，如果某个轴的元素为-1，将根据数组元素的个数，自动计算长度1234c.shape=(1,-1)print(c)c.shape=(-1,1)print(c) 这里的shape是改变原来的数组，另一个method，可以创建一个改变shape的新数组，而原数组保持不变12345c = np.array([[1,2,3],[4,5,6],[7,8,9],[0,0,7]])print('c:',c)d = c.reshape(2,6)print('c:',c)print('d:',d) 这里要注意的是，c和d共享内存数据存储内存区域，c变了，d也会变12345print(c[0])#修改c[0]c[0]=[-9,-8,-3]print('c:',c)print('d:',d) 我们可以通过dtype来获取元素的类型，我们可以在初始化的时候，指定dtype12345c = np.array([1,2,3])print(c.dtype) #int32d = np.array([1.1,3.3])print(d.dtype) #float64 下面，我们来看看常用的初始化方法 arange通过指定开始值，结束值和步长来创建一维数组，这里不包过终值1234567arange([start,] stop[, step,], dtype=None)np.arange(3)Out[51]: array([0, 1, 2])np.arange(1,10,3)Out[52]: array([1, 4, 7]) linspace通过指定开始值，终值和元素个数，来创建数组，这里包括终值12345np.linspace(1,10,5)Out[53]: array([ 1. , 3.25, 5.5 , 7.75, 10. ])np.linspace(1,2,3)Out[54]: array([ 1. , 1.5, 2. ]) np.zeros,np.ones这2个函数可以初始化指定长度或形状的全0或全1的数组1234567891011121314151617np.ones(3)Out[202]: array([ 1., 1., 1.])np.ones([2,2])Out[203]: array([[ 1., 1.], [ 1., 1.]])np.zeros(5)Out[204]: array([ 0., 0., 0., 0., 0.])np.zeros([4,3])Out[205]: array([[ 0., 0., 0.], [ 0., 0., 0.], [ 0., 0., 0.], [ 0., 0., 0.]]) np.empty可以创建一个没有任何具体值得数组1234567891011np.empty(2)Out[211]: array([ 7.74860419e-304, 7.74860419e-304])np.empty(2,dtype=int)Out[214]: array([ -1, 2147483647])np.empty((3,3),dtype=np.float64)Out[215]: array([[ 4.94065646e-324, 9.88131292e-324, 1.48219694e-323], [ 1.97626258e-323, 2.47032823e-323, 2.96439388e-323], [ 3.45845952e-323, 3.95252517e-323, 4.44659081e-323]]) 这要注意下，empty初始化的都是没有意思的值，不一定会初始化为0 2.2 存取元素这里直接粘贴一个例子，原始教程在这：http://old.sebug.net/paper/books/scipydoc/numpy_intro.html123456789101112131415161718&gt;&gt;&gt; a = np.arange(10)&gt;&gt;&gt; a[5] # 用整数作为下标可以获取数组中的某个元素5&gt;&gt;&gt; a[3:5] # 用范围作为下标获取数组的一个切片，包括a[3]不包括a[5]array([3, 4])&gt;&gt;&gt; a[:5] # 省略开始下标，表示从a[0]开始array([0, 1, 2, 3, 4])&gt;&gt;&gt; a[:-1] # 下标可以使用负数，表示从数组后往前数array([0, 1, 2, 3, 4, 5, 6, 7, 8])&gt;&gt;&gt; a[2:4] = 100,101 # 下标还可以用来修改元素的值&gt;&gt;&gt; aarray([ 0, 1, 100, 101, 4, 5, 6, 7, 8, 9])&gt;&gt;&gt; a[1:-1:2] # 范围中的第三个参数表示步长，2表示隔一个元素取一个元素array([ 1, 101, 5, 7])&gt;&gt;&gt; a[::-1] # 省略范围的开始下标和结束下标，步长为-1，整个数组头尾颠倒array([ 9, 8, 7, 6, 5, 4, 101, 100, 1, 0])&gt;&gt;&gt; a[5:1:-2] # 步长为负数时，开始下标必须大于结束下标array([ 5, 101]) 就2维数组来说 这是基本的获取方式，还有些高级的方法 使用整数序列这里简单来2个练习，原文例子很多，就是通过下标来筛选数据12345678910111213141516171819202122a = np.arange(-5,5,1)aOut[68]: array([-5, -4, -3, -2, -1, 0, 1, 2, 3, 4])a[[1,3,5]]Out[69]: array([-4, -2, 0])### 使用布尔数组按照传入的布尔数组，只有为True的才返回，也叫布尔型索引``` pythona=np.array([-3,1,5])aOut[72]: array([-3, 1, 5])a[[False,True,False]]Out[73]: array([1])a[[True,False,True]]Out[74]: array([-3, 5]) 3.附录（参考资料）文档：https://docs.scipy.org/doc/numpy-dev/reference/index.html#reference numpy快速处理数据]]></content>
      <categories>
        <category>Python-Numpy</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cognos资料汇总贴]]></title>
    <url>%2F2017%2F08%2F01%2Fcognos-doc-main%2F</url>
    <content type="text"><![CDATA[以前搞过Cognos，写过很多基础的教程，应该是14年的样子，都在CSDN上，这里贴个汇总贴吧，想要看的同学可以去看看，希望有帮助。 ReportStudio入门教程：http://blog.csdn.net/column/details/ygy-reportstudio.html Framework Manage入门教程：http://blog.csdn.net/column/details/ygy-frameworkmanager.html Cognos函数手册：http://blog.csdn.net/column/details/ygy-cognos-function.html Cognos相关的其他资料（主页不同的类别下看看）：http://blog.csdn.net/yuguiyang1990 好了，感兴趣的同学，可以自行去看看，好久不搞了，估计有疑问也解决不了了…]]></content>
      <categories>
        <category>数据可视化-Cognos</category>
      </categories>
      <tags>
        <tag>数据可视化</tag>
        <tag>Cognos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小白学习Tableau-购物篮分析]]></title>
    <url>%2F2017%2F08%2F01%2FTableau-handbook-04%2F</url>
    <content type="text"><![CDATA[Tableau实例 关于购物篮的介绍可以参考数据分析案例-购物篮分析 目标我们想要分析的是顾客在购买商品的时候，哪些商品会同时购买。 下面，我们直接开始开发Tableau 数据源我们就是用Tableau默认自带的“示例-超市” 拖入两张订单表因为我们需要分析的是每个顾客，在购买A商品的时候，还会购买哪些商品。只使用一张订单表，不是很容易看出来，所以我们需要拖入2张订单表，以此来更方便的处理数据。因为我们要分析的是每个顾客，所以我们使用顾客ID去关联 拖子类别进行分析我们按照下图所示，进行拖拽 剔除无效的值手工剔除，好麻烦，如果数据是在数据库中，直接用SQL处理掉就可以了 这里，我们就快速的实现了购物篮分析，可以看到在购买某种商品时，同时购买其他商品的数量。 参考文章人人都是可视化分析师系列：用Tableau玩转购物篮分析]]></content>
      <categories>
        <category>Tableau</category>
      </categories>
      <tags>
        <tag>数据可视化</tag>
        <tag>Tableau</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础（3）- lambda表达式]]></title>
    <url>%2F2017%2F08%2F01%2Fpython-handbook-03%2F</url>
    <content type="text"><![CDATA[这里简单整理下，lambda表达式相关内容。 什么是lambda表达式lambda表达式，是一个匿名函数，用起来方便快捷一些123456#lambda 参数:操作(参数)fun_add = lambda x: x+1print(fun_add(1))print(fun_add(10)) 这里，一个简单的加1的函数，看起来也很直观1234fun_add = lambda x,y: x+yprint(fun_add(3,4))print(fun_add(8,9)) 这是x+y的函数，的确简洁很多看网上，提到lambda表达式的话，都会提到函数式编程，一些常用的函数，像map,reduce,filter,sorted， map函数map是Python内置的一个函数，接收2个参数，一个函数，一个或多个可迭代参数1234map(func, *iterables) --&gt; map objectMake an iterator that computes the function using arguments fromeach of the iterables. Stops when the shortest iterable is exhausted. 12345678d1 = [1,2,3,4,5]def add(x): return x+10t = map(add , d1)print('原来的list:',d1)print('执行add后的list:',list(t)) 我们定义了一个函数，对传入的参数加10，一个list map把这个函数，作用在每一个list的元素上，这里呢，我们就可以用lambda表达式写，方便又直观1234d1 = [1,2,3,4,5]t = map(lambda x: x+10 , d1)print('原来的list:',d1)print('执行add后的list:',list(t)) 我们也可以传2个list，这里会计算2个list的和1234s1 = [1,2,3]s2 = [4,5,6]t = map(lambda x,y: x+y,s1,s2)print(list(t)) ##[5, 7, 9] reduce函数reduce会将function作用于sequence，function接收2个参数12345678reduce(function, sequence[, initial]) -&gt; valueApply a function of two arguments cumulatively to the items of a sequence,from left to right, so as to reduce the sequence to a single value.For example, reduce(lambda x, y: x+y, [1, 2, 3, 4, 5]) calculates((((1+2)+3)+4)+5). If initial is present, it is placed before the itemsof the sequence in the calculation, and serves as a default when thesequence is empty. 12345t = reduce(lambda x,y: x+y, [1,2,3])print(t) #6，((1+2)+3)=6t = reduce(lambda x,y: x*10+y,[1,2,3])print(t) # ((1*10+2)*10+3)=123 filter函数看名字，就是一个过滤的功能，对每个item调用function，只返回为True的1234| filter(function or None, iterable) --&gt; filter object| | Return an iterator yielding those items of iterable for which function(item)| is true. If function is None, return the items that are true. 12t = filter(lambda x: x&lt;0,range(-5,5))print(list(t)) #[-5, -4, -3, -2, -1]]]></content>
      <categories>
        <category>Python-基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas手册（5）- 用pandas完成excel中常见任务]]></title>
    <url>%2F2017%2F08%2F01%2Fpandas-handbook-05%2F</url>
    <content type="text"><![CDATA[PythonPandas 发现了一篇很好的教程，介绍一些Excel中的常用操作，怎样在pandas中实现，很不错，这里学习，顺便分享下。原文地址：用Pandas完成Excel中常见的任务， 这个是翻译的，再原文是：Common Excel Tasks Demonstrated in Pandas 好了，下面，我们开始学习下。 基础数据这个是从网上找的一个成绩单，拿了一部分数据 首先呢，我们想要，在加一列，显示总分，Excel中很方便 在pandas中呢，其实，我们就是需要“数学”，“语文”，“英语”这3列加在一起，我们怎样获取这3列呢？前面，我们说过在DataFrame中，怎样去筛选数据 12345678import pandas as pdimport numpy as npdf = pd.read_excel(r'D:\document\tableau_data\data_stu.xlsx',sheetname=0)print(df)print(df['数学'])print(df['语文']) 那我们只需要新增一列，把已知的3列加起来就行12df['总分'] = df['数学'] + df['语文'] + df['英语']print(df) 也是很方便，按照思路，直接相加就行了下面呢，我们来统计下，数学的总分、语文的总分，就是把每一列的数据都相加 DataFrame中有很多的聚合函数，这里简单介绍下12345678#数学的最大值print(df['数学'].max())#数学的最小值print(df['数学'].min())#数学的平均值print(df['数学'].mean())#数学的总分print(df['数学'].sum()) 这个和SQL里面一样，Excel里也是这样的，他会从这一列中，获取最大值、最小值等等下面，我们算个列的总分1234df['总分'] = df['数学'] + df['语文'] + df['英语']sum_data = df[['数学','语文','英语','总分']].sum()print(sum_data)print(type(sum_data)) 这样，我们构造了一个Series前面呢，我们知道，Series可以初始化一个DataFrame12df2 = pd.DataFrame(sum_data)print(df2) 初始化之后呢，是这样的，但是，结构不太一样，我们可以做一下行列转换12df2 = pd.DataFrame(sum_data).Tprint(df2) 这回样子像一些了，但是DataFrame中，需要保持结构一致，我们还需要填充几列我们可以用到reindex函数，重构一下索引12345DataFrame.reindex(index=None, columns=None, **kwargs)df2 = pd.DataFrame(sum_data).Tdf2 = df2.reindex(columns=df.columns)print(df2) 到了这里，我们只要将这个DataFrame插入到原来的DataFrame中就行了1DataFrame.append(other, ignore_index=False, verify_integrity=False)Append rows of other to the end of this frame, returning a new object. Columns not in this frame are added as new columns. 刚试了下，发现，前面不重构索引的话，也是可以的，这里会自动补全12df3 = df.append(df2,ignore_index=True)print(df3) 原文中还有些模糊匹配的例子，这里就不练习了，下面，我们看个分类汇总的小问题，这里又增加了一个班级列，要不不好测试 Excel里面实现，应该是这样的，在pandas中，我们要使用groupby这个函数1print(df[['班级','数学','语文','英语']].groupby(by=['班级']).sum() ) 原文中，还有一个格式化和rename index的问题，格式化还没搞明白，后面再说下吧1234567DataFrame.rename(index=None, columns=None, **kwargs)df2 = df[['班级','数学','语文','英语']].groupby(by=['班级']).sum()print(df2)df2 = df2.rename(index=&#123;'1班':'1班-汇总','2班':'2班-汇总'&#125;)print(df2) 我们可以通过一个dict，来替换索引的名字 好了，今天的分享就是这些，总结下呢，主要是对DataFrame中函数的理解和使用，还是得多多的练习才可以。 – 这里回头试下那个格式化的问题，刚刚学习了下，可以参考：Python基础（2）- 格式化format 主要就是format那个函数的使用，还有DataFrame中那个applymap的使用1DataFrame.applymap(func)Apply a function to a DataFrame that is intended to operate elementwise, i.e. like doing map(func, series) for each series in the DataFrame 123456789101112131415import pandas as pdimport numpy as npdf = pd.read_excel(r'D:\document\tableau_data\data_stu.xlsx',sheetname=0)print(df)df2 = df[['班级','数学','语文','英语']].groupby(by=['班级']).sum()print(df2)df2 = df2.rename(index=&#123;'1班':'1班-汇总','2班':'2班-汇总'&#125;)print(df2)def score(s): return '@&#123;:.2f&#125;@分'.format(s)df3 = df2.applymap(score)print(df3)]]></content>
      <categories>
        <category>Python-Pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础（2）- 格式化format]]></title>
    <url>%2F2017%2F08%2F01%2Fpython-handbook-02%2F</url>
    <content type="text"><![CDATA[刚刚在练习pandas的时候，遇到一个格式化的问题，没有太理解，百度了下，这里整理下。str.format()，是一个格式化字符串的函数，很强大 str.format(args, *kwargs) 主要是使用 {}和:这里直接就复制过来了，我们可以通过参数的位置来输出12345678&gt;&gt;&gt;"&#123;&#125; &#123;&#125;".format("hello", "world") # 不设置指定位置，按默认顺序'hello world' &gt;&gt;&gt; "&#123;0&#125; &#123;1&#125;".format("hello", "world") # 设置指定位置'hello world' &gt;&gt;&gt; "&#123;1&#125; &#123;0&#125; &#123;1&#125;".format("hello", "world") # 设置指定位置'world hello world' 我们也可以通过key，value的形式来格式化1234567'name:&#123;name&#125;,age:&#123;age&#125;'.format(name='lufei',age=20)Out[46]: 'name:lufei,age:20'p=['namei',20]'name:&#123;0[0]&#125;,age:&#123;0[1]&#125;'.format(p)Out[51]: 'name:namei,age:20' 填充与对齐 12345678910111213^、&lt;、&gt;分别是居中、左对齐、右对齐，后面带宽度:号后面带填充的字符，只能是一个字符，不指定的话默认是用空格填充#右对齐，长度为8，不够的默认用空格补全'&#123;:&gt;8&#125;'.format('189')Out[52]: ' 189'#右对齐，长度为8，不够的用0补全'&#123;:0&gt;8&#125;'.format('189')Out[53]: '00000189''&#123;:a&gt;8&#125;'.format('189')Out[54]: 'aaaaa189' 数值精度这个说的挺全的，直接截图来吧 参考资料：Python format 格式化函数官方介绍：Format String Syntax飘逸的python - 增强的格式化字符串format函数]]></content>
      <categories>
        <category>Python-基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas手册（4）- 对数据进行筛选和排序]]></title>
    <url>%2F2017%2F07%2F31%2Fpandas-handbook-04%2F</url>
    <content type="text"><![CDATA[PythonPandas 前几天看了篇教程：使用Pandas对数据进行筛选和排序 里面主要介绍了，我们在使用Pandas时，对数据进行筛选和排序的介绍这里简单总结分享下自己。 排序可能是版本的问题，原文中的sort函数没有了，变成了2个常用的函数 sort_index和sort_value 12345DataFrame.sort_index(axis=0, level=None, ascending=True, inplace=False, kind='quicksort', na_position='last', sort_remaining=True, by=None)Sort object by labels (along an axis)DataFrame.sort_values(by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')Sort by the values along either axis sort_index：按照索引排序，及列标签或行标签，axis=0是列标签，axis=1是行标签1234567df3 = pd.DataFrame(np.random.randn(6,4), index=list(range(0,12,2)), columns=list(range(0,8,2)))print(df3)print(df3.sort_index(axis=0,ascending=False))print(df3.sort_index(axis=1,ascending=False)) sort_value：按值进行排序，这个估计用的会多些，按数据内容进行排序123print(df3)print(df3.sort_values(by=[0],axis=0,ascending=True))print(df3.sort_values(by=[2],axis=0,ascending=False)) 排序后呢，我们可能有，只需要看到前10、后10这类的需求，需要用到另一个函数 head()、tail() 筛选筛选呢，我们上一篇介绍了loc和ilocPandas手册（3）-DataFrame-Selection By Label/Position 这里，我们实际应用下12345678910111213141516171819import pandas as pdimport numpy as npdf = pd.read_excel(r'D:\document\tableau_data\data_stu.xlsx',sheetname=0)print(df)#按照数学、语文，降序排列print(df.sort_values(by=['数学','语文'],ascending=False))#按照数学、语文，降序排列，取前3print(df.sort_values(by=['数学','语文'],ascending=False).head(3))#按照数学、语文，降序排列，取后3print(df.sort_values(by=['数学','语文'],ascending=False).tail(3))#筛选数学大于90的print(df.loc[df['数学']&gt;=90])#筛选数学大于等于90，且语文小于60的print(df.loc[(df['数学']&gt;=90) &amp; (df['语文']&lt;60)])#筛选数学或语文大于60分，爱英语排序print(df.loc[(df['数学']&gt;=60) | (df['语文']&gt;=60)].sort_values(by=['英语'])) 数据如下， 我们主要是多种过滤条件的整合使用，大家多练习就可以掌握了 附录（参考资料）使用Pandas对数据进行筛选和排序]]></content>
      <categories>
        <category>Python-Pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas手册（3）- DataFrame-Selection By Label/Position]]></title>
    <url>%2F2017%2F07%2F31%2Fpandas-handbook-03%2F</url>
    <content type="text"><![CDATA[PythonPandas 序这里主要介绍下，在DataFrame中一些筛选的操作，常用的有下面这些 熟练掌握上面的几个方法，操作DataFrame应该就足够了123456789101112import pandas as pdimport numpy as npd = &#123;'one' : pd.Series([1., 2., 3.], index=['a', 'b', 'c']), 'two' : pd.Series([1., 2., 3., 4.], index=['a', 'b', 'c', 'd'])&#125;df = pd.DataFrame(d)print('原始数据：\n',df)print('index 为a的数据：\n',df.loc['a'])print('index下标为2的数据：\n', df.iloc[2]) .loc函数，主要就是通过label来获取row数据 前面的例子，都是通过label来输出指定的行数据，其实也可以控制输出指定的列12345df2 = pd.DataFrame(np.random.randn(6,4),index=list('abcdef'), columns=list('ABCD'))print(df2)print(df2.loc['c':])print(df2.loc['d':,['A','D']]) 我们还可以实现更复杂的筛选我们只输出指定的列，label 为a的行数值大于-1且小于0的列1print(df2.loc[:,(df2.loc['a']&gt;-1) &amp; (df2.loc['a']&lt;0)]) #输出指定单元格数据print(df2.loc[‘a’,’C’]) .iloc函数就是通过下标来筛选数据 123456789df3 = pd.DataFrame(np.random.randn(6,4), index=list(range(0,12,2)), columns=list(range(0,8,2)))print(df3)#输出第2行print(df3.iloc[1])print(df3.iloc[:3])print(df3.iloc[3:5,1:3])print(df3.iloc[[1, 3, 5], [1, 3]]) 附录（参考资料）Indexing and Selecting DataSelection By Label]]></content>
      <categories>
        <category>Python-Pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas手册（2）- DataFrame]]></title>
    <url>%2F2017%2F07%2F31%2Fpandas-handbook-02%2F</url>
    <content type="text"><![CDATA[PythonPandas 序DataFrame是2维的标签数组，可以把他当成电子表格（Excel），数据库里的表，a dict of Series。DataFrame初始化，也可以有不同的输入， 在Series中呢，我们有一个index的概念，在DataFrame中，我们除了index，还有一个columns的概念index：行标签columns：列标签 DataFrame初始化1class pandas.DataFrame(data=None, index=None, columns=None, dtype=None, copy=False) 我们看到，这里有data，index，columns我们可以只初始化data，其他都默认123456import pandas as pdimport numpy as npdf = pd.DataFrame(np.random.randn(5))print(df) 我们看到，index，依然是下标从0开始，columns呢，也是从0开始的我们可以，初始化index，和columns12df = pd.DataFrame(np.random.randn(5),index=['i1','i2','i3','i4','i5'], columns=['a']) From dict of Series or dicts1234d = &#123;'one' : pd.Series([1., 2., 3.], index=['a', 'b', 'c']), 'two' : pd.Series([1., 2., 3., 4.], index=['a', 'b', 'c', 'd'])&#125;df = pd.DataFrame(d) 这里，我们定义一个dict，value是Series，就是我们给2个一维数组都加了一个key，然后把他们拼到一起，就成了一个DataFrame，key就变成了columns 为了让数据均匀，空的数据会默认为NaN这些都是默认的，我们也可以显示初始化index和columns1df = pd.DataFrame(d,index=['a','c','p','q']) 我们显示初始化index的话，会用我们指定的index去和data中Series的index去匹配，匹配上了就使用，匹配不上就剔除掉了，如上，只有’a’,’c’是有的，所以其他的都剔除掉了，像’p’,’q’是data中没有的，所以就是用NAN代替了columns也是同样的道理1df = pd.DataFrame(d,index=['a','b','c'],columns=['id','one','age']) 我们可以查看DataFrame的index和columns12print(df.index)print(df.columns) From dict of ndarray/lists123d = &#123;'one' : [1., 2., 3., 4.],'two' : [4., 3., 2., 1.]&#125;df = pd.DataFrame(d) index会默认初始化，range(n)这时，我们显示初始化index的时候，index的长度一定要和dict中ndarray的长度一样，不然，会报错1df = pd.DataFrame(d,index=['a','b','c','d','e']) From structured or record array12345data = np.zeros((2,), dtype=[('A', 'i4'),('B', 'f4'),('C', 'a10')])data[:] = [(1,2.,'Hello'), (2,3.,"World")]df = pd.DataFrame(data)print(df) 这里先定义了一个数据结构，（对numpy还不熟，希望没说错），2行，3列，分别指定了每一列的数据类型；然后进行初始化 这里的index是默认range(n)初始化的，columns是data中指定的名字，numpy中的数组后面也得学习下1df = pd.DataFrame(data,index=['ff','ss']) 我们显示初始化index的话，一定要和data中的行数一样，columns的话，会取data中名字一样的1df = pd.DataFrame(data, columns=['one','two','C','B','A']) From a list of dicts123data2 = [&#123;'a': 1, 'b': 2&#125;, &#123;'a': 5, 'b': 10, 'c': 20&#125;]df = pd.DataFrame(data2)print(df) 这里会将list中的元素做一个union，合并到一起去，如果显示初始化index，columns的话，index的长度一个要和list的长度一致，columns 的话，则会自动处理，有则显示，无则显示NAN（有无表示是否和dict中的key匹配的上）1df = pd.DataFrame(data2,index=['x','y'],columns=['a','b','c','d']) From a dict of tuples123456df2 = pd.DataFrame(&#123;('a', 'b'): &#123;('A', 'B'): 1, ('A', 'C'): 2&#125;, ('a', 'a'): &#123;('A', 'C'): 3, ('A', 'B'): 4&#125;, ('a', 'c'): &#123;('A', 'B'): 5, ('A', 'C'): 6&#125;, ('b', 'a'): &#123;('A', 'C'): 7, ('A', 'B'): 8&#125;, ('b', 'b'): &#123;('A', 'D'): 9, ('A', 'B'): 10&#125;&#125;)print(df2) 这个感觉类似Excel里面合并单元格的操作，就不多做练习了，啥时候用上了再说 From a Series这个和前面都比较类似，我们看个例子1234s = pd.Series(np.random.randn(5),index=['a','b','c','d','e'])print(s)df3 = pd.DataFrame(s,index=['x','y','a','b'])print(df3) 常用构造函数这里就不多说了，大家可以自行学习下 Column selection,addition,deletion这里主要是说对DataFrame中对column的一些操作123456789101112d = &#123;'one' : pd.Series([1., 2., 3.], index=['a', 'b', 'c']), 'two' : pd.Series([1., 2., 3., 4.], index=['a', 'b', 'c', 'd'])&#125;df = pd.DataFrame(d)print(df)#输出列名为one的数据print(df['one'])#给列three赋值，one列*two列df['three'] = df['one']*df['two']#给列flag赋值，one列的值是否大于2df['flag'] = df['one'] &gt; 2print(df) 删除操作123del df['flag']df.pop('three')print(df) 1DataFrame.insert(loc, column, value, allow_duplicates=False)[source] 关于DataFrame的其他操作，后面会再详细说明，这里就简单说到这了。 附录（参考资料）官方教程：DataFrame ———update at 2017-08-07 DataFrame使用后记 记录下DataFrame使用的小技巧 索引对象 pandas中的索引对象负责管理轴标签和其他元数据（比如轴名称）。构建Series或DataFrame时，所用到的任何数组或其他序列的标签都会转换成一个index。 索引创建后是不可以修改的12345678910111213141516171819202122232425262728obj = pd.Series([4, 7, -5, 3])objOut[144]: 0 41 72 -53 3dtype: int64obj.indexOut[145]: RangeIndex(start=0, stop=4, step=1)o_ind = obj.indexo_indOut[147]: RangeIndex(start=0, stop=4, step=1)o_ind[0]=9Traceback (most recent call last): File "&lt;ipython-input-148-f7ee6348297a&gt;", line 1, in &lt;module&gt; o_ind[0]=9 File "D:\Users\yugui\Anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 1620, in __setitem__ raise TypeError("Index does not support mutable operations")TypeError: Index does not support mutable operations]]></content>
      <categories>
        <category>Python-Pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas手册（1）- Series]]></title>
    <url>%2F2017%2F07%2F30%2Fpandas-handbook-01%2F</url>
    <content type="text"><![CDATA[PythonPandas 要学习pandas了，，看官网上的资料还是很多的，就根据找到的资料简单总结下吧。这里也有很多同学分享的资料，这里都整理下，按照自己的理解整理下。 序这里的主要内容，参考官方教程：http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dsintropandas里面有3个基本的数据结构， 我们可以把Series，理解成一维数组，但是又和常规的一维数组不太一样。 Series是一维的标签数组，可以存储任意的数据类型（integers，strings，floating point numbers，Python objs，etc.)这里为什么是标签数组呢？因为他多了一个轴的概念，类似索引，我们往下看下就知道了。 Series初始化引入必要的类12import pandas as pdimport numpy as np 基本初始化语法：1s = pd.Series(data, index=index) 这个data，就是我们要初始化的数据，index，就是那个标签了，即索引data呢，常规可以为： from ndarray如果data是ndarray，index的长度必须和data的长度一样，或者保持默认，index会自动初始化，就是下标从0开始1s = pd.Series(np.random.randn(5), index=['a', 'b', 'c', 'd', 'e']) 这里呢，我们data的长度是5，我们index的长度也是5如果不是5呢，我们试试 这里是会报错的，少了不行，那多了呢？ 也是不行的，所以，如果初始化index的话，长度一定要和data一样当然，默认是可以的，1s = pd.Series(np.random.randn(5)) index默认初始化，从0开始 from dict如果data为dict，因为dict是key，value的，所以，默认初始化时，会使用key来初始化index 当然，我们也可以，显式初始化index 通过上面的例子，我们发现，如果指定的index没有包括所有的data中的key，那么就只显示index中有的；如果指定的index中有data中key没有的，那么就用NAN来赋值 from scalar value如果data是常量，那么我们必须初始化index 刚试了下，好像也不用，默认会初始化一个长度的 Series使用Series使用起来也很方便 Series is ndarray-like我们可以使用下标，1s = pd.Series(np.random.randn(5), index=['a', 'b', 'c', 'd', 'e']) Series is dict-like我们也可以像dict一样，使用index来操作Series 附录（参考资料）博客：1.1 pandas数据结构Series官方教程：Intro to Data Structures ——update at 2017-08-07 Series使用后记这里记录些Series使用上的心得Series使用起来，不仅可以使用下标来获取元素，也可以使用index来获取12345678910111213141516171819202122232425262728293031323334353637383940414243s = pd.Series(np.random.randn(5) , index=list('abcde'))sOut[89]: a -0.434789b -0.047950c -0.826720d 1.493415e 0.806696dtype: float64s[0]Out[90]: -0.43478889663783105s['a']Out[91]: -0.43478889663783105s[1:3]Out[92]: b -0.04795c -0.82672dtype: float64s['b':'d']Out[93]: b -0.047950c -0.826720d 1.493415dtype: float64s[[3,2,1]]Out[94]: d 1.493415c -0.826720b -0.047950dtype: float64s[['b','c','a']]Out[95]: b -0.047950c -0.826720a -0.434789dtype: float64 对于NaN值得处理,我们可以使用isnull，notnull来判断是否有NaN值123456789101112131415161718192021222324252627a = &#123;'lufei':10,'namei':30,'qiaoba':40&#125;s = pd.Series(a,index=['lufei','namei','qiaoba','suolong'])sOut[98]: lufei 10.0namei 30.0qiaoba 40.0suolong NaNdtype: float64s.isnull()Out[99]: lufei Falsenamei Falseqiaoba Falsesuolong Truedtype: bools.notnull()Out[100]: lufei Truenamei Trueqiaoba Truesuolong Falsedtype: bool Series的索引就可就地修改，直接使用s.index123456789101112s.indexOut[104]: Index(['lufei', 'namei', 'qiaoba', 'suolong'], dtype='object')s.index=list('abcd')sOut[107]: a 10.0b 30.0c 40.0d NaNdtype: float64]]></content>
      <categories>
        <category>Python-Pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小白学习Tableau-堆叠条按值排序]]></title>
    <url>%2F2017%2F07%2F30%2FTableau-handbook-03%2F</url>
    <content type="text"><![CDATA[序昨天学习了下堆叠条形图，刚刚看到个类似的教程，说的是，在堆叠条中按值进行排序，挺有意思的。 又学习到了一招，简单分享下。 按值在堆叠条中对段进行排序我们先实现一个简单的堆叠条， 使用维度：地区、类别 使用度量：销售额 下面，我们来看下，是怎样让每个类别的数据块按照销售额排序的。 我们观察下，上面的图，每个地区一个颜色，默认应该是按地区来排序的，每个数据条排序都一样。 我们首先，在颜色“地区”上右键单击，选择“属性” 这里的“维度”、“度量”我都可以理解，但是并没有找到“属性”在Tableau中是怎样定义的，按照以前对BI中维度模型的概念来理解的话， 比如，日期维度，包含3个层级，年、月、日，层级月的属性的话，可能是月份名称、月份中文名称等等。我们选完属性之后，会变成这个样子 然后，我们选中，维度“类别”和“地区”，创建一个合并字段 这个合并字段，是个什么东西呢？其实就是新增了一个联合维度，就是类别和地区的一个笛卡尔积，内容是这样的 然后，我们把这个合并字段，拖到详细信息上， 然后，我们在合并字段上，选择排序 我们，依次，选择“降序”，“按销售额来” 最后结果，是这样的 实例呢，在Tableau Public上，练习04-堆叠条按值排序 最后的话，后面还得详细理解下这里用属性和合并字段一起使用的这个操作，有领悟的话，会在分享。 附录官方教程：按值在堆叠条中对段进行排序]]></content>
      <categories>
        <category>Tableau</category>
      </categories>
      <tags>
        <tag>数据可视化</tag>
        <tag>Tableau</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小白学习Tableau-堆叠条形图]]></title>
    <url>%2F2017%2F07%2F30%2FTableau-handbook-02%2F</url>
    <content type="text"><![CDATA[序昨天在看水足迹那个可视化题目的时候，就想做一个堆叠条形图，但是发现只有一个维度，怎么也拖不出来，后来改了下数据源，成功实现了。今天搜到个例子，发现了解决办法，只能说明，还是对Tableau不熟啊，没有能领悟Tableau的内涵。 堆叠条形图教程中介绍了2种方法，我们都来实践一下。这里面，最大的收获，就是原来那个“度量名称”和“度量值”是可以拖拽过去使用的，我也是醉了，这个操作得好好研究下，理解下Tableau的机制。 这里的度量名称，应该就是所有度量的一个集合；度量值应该就是“度量名称”集合中选定的度量。 这里我们就用 http://www.makeovermonday.co.uk/ 上的数据 对每个维度上使用一个单独的条首先，我们将“食物”维度，拖到列上， 然后，我们需要观察的是3种水足迹的值，并让他们显示成堆叠图的样子 我们把度量名称，拖到颜色上 因为，我们只看3个水足迹的量，我们就筛选下 （这个颜色是我之前设置过的，大家操作完，颜色可能和我不一样，但样式是一样的）就这样，我们完成了，列上是每一种食物，然后行上面是选定的3中水足迹，一个简单的堆叠图就完成了。 对每个度量使用一个单独的条这个操作其实就是和上面的操作的相反我们将度量名称拖到列上，并筛选 然后，将度量值拖到行上 最后，将维度，食物拖到颜色上 这样，我们就在3个度量值上，看食物维度的一个堆积情况。好了，堆积图的例子就完成了，主要是理解下Tableau的思想。 小例子，发布在Tableau Public上：练习03-堆叠条形图 附录（参考资料）官方教程：使用多个度量创建堆叠条形图]]></content>
      <categories>
        <category>Tableau</category>
      </categories>
      <tags>
        <tag>数据可视化</tag>
        <tag>Tableau</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小白学习Tableau-标签云]]></title>
    <url>%2F2017%2F07%2F30%2FTableau-handbook-01%2F</url>
    <content type="text"><![CDATA[序周末参加了2天的Tableau培训，发现这个东西做可视化分析还是很方便的，用户体验也很好。 也听了很多大师的介绍，受益良多。不管怎样，要开始学些这个Tableau了。之前也有做过IBM Cognos的开发，2个产品对比一下，从直观上来看，就是Tableau的可视化效果好很多，而且还融合了一些分析功能，挺不错的。很久之前就知道了Tableau其实，只是一直都没有去研究她。学习的一开始呢，先是模仿，熟练掌握了Tableau之后，就可以任意发挥了。Tableau入门的话，官方就有很多的资料，挺全的，还有视频教程。 标签云之作这个比较简单，主要参考了jiyang大神的教程，后面有链接我们使用Tableau自带的示例库 我们主要就使用“类别”、“子类别”、“销售额” 我们把子类别拖到文本上 这里会显示，所有的子类别，然后呢，我们希望子类别可以根据销售额的大小而控制大小 把销售额拖到标记-大小上 默认的话，可能会显示这个树状图，但是我们并不想要这样，我们想要显示文本 我们修改下，我们把自动，修改为文本 好了，已经可以按销售额的大小来显示子类别的内容了当然，我们一般见到的标签云，都是有颜色的，那我们就继续用销售额的大小来控制颜色 将销售额拖到颜色上 这样呢，我们就用销售额控制了文本的大小和颜色， 一个简单的标签云就实现了 好了，这个例子就是这样简单，小例子，发布在Tableau Public上， 练习02-销售额标签云 附录（参考资料）参考学习jiyang大神的作品：https://public.tableau.com/profile/jiyang#!/vizhome/_3516/sheet0]]></content>
      <categories>
        <category>Tableau</category>
      </categories>
      <tags>
        <tag>数据可视化</tag>
        <tag>Tableau</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BeautifulSoup教程（2） - 实例-解析博客专栏]]></title>
    <url>%2F2017%2F04%2F20%2Fbs-handbook-02%2F</url>
    <content type="text"><![CDATA[前几天学习了下Beautiful Soup的使用，本来想多写些内容的，但是发现，官方的介绍实在太详细了，每种方法基本都覆盖到了， 直接看官方的例子就足够了，而且还有一个中文版的，这里的话，就简单实践下，介绍几个常用的方法和一些小经验。 官方文档：https://www.crummy.com/software/BeautifulSoup/bs4/doc/ 这里，我们就简单的解析下博客专栏，https://blog.hellobi.com/ 这里的话，没有做太多的限制，我们直接解析就可以 1. 分析网页我们再Firefox中，使用 Firebug很方便 这里的话，文档结构也很清晰，很适合我们练习 基本代码，我们获取网页信息12345678910111213# -*- coding: utf-8 -*-import urllibfrom bs4 import BeautifulSoupimport sysreload(sys)sys.setdefaultencoding('utf-8')#加载网址，获取当前页面def getHTML(url) : page = urllib.urlopen(url) html = page.read() return html 2.常用类 123html = getHTML('https://blog.hellobi.com/')soup = BeautifulSoup(html, "html.parser") 这里，我们初始化Beautiful Soup，使用“html.parser”,这是一个默认的解析器，他还有其他的解析器，官网有介绍，这里就不说了，其他也没有测试过 这里，顺便说下Python中查看类的帮助信息，使用dir()函数，可以查看类的属性和方法列表12print type(soup)print dir(soup) 我们还可以使用help函数，查看详细的帮助信息 1print help(soup) Beautiful Soup是最基础的类，其他的还有Tag，这个用起来和HTML中的标签类似 12345print soup.titleprint type(soup.title)#print soup.headprint type(soup.head) 这里，的title，head都是HTML中的tag，也是Tag类的对象 123print soup.title.stringprint type(soup.title.string)print dir(soup.title.string) 然后，我们需要使用Tag的内容时，就用到了NavigableString 3.常用函数123456789101112131415161718blog_index=1for blog in soup.find_all('div', class_='blog-item'): print '----------------------------------------------------------' print '第%s篇博客' %(blog_index) title = blog.select_one('div.caption &gt; h2 &gt; a') print '博客标题: ',title.string author = blog.select_one('div.caption ul a') print '作者: ',author.text.strip() vote = blog.select_one('div.cp div.blog-votes span') print '推荐次数: ',vote.string info = blog.select_one('div.cp div.blog-views span') print '阅读次数: ',info.string blog_index+=1 上面的代码，就是遍历的首页的所有博客信息，很简单，就是一个find_all()和一个select_one() find_all() select_one() select() 方法中传入字符串参数,即可使用CSS选择器的语法找到tag: 思想其实都一样，就是根据一定的规则，找到我们想要的标签 后面的话，我们再找到总页数，遍历一下，就可以获取所有的博客基本信息啦1https://blog.hellobi.com/?page=2 传递页码就可以了 好了，这里就举这一个简单的小例子，大家多在实践中使用就好了]]></content>
      <categories>
        <category>Python-爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>BeautifulSoup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQLAlchemy手册（四）- 关联查询]]></title>
    <url>%2F2017%2F04%2F16%2Fpython-database-04%2F</url>
    <content type="text"><![CDATA[上一回，我们介绍了，常用的单查询SQL，这里，我们介绍下，怎样进行关联查询，就是join的使用数据库使用表信息：t_class t_student t_student表存有t_class表的id，获取学生所属的班级信息1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# -*- coding: utf-8 -*-from sqlalchemy import MetaData, create_enginefrom sqlalchemy.ext.declarative import declarative_basefrom sqlalchemy import Column, Integer, String, Date, or_, and_from sqlalchemy import ForeignKeyfrom sqlalchemy.orm import relationshipfrom sqlalchemy import text, funcfrom sqlalchemy.orm import sessionmakerimport sysreload(sys)sys.setdefaultencoding('utf-8')Base = declarative_base()engine = create_engine('postgresql://postgres:shishi@localhost:5432/postgres')Session = sessionmaker(bind=engine)session = Session()class StuClass(Base): __tablename__ = 't_class' id = Column(Integer,primary_key=True) class_name = Column(String(10)) def __repr__(self): return "&lt;StuClass(id='%s', name='%s')&gt;" % (self.id, self.class_name)class Student(Base): __tablename__ = 't_student' id = Column(Integer , primary_key=True) stu_name = Column(String(10)) class_id = Column(Integer , ForeignKey('t_class.id')) stuClass = relationship("StuClass") def __repr__(self): return "&lt;Student(id='%s', stu_name='%s', class_id='%s', class_name='%s')&gt;" % (self.id, self.stu_name,self.class_id,self.stuClass.class_name)#使用filter定义2张表的关系for stu, stuc in session.query(Student, StuClass).filter(Student.class_id == StuClass.id).all(): print stu,stuc#使用join需要ForeignKeyfor x in session.query(Student).join(StuClass).filter(StuClass.id == 803).all(): print x 下面，我们再写一个统计班级人数的SQL 用SQLAlchemy来写的话，是这样的12for rs in session.query(StuClass.class_name,func.count('*').label('stu_count')).join(Student).group_by(StuClass.class_name).order_by(StuClass.class_name).all(): print('班级:' + rs[0] + ' 人数：' + str(rs[1])) 好了，同样的功能，我们换成子查询练习下12345 select a.class_name,b.num from t_class aleft join ( select class_id , count(1) num from t_student group by class_id)b on b.class_id = a.idorder by a.class_name 那SQLAlchemy是这样滴：1234 sub = session.query(Student.class_id,func.count('*').label('num')).group_by(Student.class_id).subquery()for sc,num in session.query(StuClass,sub.c.num).outerjoin(sub, StuClass.id == sub.c.class_id).order_by(StuClass.class_name).all(): print sc,num 今天暂时先到这里，后面的话，官网上例子挺全的，大家可以自行练习下，后续也会再整理]]></content>
      <categories>
        <category>Python-数据库</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>SQLAlchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python基础（1）- 私有变量]]></title>
    <url>%2F2017%2F04%2F16%2Fpython-handbook-01%2F</url>
    <content type="text"><![CDATA[今天简单看了看Python中的面向对象的一些教程，简单记录下，和Java中还是有很多类似的看的是这个博客：访问限制 比如我们定义一个Student类一个init构造函数，初始化2个属性，一个名字，一个成绩；还有一个打印函数，输出学生的名字和成绩12345678910111213class Student(object): def __init__(self, name, score): "initial student" self.name = name self.score = score def print_score(self): "print student info" print '%s : %s' %(self.name, self.score)s1 = Student('lufei' , 99)s1.print_score() 了解Java的同学都知道，我们一般定义实体类的话，一般都是private，然后定义get、set方法，如果只是上面的代码，我们就可以随便的调用name和score了我们需要加上限制1234def __init__(self, name, score): "initial student" self.__name = name self.__score = score 我们在变量前面加上“__”就可以了 如果我们要引用的话，就会报错了，同样的，我们也可以加上get、set方法来使用12345678910def get_name(self): return self.__namedef set_name(self,name): self.__name=nameprint s1.get_name()s1.set_name('libai')print s1.get_name() 原文还有很多其他内容，大家可以自行看看，我就简单记录这些]]></content>
      <categories>
        <category>Python-基础</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQLAlchemy手册（三）- 常用单查询]]></title>
    <url>%2F2017%2F04%2F15%2Fpython-database-03%2F</url>
    <content type="text"><![CDATA[好了，在前面，我们大概了解了SQLAlchemy的使用，日常，我们可能经常会使用些复杂点儿的查询，我们先练习下 #1. common filter这里，官方都有介绍，我们主要参考这里：http://docs.sqlalchemy.org/en/rel_1_1/orm/tutorial.html里面讲的都很清楚，我们这里简单练习下首先是基本的脚本，后面，我们直接写query数据库表： 1234567891011121314151617181920212223242526272829# -*- coding: utf-8 -*-from sqlalchemy import MetaData, create_enginefrom sqlalchemy.ext.declarative import declarative_basefrom sqlalchemy import Column, Integer, String, Datefrom sqlalchemy.orm import sessionmakerimport sysreload(sys)sys.setdefaultencoding('utf-8')Base = declarative_base()class Book(Base): __tablename__ = 't_book' id = Column(Integer,primary_key=True) name = Column(String(10),primary_key=True) publish_date = Column(Date,primary_key=True) def __repr__(self): return "&lt;Book(id='%s', name='%s'" % (self.id, self.name)engine = create_engine('postgresql://postgres:shishi@localhost:5432/postgres')Session = sessionmaker(bind=engine)session = Session() 我们一般的过滤的话，要使用filter这个函数1234567891011print '----：equals'for book in session.query(Book).filter(Book.id==190): print bookprint '----：not equals' for book in session.query(Book).filter(Book.name != 'Java'): print bookprint '----：like'for book in session.query(Book).filter(Book.name.like('白%')): print book 常用的SQL中，我们还有and，or我们需要引入其他类from sqlalchemy import or_, andprint ‘—-：or’for book in session.query(Book).filter(or(Book.name.like(‘白%’),Book.id == 191)):print bookprint ‘—-：and’for book in session.query(Book).filter(and_(Book.name.like(‘白%’),Book.id == 191)):print book 2. 自定义SQL这里，我们使用text，来自定义自己的sql123456789from sqlalchemy import or_, and_print '----：or'for book in session.query(Book).filter(or_(Book.name.like('白%'),Book.id == 191)): print book print '----：and'for book in session.query(Book).filter(and_(Book.name.like('白%'),Book.id == 191)): print book 在SQL中，我们经常会使用参数传递，这里也是可以的123print '-----: param'for book in session.query(Book).filter(text("id=:p_id_1 or id=:p_id_2")).params(p_id_1=192,p_id_2=193).order_by(text("id asc")).all(): print book 到这里，自定义SQL貌似还不够灵活，不能像在数据库中随便写SQL那样，下面，我们再试试另一种方式12345print '-----: custom'for book in session.query(Book).\ from_statement(text("select *from t_book where id = :p_id_1 or id = :p_id_2 ")).\ params(p_id_1=190, p_id_2=191).all(): print book 这下，我们就可以随便写我们的SQL了， 3.聚合函数我们先看看count123456789from sqlalchemy import funcprint '----: count'print session.query(func.count('*')).select_from(Book).scalar()print session.query(func.count(Book.id)).scalar()print '----: group by'for rs in session.query(func.count(Book.id) , Book.name).group_by(Book.name).all(): print rs[1]+' count： '+str(rs[0])]]></content>
      <categories>
        <category>Python-数据库</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>SQLAlchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BeautifulSoup教程（1） - 简介及安装]]></title>
    <url>%2F2017%2F04%2F14%2Fbs-handbook-01%2F</url>
    <content type="text"><![CDATA[最近在学习Python，按照一些博客练习爬虫，最简单的步骤，就是访问一个主页，根据正则表达式去获取我们想要的标签数据； 比如这样：1234567891011#加载网址，获取当前页面def getHTML(url) : page = urllib.urlopen(url) html = page.read() return htmldef getImage(html) : reg = r'src="(.+?\.jpg)"' reg2 = r'&lt;img alt="(.+?)" src="(.+?\.jpg)' image_reg = re.compile(reg2) img_list = re.findall(image_reg,html) 简单的话，这样还好，如果复杂些的话，像我一样对正则表达式不熟悉的话，可能就不太好实现了， 后面发现这个beautifulSoup解析HTML很方便，这里简单学习下， 官网地址：https://www.crummy.com/software/BeautifulSoup/ 还有中文文档：https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html 1. 简介这里说的不错，Python爬虫利器二之Beautiful Soup的用法 2. 安装Python里面安装东西很方便，直接使用pip就行了 pip install beautifulsoup4 3.小例子我们先写个小例子看看123456789101112131415161718 # -*- coding: utf-8 -*-import urllibimport refrom bs4 import BeautifulSoup#加载网址，获取当前页面def getHTML(url) : page = urllib.urlopen(url) html = page.read() return htmlhtml = getHTML('https://movie.douban.com/top250')soup = BeautifulSoup(html, "html.parser")for img in soup.find_all('img'): print img.get('src') 这里，我们就输出了所有的img标签 后面，我们再来继续练习使用]]></content>
      <categories>
        <category>Python-爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>BeautifulSoup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（十二）- 控件使用-从步骤插入数据]]></title>
    <url>%2F2017%2F04%2F14%2FKettle-handbook-12%2F</url>
    <content type="text"><![CDATA[这里介绍一个控件的小功能，也是最近才发现的，之前在“表输入”中要使用参数的话，一般都是使用变量，其实，还有个功能也可以尝试使用整体流程就是这样，我们第一个 query_paramter，就是查询了我们想设置的参数然后，就是我们真正需要的，我们再表输入中，使用 “?”来占位，然后“从步骤插入数据”，选择上一个步骤，然后会将数据替换占位符最后，我们将文件导出即可，奥对了，我们可以改成日志控件，直接输出查看刚刚，上面还有一个“执行每一行”，这个就是，如果我们有多个参数，就可以使用这个参数了，很方便，好了，就介绍到这里先。]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQLAlchemy手册（二）- 增删改查]]></title>
    <url>%2F2017%2F04%2F13%2Fpython-database-02%2F</url>
    <content type="text"><![CDATA[好了，这里，我们来看看，怎么使用SQLAlchemy，先从最简单的增删改查来看看，这里我们使用PostgreSQL数据库 1. 连接PG数据库 因为SQLAlchemy底层连接，还是使用这些框架，所以我们想要连接PostgreSQL，还得先安装下这些，这里我们使用psycopg2 不装的话，我们连接时，会报错1pip install psycopg2 安装好后，我们来看看怎样连接PG我们再SQLAlchemy中连接数据库的话，需要使用这个叫做Engine的类，关于介绍我们可以看官方的教程：http://docs.sqlalchemy.org/en/rel_1_1/core/engines.html 他可以适配大部分数据库，大家可以自行尝试下 1234567#!/usr/bin/pythonfrom sqlalchemy import create_engineengine = create_engine('postgresql://postgres:shishi@172.16.201.12:5432/postgres',echo=True)print engine 2. create table/drop table这里，我们来看看怎样用orm的思想去create table、drop table按照orm的思想，这里还有一个MetaData类得看下 这里的示例，我们定义一个table“t_users”,有2个字段，id，user_name123456789101112131415161718#!/usr/bin/pythonfrom sqlalchemy import Table, Column, Integer, String, MetaData, create_engineengine = create_engine('postgresql://postgres:shishi@172.16.201.12:5432/postgres',echo=True)metadata = MetaData()users = Table('t_users',metadata, Column('id',Integer), Column('user_name',String(20)) )#do the drop firstmetadata.drop(engine)#now create the tablemetadata.create(engine) 我们执行下上面，我们create_engine的时候，指定了echo=True，我们就可以看到输出语句了这里的drop，有一个参数，checkfirst=True,默认就是TRUE，会判断该表是否存在，如果存在就执行删除，create也是一样的，会先判断是否存在 我们看看数据库 –晚上又看了看，感觉上面的方法可以使用，但是，貌似不是ORM思想的方法，又看了看官方的文档，自己先试试，明天再接着整理 3. 查询数据好了，上面的Table方式，可以用来创建表，这里参考官方的教程：http://docs.sqlalchemy.org/en/rel_1_1/orm/tutorial.html 3.1 定义Mapping类和Java里的一样，我们得定一些实体类，和数据库中的表做映射123456789from sqlalchemy import Column, Integer, Stringfrom sqlalchemy.ext.declarative import declarative_baseBase = declarative_base()class SomeClass(Base): __tablename__ = 'some_table' id = Column(Integer, primary_key=True) name = Column(String(50)) 像上面这样，我们就定义了一个类，我们来实际写一个看看这里，我们映射了数据库中的“t_users”表，有2个字段，“id”和“user_name” 12345678910111213141516#!/usr/bin/pythonfrom sqlalchemy.ext.declarative import declarative_basefrom sqlalchemy import create_enginefrom sqlalchemy import Column, Integer, StringBase = declarative_base()class User(Base): __tablename__='t_users' id = Column(Integer,primary_key=True) user_name = Column(String(10)) def __repr__(self): return "&lt;User(id='%s', user_name='%s')&gt;" % (self.id, self.user_name) 3.2 session下面，我们就来查询下数据12345678910111213141516171819202122232425#!/usr/bin/pythonfrom sqlalchemy.ext.declarative import declarative_basefrom sqlalchemy import create_enginefrom sqlalchemy.orm import sessionmakerfrom sqlalchemy import Column, Integer, StringBase = declarative_base()class User(Base): __tablename__='t_users' id = Column(Integer,primary_key=True) user_name = Column(String(10)) def __repr__(self): return "&lt;User(id='%s', user_name='%s')&gt;" % (self.id, self.user_name)engine = create_engine('postgresql://postgres:shishi@172.16.201.12:5432/postgres',echo=True)Session = sessionmaker(bind=engine)session = Session()#query all usersuser_list = session.query(User).all()print user_list 上面的话，我们还用到了session，这里的session，暂时理解为，可以开一个事务，处理我们的逻辑，我们执行select的话，也需要这个这里，我们就直接查出所有的记录就可以了 4. insert123new_user = User(id=666,user_name='ayang')session.add(new_user)session.commit() 5.update123456#update the userupdate_user = session.query(User).filter_by(id=666).first()print update_userupdate_user.user_name='apple'session.commit()]]></content>
      <categories>
        <category>Python-数据库</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>SQLAlchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQLAlchemy手册（一）-简介及安装]]></title>
    <url>%2F2017%2F04%2F12%2Fpython-database-01%2F</url>
    <content type="text"><![CDATA[Python新手，目前学习中，最近发现个叫SQLAlchemy的ORM框架，就是类似Java里面的Hibernate啊，Mybatis啊之类的，这里也简单记录下。官网地址：http://docs.sqlalchemy.org/en/rel_1_1/ 1. SQLAlchemy 介绍这里直接摘一下百度百科的内容，简单说就是ORM框架，更加方便的去和数据库库连接。 2. SQLAlchemy安装熟悉Python的同学应该很擅长这个了，而且官方文档上也有介绍，http://docs.sqlalchemy.org/en/rel_1_1/intro.html#installation 2.1 pip 安装1pip install SQLAlchemy 2.2 setup.py1python setup.py install 家里网速不太好，就直接下了个包，用这种方式安装了，没啥问题 我们引入验证一下好了，第一回，先简单说到这，下一回，我们来看看怎么使用]]></content>
      <categories>
        <category>Python-数据库</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>SQLAlchemy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（十一）- 用PGP加密、加密文件]]></title>
    <url>%2F2017%2F04%2F11%2FKettle-handbook-11%2F</url>
    <content type="text"><![CDATA[看到有同学提问，以前也没用过，百度了一下，找了些资料，这里记录下。 1. 安装gpg4win这个gpg4win是干嘛的呢，我们可以去他的官网看看：gpg4win目前，只知道他是加密的，这个是对Windows平台使用的这里可能还有个PGP的概念，看看百度百科 好了，具体概念，大家可以自行找找，我们下载下来，然后安装一下即可这个是昨天安装的，就不粘贴步骤了，安装完后，我们要先创建一个证书的东西，我们打开这个管理界面打开后，是这样一个界面，（网上有这个的安装配置教程，这里也简单介绍下，不清楚的可以再百度看看）我们新建一个Certificate我们选择一个加密方式，使用第一个就可以了我们输入些基本信息然后next就可以然后，我们得输入一段密钥好了，这里，就配置完成了 2. 用PGP加密文件好了，这里，我们新建一个作业，我们主要使用这2个控件一个很简单的流程，我们做些简单的配置，一个是GPG的目录（就是我们上面安装的那个）还有就是，我们的要加密的文件和一个目标文件名，注意，这里我们得填写一下“用户ID”，就是我们前面新建的那个用户名就可以了这里，可以勾选一下，目标是一个文件好了，然后，我们执行下就可以了我们源文件：加密后的文件：下面，我们再看看，怎样解密 3. 用PGP解密文件知道了加密，解密也是一样的，这里的话，配置和上面差不多，这里，我们要填写一个“密钥”，就是我们上面创建时，输入的一个密码我们运行一下，解密后，是一样的好了，就简单介绍到这里]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（十）- 跨库查询]]></title>
    <url>%2F2017%2F04%2F10%2FKettle-handbook-10%2F</url>
    <content type="text"><![CDATA[Kettle整体使用起来，还是很方便的，熟悉应用了之后，就是对控件的熟悉和使用了，只要思路有了，就是整合下Kettle中各个控件的使用就行。这里，简单介绍下一个“跨库查询”的控件。有的时候，我们一个脚本，可能只是临时性的，或者需要实时的去查一下，同步到数仓的话，可能不太方便，我们就可以使用跨库查询的控件用到的表信息 1. 数据库连接(Database Join)我们先用这个控件来实现一下用起来也很简单表输入：是我们第一个库中的SQL数据库连接：是我们另一个库的SQL我们用关联的字段放在where条件后，使用“?”来占位，并在下面，选择要传入的参数默认的话，是JOIN，我们也可以勾选Outer Join，然后，我们看下，输出就行这是后面导出的文件，这里，我们就简单实现了跨库的查询 2. 数据库查询我们再来看另一个控件，“数据库查询”，这个控件同样可以实现跨库，但是有一个小问题首先，我们使用上一次的数据来看我们执行下，结果看上去是一样的这其实有个隐藏的问题，我们再增加几条记录看看比如：现在1号有2条记录，正常的话，我们导出也是要有2条的我们执行下看看我们会看到，数据并没有增加，这是控件导致的，先获取左边的结果集，然后一条一条去右边匹配；匹配到第一条记录后，就会跳出，直接去匹配下一个，所以，我们有2条记录，也只会找到第一个。这并不是我们想要的，我们再试下第一个控件使用这个“数据库查询”控件的话，可以通过将1-N关系汇总，将N的一方，放在前面最后的结果也是可以的]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（九）- 发送邮件]]></title>
    <url>%2F2017%2F03%2F30%2FKettle-handbook-09%2F</url>
    <content type="text"><![CDATA[在Kettle里面，我们每天执行完调度之后，想要监控下JOB的执行状态，通常我们可以会发送邮件，可以的话，还可以发送短信。 在Kettle里面，发送邮件很方便，这里，我们就简单的测试下。 1. 在作业中发送简单邮件我们只需要使用到这个控件就可以了，这样，一个简单的发送邮件流程就好了 控件的配置：收件人，抄送啊，信息，自行填写就行，多个收件人，使用“空格”分隔在服务器这里，我们填上服务器的信息就可以了这里是邮件消息的一些配置，暂时先到这里，我们测试下结果然后，查看邮箱，我们会接收到这个邮件，刚刚简单测了下这个“回复名称”，就是这里试过中文，会有问题，有乱码，可能是Windows下的原因，没有再去测试验证就是收到邮件时的一个发件人的名称，不同邮箱显示的不一样 2. 增加附件附件的话，也很简单，上面的面板中直接配置就可以了然后，我们需要将待发送的邮件，添加到结果集中在控件中，我们添加好文件就行了。我们再次发送，验证下好了，附件也可以了，思路就是这样的，实际应用时，可能还有些问题得注意下 3. 自定义邮件内容到这里，我们会看到，邮件的正文内容，可能并不是我们想要的， 我们想要的可能是这样的信息这就需要自定义正文内容，我们需要勾选下面这个选项这里是可以使用变量的，我们可以拼接HTML来实现好了，邮件的介绍，大概就这些，在转换中，也是可以使用的，大同小异]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（八）- 循环]]></title>
    <url>%2F2017%2F03%2F29%2FKettle-handbook-08%2F</url>
    <content type="text"><![CDATA[有的时候，我们想要在Kettle中实现这个循环的功能，比如，批量加载数据的时候，我们要对10张表执行同样的操作，只有表名和一些信息不一样，这时，写个循环就省事儿多了 1. 遍历结果集实现这里的话，我们主要是通过一个将结果集返回，然后通过转换的设置来实现的 1.1 query_the_result这个转换，只要是将我们要遍历的结果集返回，表输入，我们就是返回了5条记录，来做遍历复制记录到结果，这个控件的作用，就是我们可以在后面的转换继续使用这个结果集。 ##1.2 traverse_the_result这里呢，我们就是需要遍历的转换了，这里，我们只是获取结果集，然后将结果集输出还有一个很重要的一步，怎样让这个转换可以根据结果集的条数，去循环执行呢？就是这个“执行每一个输入行”我们执行下看看 2. 使用JS实现网上有很多的例子，介绍怎样用JS来控制循环，这里我们也简单的测试下 2.1 query_the_result这一步，和上面的一样，就是将结果集返回 2.2 travers_the_result这里主要是使用JS将结果集进行遍历，通过JS，将一些结果存放到变量里面，在后面的操作中就可以使用了，通过${xxx}的方式使用这个其实和Java、JS里面循环思路一样，通过结果集的总数“total_num”和下标“LoopCounter”进行判断 2.3 evaluate_the_loop_count这一步，就是判断下标的值和结果集的总数，进行对比， 2.4 print_the_log输出下，我们想要使用的变量 2.5 manage_the_loop_index这一步，给下标加一，然后获取下一条记录好了，执行下，我们看看好了，循环的使用先介绍到这里]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（七）- 资源库的使用]]></title>
    <url>%2F2017%2F03%2F29%2FKettle-handbook-07%2F</url>
    <content type="text"><![CDATA[1. 为什么使用资源库之前，我们新建转换或者作业的时候，都是直接保存在本地，而如果我们是多人开发的话，除了使用SVN等版本控制软件，还可以使用Kettle的资源库，他会将转换、作业直接保存在数据库中，而且，连接资源库的话，我们就不需要每一次都新建数据库连接了，用起来还是蛮方便的。 2. 新建资源库Kettle7.0里面，是在右上角这个Connect来连接的 2.1 资源库的类型资源库有3中类型Pentaho RepositoryDatabase Repository（使用数据库存储）File Repository（使用文件存储） 2.2 新建Pentaho Repository我们单击上面的get started 之后，就会进入新建界面http://localhost:8080/pentaho一开始还没搞懂这个Server到底怎么启动，后来google了半天发现后来又找到了这个，应该是要安装其他的组件才行，这个类型的库就放弃吧。。 2.3 Database Repository好了，这回，我们选择哪个database的资源库我们填一个connection的名字，然后配置一个资源库的连接就可以了，最好给kettle新建一个数据库使用至于数据库连接的话，和转换里面是一样的，大家可以自行新建一个配置好，以后，大家选择Finish就可以了，然后，我们可以连接下这个库，注意下，这里的用户名和密码，默认是admin/admin，大家直接登录就好了，这是Kettle自己初始化的这个怎么改呢，暂时还没有发现，待研究，等我再google看看，估计官网上会有。（找了下，发现了在哪改密码，就是刚刚的搜索资源库)连接后，我们正常使用就好了，没啥两样，会多一些功能，比如，探索资源库这里我们再保存作业和转换的话，会直接保存在数据库中，而且，很好的一个功能，个人感觉，就是数据库连接只需要创建一次，在哪里都可以用了，不需要再次创建。 2.4 File Repository这个和database的资源库，就差不多了，只不过是基于文件的，保存在本地就可以了这个就和Eclipse一个工作区差不多，转换、作业都保存在这个目录下好了，关于资源库，就简单的说这些了，大家可以自行连接，试试。]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（六）- Hop小记]]></title>
    <url>%2F2017%2F03%2F29%2FKettle-handbook-06%2F</url>
    <content type="text"><![CDATA[1. 什么是Hop在我们前面，使用Kettle过程中，控件与控件之间的连线，这里，我们详细介绍下它，它在Kettle中叫Hop（跳）。 2. Hop的发送方式（转换）在转换中，一般情况，控件和控件之间只有一个Hop，当然，如果需要的话，我们拖了2个控件出来，像这样：Kettle会提示你，下面的信息，让你选择，数据发送的方式 2.1 分发记录目标步骤轮流接收记录，其实就是你一条，我一条，轮着接收数据，这个我们试一下就知道了，我们执行下，看看这个结果试试，我们再步骤度量中，可以看到，a.txt和b.txt分别写入的数量看看结果文件，就是这样的 2.2 复制记录所有记录同时发送到所有的目标步骤，这个看起来就简单多了，比如上面的例子，2个文本文件会接收到同样的所有的数据，我们也试一下结果文件的话，就是2个节点，接收到的数据都是一样的 3.Hop的状态（作业）在作业中，Hop主要用来控制流程有3种状态，一个锁，一个绿色的对号，一个红色的叉号简单来说，：表示无论上一步执行成功还是失败，都一定会执行下一步：表示上一步执行成功才会执行下一步：表示上一步执行失败执行下一步比如我们上面的例子，我们的转换执行成功后，就结束了，如果转换执行失败了，我们就发送邮件。]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（五）- 实例-增量同步数据]]></title>
    <url>%2F2017%2F03%2F28%2FKettle-handbook-05%2F</url>
    <content type="text"><![CDATA[综合前面的几个例子，我们这里来是实现下增量数据的同步。这里只是分享一种方法，实际工作中，还会有其他更好的方案。增量同步的整体思路一般就是：首先拿到这张表的增量数据，怎么拿增量呢，源表需要有一个时间字段，代表该条记录的最新更新时间（及只要该条记录变化，该时间字段就会更新），当然有时间字段最好了，没有的话，可能需要做全表对比之类的操作；正常情况下，业务系统的表中都是有主键的，我们拿到增量数据之后，需要判断该记录的新插入的，还是更新的记录，如果是更新记录，我们需要先将数据加载到中间表，然后，根据主键将目标表中已存在的数据删除，最后再将本次的增量数据插入到目标表。 1.配置表的设计（元数据表）首先我们需要一张配置表，来保存我们要增量同步的表的基本信息1234567--元数据表create table tm_etl_table( table_name varchar(50), --表名 is_run int , --调度状态 update_time timestamp,--表数据更新时间 etl_insert_time timestamp --记录更新时间); 我们初始化一条记录，我们就以这张ods_tm_book表一些基础表准备 12345678910-- 源表create table tm_book(id int,book_name varchar(10),latest_time timestamp);-- 源表数据初始化insert into tm_book(id,book_name,latest_time)select x,x||'_name',clock_timestamp() from generate_series(1,10) x;-- 目标表和中间表create table ods_tm_book(id int,book_name varchar(10),latest_time timestamp,etl_insert_time timestamp);create table staging_tm_book(id int,book_name varchar(10),latest_time timestamp); 源表中的数据 2.同步数据的流程开发整体流程是这样的，注意下，这个只是为了简单演示了这个增量的例子，实际应用的话得修改，这是有漏洞的。 2.1更新元数据表的状态并获取表更新时间就是我们第一个状态，我们更新tm_etl_table表，更新is_run=0，表示我们开始同步数据了，update_time，初始化为 ‘1970-01-01’，表示我们要拉取所有的数据这里，我们将该表的更新时间作为变量，我们会在后面的转换中使用 2.2 加载数据到中间表我们这里，直接表对表，将数据插入到staging其中，表输入中，我们需要根据前面的更新时间变量，获取增量数据，注意，需要勾选上“替换SQL语句中的变量”这里，我们直接就表输出到中间表，每次都需将清空表数据 2.3 加载数据到目标表这里，主要有3段脚本（为了方便，就这样吧），根据主键ID，清空目标表数据，然后，将数据插入到目标表，最后，更新tm_etl_table表中的记录状态好了，用Kettle实现一个增量的逻辑大概就是这样了， 3.小结这里整理几个问题 3.1 中间表这里的话，使用了中间表，Kettle中是有一个控件的，应该叫那个“插入/更新”，可以根据主键将数据更新掉，这个控件之前使用时，发现很慢，就一直没用，后面的话，可能会写个例子，简单测试看看。使用中间表，缓存下数据，也是不错的方法。 3.2 增量流程目前公司中，增量抽取，是这样的，首先各个业务系统的数据导出到文本文件，然后批量将文件加载到数据仓库中（这里使用循环加载的）。因为每天的数据量比较大，如果知己到表的话，会很慢，使用文件，一些数据库都有批量加载的命令，很快很方便，比如：PostgreSQL中的copy命令，Greenplum中的外部表，还有Mysql中的load data等等。]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（四）- 变量的使用]]></title>
    <url>%2F2017%2F03%2F28%2FKettle-handbook-04%2F</url>
    <content type="text"><![CDATA[我们在这一回，介绍下，Kettle中全局变量的使用，我们前面说过的配置文件，其实就是配置全局变量的地方Kettle手册（三）- 配置文件的使用及密码加密 1. 全局变量 就是我们上面说的kettle.properties文件，我们在里面定义的变量，我们可以在所有的转换或者作业中获得到，比如，我们前面，说的数据库参数之前，我们已经在数据库连接中测试过，是可以，这里，我们输出下这个变量，看看 1.1 输出变量的值我们这里，用到了“获取变量”这个控件我们单击，”Get Variables”,就可以获取到当前的全局变量信息我们选择几个输出试试还有一个，”日志“控件，拖好之后，我们直接执行，日志中，我们会看到，我们定义在文件中的参数（加密的参数，我没有重启，所以显示的还是原来的）那我们，可不可以，动态的增加变量呢？ 1.2 动态增加变量刚刚也在网上找了些资料，尝试了下，这里简单分享下（貌似，这得算是对局部变量的操作，暂时就放在这里吧）我们先试下在转换中设置变量，作业中也是可以使用的，我们后面再说测试流程是这样的， 我们再表输入中，有2个时间参数，然后作为变量比如，有这样一个场景，我们每天需要定时调度一些SP，SP都有开始时间，结束时间，调用时，需要传参数进去，这个时候，我们在使用Kettle的时候，就可以通过这样的方式，去设置变量，然后再调用SP我们单击获取字段后，就可以了，这里可以修改变量存在的范围执行后，输出，后面，我们就可以使用这2个时间变量了这里使用的时候，也遇到一个问题，就是变量的默认值，一直都没有生效，不知道为什么，不管是，静态值，还是变量值，都没有办法，待研究。 2. 局部变量（命名参数） 在kettle中，相对于全局变量，我们还可以使用局部变量。感觉，这个全局变量，局部变量，都是相对而言的，就网上大部分资料来说，Kettle中的局部变量就是“命名参数”我们再转换中，右键单击，选择，转换设置 我们选择，“命名参数”，定义一个变量，我们给一个默认值然后，在日志中，将变量输出我们执行下，这个转换，运行时的界面，我们可以看到，这个参数是可以动态改变的，或者，我们再命令行调这个转换的时候，同样可以给他赋值运行结果，这个就是简单的局部变量了]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（三）- 配置文件的使用及密码加密]]></title>
    <url>%2F2017%2F03%2F28%2FKettle-handbook-03%2F</url>
    <content type="text"><![CDATA[好了，我们上一回，练习了一个从数据库导出数据到Excel的例子，我们想一下，如果有很多个转换，我们没链接一次数据库，是不是都需要重复的输入那些数据库地址啊，数据库啊，用户名啊之类的。其实是不用的，我们可以使用变量的方式，写在配置文件中，下面，我们来看看。而且，我们平时开发，都有开发环境、UAT环境、生产环境，连接的地址都不一样，也不可能手动的去修改。 1. Kettle的配置文件 配置文件在哪呢？Windows下，是再当前用户的目录下，一般再C盘，Users下面，有一个当前用户的文件夹，下面有.kettle文件夹进入之后，我们会看到一个kettle.properties的文件，我们的数据库配置信息，就可以放在这里， 我们打开之后，编辑一下保存后，我们要重新启动下Kettle，因为这个配置文件是启动时加载的重启后，我们将上一次，配置的转换打开，使用变量替换下之前的配置，Kettle中，我们使用${xxx}，表示引用一个变量，执行时，会自动替换我们测试下，同样时可以成功的。好了，这样，以后，不管是，数据库地址变化，还是部署生产，我们只需要修改配置文件就可以了。 2. 密码加密 这里，顺便说下，加密的问题，比如，我们上面的数据库密码，是明文的，这样是不太安全的，而实际上，我们都是需要对密码进行加密的我们进到Kettle的安装目录我们会看到，这里有一个Encr.bat，这就是可以加密的脚本使用方法我们输入1Encr.bat -kettle postgres 执行后，会生成，这样一个加密后的密码，然后，我们可以使用这个加密后的字符串，替换我们的密码1pg_password = Encrypted 2be98afc86aa7f2e4cb79ff228dc6fa8c 大家可以试下，这样也是可以的，好了，这个例子就到这。]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（二）- 将数据导出为Excel]]></title>
    <url>%2F2017%2F03%2F27%2FKettle-handbook-02%2F</url>
    <content type="text"><![CDATA[好了，我们先来看第一个例子，就是怎样将数据库中的数据，导出为Excel。平时，如果我们需要将数据导出Excel的话，我们可能会直接复制，然后粘贴出来，但是数据量大的话，就不好用了；或者使用Java等开发语言，写代码，导出Excel；或者一些数据库连接工具自带的导出功能。其实，我们用Kettle的话，还是很方便的，但是平时用下来，Kettle的这个功能还是有些缺陷的，比如导出Excel2007+的时候，经常会报错，我一直也没有解决，这次记录博客顺便研究看看。 1. Kettle的下载及使用 正式开始之前，我们简单说下Kettle的安装配置啥的，Kettle是绿色的，下载之后，直接运行就可以了刚刚在网上下了个最新版的，后面，我们就是用这个7.0版本介绍官网地址：Kettle官网 他这个网站，应该是不太好访问，有VPN的话，可以用起来，下载的话，大概800M左右，后面看看上传一份，昨天为了下载，现冲了个蓝灯的会员解压以后，目录大概是这样的，我们会看到，这里有.bat文件和.sh文件，.bat就是我们在windows下使用的，.sh就是在Linux下使用的，我们找到 Spoon.bat这个文件，就可以启动Kettle了，奥，对了，得先安装下Java打开后，就是这样了，都是图形界面的，很好用Kettle中，主要有2中任务，一个是作业，一个是转换。一般来说，转换是一系列具体的操作，比如：调度SP，导出Excel等等；作业的话，就是按照一定流程来调度一系列转换。大概是这样，实际上，他们也是可以嵌套调用的，我们后面可以再讨论。 2. 第一个转换-将数据导出为Excel 为了实现这个功能，我们需要： 连接到数据库 导出为Excel 首先，我们新建一个转换，新建，之后，我们可以看到，工具箱中，有很多的控件，我们都可以使用，很多我也没有用过，大家可以自行去尝试使用好了，下面，我们就开始介绍我们这次的主题，导出数据到Excel既然，是导出数据，说明我们肯定有一个源头，一个目标，源头是我们的一个数据库，我们得先连接到这个数据库 新建数据库连接我们在主对象库中，DB连接上，右键单击，新建在这里呢，我们可以看到，有很多的数据库可以选择，我们只需要填写基本的连接信息就可以了我们这里连接的是Postgresql，配置好后，测试下，（坑，刚刚在windows上装的数据库，一直连不上，白名单都加好了，就是不行，结果是防火墙忘关了。。）好了，可以连接到数据库了，下面，我们得把数据导出啊，我们需要使用输入这个控件输入下面，有很多的控件，我们这次只使用表输入，因为我们是直接从数据库中拿数据这里直接就是拖拽的，拖过去就行了，双击之后，可以编辑，这里我们就使用刚才的数据源连接，然后查询一张表，表的话，随便create一张就可以了，我们还可以预览数据源头好了，同样的思路，我们需要一个目标，就是输出了，输出到Excel同样的，我们托好之后，双击就可以编辑了，这里，我们主要关注2个配置，一个是excel保存地址，和字段我们选择一个地址，然后得，看下字段那个tab，我们单击，获取字段，就可以从源头获取表中的字段了，当然，我们可以只导出，我们需要的字段，一步一步来的话，上面获取，可能会获取不到，因为，有一步，需要将2个控件，连起来，源头有了，目标也有了，得让他们关联起来啊，再Kettle中，这个连线叫做Hop（跳），就像一个管道一样，将数据流从一个点，指向另一个点。都好了，以后，我们就运行下和Java里面，一样，绿色的话，就代表成功了我们看下文件好了，我们的第一个例子，就成功了，还是很简单的，主要就是Kettle中控件的熟悉。]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（一）- 序及Kettle简介]]></title>
    <url>%2F2017%2F03%2F27%2FKettle-handbook-01%2F</url>
    <content type="text"><![CDATA[1. 序 好久没有写博客了，新的一年总得留下点儿什么。目前主要负责数据仓库这一块任务，平时用用Kettle、SSIS这类ETL工具，而且工具的使用整理起来会方便些。所以先从Kettle开始，一点点整理下最近BI开发中掌握的知识。以前有做过BI报表Cognos开发还有些入门级的Java，都在CSDN博客上，感兴趣的同学可以去看看：于贵洋的博客好了，下面就根据自己的经验和理解，整理下Kettle的知识。 2. Kettle简介 Kettle这东西是干嘛的呢？Kettle是一个开源的ETL工具，所以基本的数据抽取、转换、加载，他都可以。比如：我要把一个mysql数据库的数据同步到一个Postgres数据库，我们有哪些办法呢?大概会有: 将数据导出为文本文件，使用PG的copy命令直接加载 数据量少的话，直接拼接成insert脚本，批量插入 一些开源的小工具，提供2种数据库直接的同步 Kettle 等等方法再比如：我每天需要统计一些系统中的异常数据，导出为Excel，用邮件发送给指定的开发人员处理，该怎样做呢？ Java或者其他开发语言做定时任务 Kettle 和其他的ETL工具相比，他有什么优势呢？ Kettle是基于Java开发的，是开源免费的，大家可以直接在网上下载；跨平台，Windows，Linux都可以使用；使用起来简单快捷。 既然开源，相比于其他收费产品，劣势也就很显然了，比如稳定性啊，BUG修复处理啊，而且基于Java，性能上会差些。当然都是相对来说，一般数据量使用或者逻辑不复杂的话，使用起来是很适合的。 刚刚也在社区上，发现了Kettle的视频，kettle视频，大家可以看看，应该用的到。Kettle的基本介绍就这些，后面会根据实际的例子，来介绍下Kettle的使用。]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python-连接MySQL]]></title>
    <url>%2F2017%2F01%2F18%2Fpython-database-05%2F</url>
    <content type="text"><![CDATA[MySQL是很常用的数据库，这里，我们来看看，怎样使用Python连接MySQL连接MySQL，通常使用PyMySQL #安装PyMySQL主页：https://pypi.python.org/pypi/PyMySQLGitHub地址：https://github.com/PyMySQL/PyMySQL/ 通常我们使用1234pip install PyMySQL#或者加上本地源pip install PyMySQL -i https://pypi.douban.com/simple/ 就可以了，但是，我这里一个网速不行，一个报错，说编码有问题（错误忘记截图了）那就试下，手动下载这个whl文件，进行安装 这个安装没有问题 然后，我们验证下12345import pymysqldb = pymysql.connect("localhost","root","shishi","test" )print(db) 没有报错，说明成功了 #实例使用的话，可以参考文档http://pymysql.readthedocs.io/en/latest/modules/cursors.html 下面，我们看看简单的使用1234567891011121314151617181920212223# -*- coding: utf-8 -*-"""Created on Wed Oct 18 12:18:14 2017@author: hexo"""import pymysql#新建连接con = pymysql.connect("localhost","root","shishi","test" )#创建一个游标cursor = con.cursor()# 执行SQLcursor.execute('show databases')#获取所有结果集rs = cursor.fetchall()print(rs)#关闭连接con.close() 中文乱码问题暂时只想用来查询，所有其他的操作后续在整理，这里记录一个问题，中文乱码的问题按照上面的代码来查询数据的话，如果有中文，会显示乱码 我们只要在新建连接的时候，修改下就行了12#新建连接，加上这个charset参数就行了con = pymysql.connect("xxx","xxx","xxx","xxx",charset='utf8' ) select这里获取结果集的fetch方法]]></content>
      <categories>
        <category>Python-数据库</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>PyMySQL</tag>
      </tags>
  </entry>
</search>
