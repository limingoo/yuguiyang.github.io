<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[博客迁移整理中(置顶)]]></title>
    <url>%2F2017%2F12%2F31%2Ftop%2F</url>
    <content type="text"><![CDATA[Hi，欢迎访问我的博客。最近正在迁移文章到这里，不定时更新中，请期待。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-疑问汇总]]></title>
    <url>%2F2017%2F10%2F12%2Fmysql-handbook-12%2F</url>
    <content type="text"><![CDATA[MySQL疑问汇总 这里吧同学们遇到的问题都汇总起来，方便大家一起查阅。 update at 2017-10-13Workbench安装问题昨天就说过Workbench的安装问题，具体的可以往下看，这里记录一个类似问题因为安装Workbench需要一些依赖先安装，比如那个.NET Framework，官网上提供的连接地址应该没有修改，如果直接跳转去下载，应该是.NET FRAMEWORK４.５的，但实际安装的时候，是需要4.5.2的 而且在安装4.5的时候，可能还会遇到这样的情况，说本地已经安装过了，所以去下载4.5.2就可以了， 后面经过同学的验证，就是这样解决的，没有问题。 连接测试数据库今天遇到一个使用客户端连接测试数据库一直不行的问题。背景是这样的：测试数据库在阿里云上，很多人都可以访问，说明数据库配置啥的没有问题，但是有一位同学的电脑就是访问不了，报下面这个错误 网上也找过，都说是服务器端配置问题，但是已知服务器端没有问题，这就奇了怪了，该电脑也是可以ping通服务器的。后面了解到，这位同学是在外企，公司的网络是美国的网络，但是网上查说国外访问国内的阿里云是可以的。没有想到其他什么原因，估计就是所在的公司网络有问题，导致连不到阿里云。最后，同学回家后，用家里的网络就可以访问了。这种问题，常见解决方法就是，排查2个方面 服务器端配置问题 本地网络问题 ###SQLZoo练习题原题地址http://zh.sqlzoo.net/wiki/More_JOIN_operations第13题 这道题呢，不是很难，有2点比较难 英文的，哈哈，借助了有道翻译 一个字段没理解含义，导致出错 题目是啥意思呢？就是说要返回一个演员列表，按照字母顺序，主演过30个以上的角色我们只要搞明白那几张表就行了 这个演员名单表，存的是电影ID和演员ID，注意这个ord表示的是，电影中演员的排名，ord=1才表示主演参考介绍http://zh.sqlzoo.net/wiki/More_details_about_the_database. 这些都搞明白，SQL就容易了123456789select name from actor where id in (select actorid from casting where ord=1 group by actorid having count(distinct movieid)&gt;=30)order by name desc-- 或者这样select a.name from actor a join casting b on a.id=b.actorid where b.ord=1 group by a.namehaving count(distinct b.movieid)&gt;=30 上面的order by可以不要，好辣，今天问题整理到这里。 update at 2017-10-121. Workbench、Navicat是干嘛的，和SQL有啥关系？Workbench和Navicat都是数据库管理工具，让我们可以方便快捷的管理和使用数据库。类似的工具有很多，免费的、收费的好多好多，就好像共享单车，ofo、mobike、小蓝单车、小鸣单车……用哪个都可以，看我们的选择了。 SQL呢？是一种查询语言，虽然和写代码编程有关，但是不用怕，记住九九乘法表，后面就是活学活用了。我们为了和数据库成为好朋友，得多和他聊天吧，但我们和他不是一个星球的，那咋办？巧了，他听得懂SQL，那我们学下SQL，就可以愉快的玩耍了。这里呢，我们还得注意一下，目前市场上在使用的数据库有很多， SQL是一套标准，其他每个数据库都会实现规定的基本功能，然后拓展些自己的语法，所以很不幸，换一个数据库，我们的SQL不一定完全跑的通。但是，只要我们掌握了标准的SQL语法然后针对不同的数据库，关注下特殊语法就可以了（会骑ofo，换了Mobike就不会骑了吗？） 综上所述，我们是通过管理工具（Workbench、Navicat……）去使用和管理数据库（MySQL…..），在管理工具中，我们使用SQL来和数据库互动。 2. Workbench的使用参考：MySQL-Workbench使用 3. sqlzoo第九题题目地址: sqlzoo首先是关于all的疑问 关于all可以参考这里先简单了解下：MySQL-子查询的使用 SQL这样写是没问题的1234567891011121314151617SELECT name, continent, population FROM world x -- 返回符合条件的所有数据WHERE -- 使用all，判断该州的所有人口数是否都 &lt;= 25000000 25000000 &gt;= ALL( -- 我们使用子查询，获取每一个州的所有人口数 SELECT population FROM world y WHERE x.continent =y.continent ) 疑问1： 能不能把这个all放到后面像这样 答案是不行的，这个不符合all的使用语法，具体语法参考官方文档：https://dev.mysql.com/doc/refman/5.7/en/all-subqueries.html all一定要放在一个比较运算符的后面才行，所以，替换是不符合规范的，会报错 疑问2：为什么下面的写法不对123456789101112131415SELECT name, continent, population FROM world WHERE 25000000 &gt;= ALL( SELECT MAX(population) FROM world GROUP BY continent ) 我们分析下题目，他是说要查询出州下面每个城市的人口数都小于等于25000000的记录上面我们就是使用all来实现去判断每个州下面的所有城市的人口数都&lt;= 25000000，现在呢，不想去判断每个城市的人口数了，我只要拿出该州下面最大的人口数，然后判断这个最大值是不是&lt;= 25000000就可以了。思路是对的，但是SQL稍微有些问题12345678910111213141516171819-- 要么返回所有记录，要么没有返回记录SELECT name, continent, population FROM world WHERE -- 因为主表world和子查询没有关联关系，所以这里的all就变成了“每个州的最大人口数是不是都 &lt;= 25000000 -- 也就是说，下面子查询的返回值，如果都是 &lt;= 25000000，那就会返回world表的所有数据；如果有一个州的人口数 &gt;25000000，那就没有记录返回 25000000 &gt;= ALL( -- 获取每个州，最大的人口数 SELECT MAX(population) FROM world GROUP BY continent ) 所以，我们要做的改动，就是让主表和子查询有关联关系，即判断每一个州的最大值 不使用all也可以，12345-- 查询符合条件的州即可select name,continent,population from world where continent in ( -- 使用having直接过滤，判断每个州的最大人口是否&lt;= 25000000 select continent from world group by continent having max(population) &lt;= 25000000) 好了，这个问题，也写到这，大家可以看看还有没有别的写法。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-Workbench使用]]></title>
    <url>%2F2017%2F10%2F12%2Fmysql-handbook-11%2F</url>
    <content type="text"><![CDATA[MySQLWorkbench使用 简单介绍下Workbench的使用Workbench是MySQL官方提供的一个可视化管理工具，跨多个平台而且免费的，详情参考官网。我们从下载地址下载，安装就行了 安装可以单独下载，也可以使用提供的一个管理工具统一下载管理，管理工具提供了整个MySQL所有相关组件的统一管理维护，也挺方便。 我们这里就是用独立安装包了，下载完之后安装，可能会失败，因为他有2个依赖要先安装，失败的话，会提示你缺少的依赖。 这里有个小坑，这里说的是.NET Framework 4.5，但实际上需要的是4.5.2貌似不太一样，所以刚刚一直安装失败，注意下就行了.NET Framework 4.5.2 安装过程就是一路Next就行了，没啥特殊的。安装完之后，我们在桌面或者菜单找到这个 主界面应该是这样： 新建连接我们点击加号，新增一个连接 在弹出的界面上，输入数据库的连接信息，就可以了，密码下图所示的地方输入 输入完成后，我们测试下连接是否正常 都正常后，我们单击OK就好了 返回主界面后，可以看到我们刚刚新建的连接，我们双击就可以打开这个连接，开始我们的SQL学习了 功能介绍我们目前主要关心的是SQL的学习，所以那些管理功能可以先忽略掉我们主要关注“当前可以使用的数据库”和SQL的编辑界面就OK了 我们以demo库为例 这里就是demo库中的表，数据库中的数据是以表的形式保存的比如，我们看看学生表的表结构信息，单击这个表旁边的”i”图标 右边会显示一个表的信息界面，字段啊，索引啊，触发器之类的，我们这里就看看DDL 这里展示的就是t_student表的表结构信息 SQL练习下面，我们看看，怎样去练习SQL我们回到刚刚的query界面 首先，我们要选择我们使用的数据库1use demo; 写好SQL，我们单击那个像闪电一样的图标执行，下面的输出窗口会显示执行结果，绿色的对号表示正确执行 然后，就可以各种SQL开始练习了1select *from t_student; 后面，我们就可以参考前面的博客，开始学习吧MySQL-基本语法介绍]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tableau实例-帕累托图]]></title>
    <url>%2F2017%2F09%2F20%2FTableau-handbook-06%2F</url>
    <content type="text"><![CDATA[Tableau实例帕累托图 前面，我们了解了《帕累托的故事》 和 二八定律与长尾理论，这里，我们学习下，在Tableau中，如何适用Tableau来绘制帕累托图。 准备数据源：官方数据源“示例-超市”因为，帕累托分布，主要是20%的商品可以产生80%的价值，所以，我们可以使用示例数据中的订单数据，来看看订单的销量是否符合帕累托分布。 初级-每个类别的销售额分布我们先来研究下，看产品的类别销售额是否符合帕累托分布，帕累托图有一个柱形图，有一个折线图，柱形图，表示每个类别的销售额，而折线图表示每个类别的销售额占比 柱形图就直接使用子类别和销售额就行了 然后，我们实现折线图在行中，再拖一个销售额 然后修改标记的类型，改为“线” 效果图 但这个折线图，并不是我们要的，我们想要的是一个占总额的百分比我们要修改第2个“总计销售额”，添加表计算 我们主要计算类型，选择汇总-总计，并且勾选“添加辅助计算”，选择“总额百分比” 我们把修改好的“总额百分比”，拖到“标签”上，然后效果如图所示 顺手，我们修改下折线图的颜色，因为后面，我们还要合并呢 我们再折线图的y轴上，右键，单击选择“双轴” 他自动会变成这样，我们得手动改成柱形图和折线图（颜色也白改了…） 调整完之后，就是这样啦， 我们调整下柱形图的y轴刻度，让他更像些 进阶-产品占比与销售额占比上面，我们做的是，哪些类别的销售额占80%，这里，我们想根据产品来看，并且想知道产品的百分之多少占了80%的销售额 和上面的步骤类似，这里，我们使用产品名称，效果如图 但是这里，产品实在是太多了，我们想让x轴更简洁一些，就显示百分比好了 未完待续……]]></content>
      <categories>
        <category>Tableau</category>
      </categories>
      <tags>
        <tag>数据可视化</tag>
        <tag>Tableau</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二八定律与长尾理论]]></title>
    <url>%2F2017%2F09%2F18%2Fanalysis-method-01%2F</url>
    <content type="text"><![CDATA[二八定律前面，我们整理了《帕累托的故事》，里面有说过“帕累托法则”，就是这个二八定律，简单说来，就是 世界上20%的人占有80%的财富，即财富的分布式是不平衡的 这里理论不单单应用在经济学领域，其他领域也一样适用。比如：在零售或销售行业，80%的利润可能来自20%的客户，所以运用该分析方法，就可以专注于维护这20%的客户，而不是将主要精力放在那80%的客户身上，因为他们只产生了20%的利润。通过80/20分析方法，可以有效的找到影响利润的主要因素 二八定律应用场景这里列举几个广泛使用的场景 在管理学中，企业80%的利润来自20%的项目或客户 心理学中，20%的人身上集中了80%的智慧，他们一出生就鹤立鸡群 20%的有目标，80%的人爱瞎想 20%的人把握机会，80%错失机会 20%的人会坚持，80%的人会放弃 长尾理论长尾理论的关注点和二八定律不同，上面是说要关注产生重大影响的20%，而长尾理论则提倡关注剩下的那80%，在一些互联网公司，如果那80%的尾巴足够长，就可能会产生超过前面20%的那部分产生的利润。 在互联网时代，关注成本或是生产、运输成本都大大的降低，这长尾巴就产生了优势。该理论在Google、亚马逊、沃尔玛，都用数据证实过，互联网行业尤其适用。 小结二八定律的话，告诉我们关注可以产生重大影响的事情，抓住主要的；长尾理论的应用应该还是处在互联网公司中，等看看那本书，详细了解下。有的时候，换个角度看问题，的确可以发现不一样的世界。 参考资料资料来源“百度百科”，“维基百科”]]></content>
      <categories>
        <category>数据分析思维</category>
      </categories>
      <tags>
        <tag>数据分析思维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[帕累托的故事]]></title>
    <url>%2F2017%2F09%2F18%2Fjotting-01%2F</url>
    <content type="text"><![CDATA[周末听Tableau的一个培训，遇到一个帕累托图，就是关于二八定律的那个帕累托，就想着，他到底是干嘛的呢，然后，就有了这篇《帕累托的故事》，让我们开始了解一下这个经济学家。 帕累托简介 维弗雷多·帕累托（Vilfredo Pareto ，1848年7月15日—1923年8月19日），意大利经济学家、社会学家，洛桑学派的主要代表之一。生于巴黎，曾就读于意大利都灵大学，后来任瑞士洛桑大学教授。– 百度百科 哦，他是一个经济学家、社会学家，怪不得会提出“20%的人掌握着百分之80%的财富”，以前，是先知道这句话，然后才知道帕累托的大名的。 洛桑学派洛桑学派主要代表之一，什么是洛桑学派，干嘛的呢？估计是因为在洛桑大学任教，所以统称为“洛桑学派”吧 “洛桑学派”指的是，以法国人瓦尔拉斯和意大利人帕累托为中心的新古典思想流派的一支。洛桑学派的主要特点在于它对一般均衡理论的推进，从而在广度上和深度上都扩大了新古典研究方法在经济学上的适用性。洛桑学派也被称为“数理学派”（因为它们强调数理的解释说明）或者“意大利学派”（因为在早期的队伍里有很多意大利人）。– 百度百科 数理学派，难道就是用数学和统计学来证明一些经济学理论？感觉好像是这样，他们一定是实战派，用数理方法来验证实际问题，厉害。“对一般均衡理论的推进”，看看这个一般均衡理论是啥？ 一般均衡理论 一般均衡理论（General Equilibrium Theory） 是1874年法国经济学家瓦尔拉斯在他的《纯粹经济学要义》中创立的。瓦尔拉斯认为，整个经济体系处于均衡状态时，所有消费品和生产要素的价格将有一个确定的均衡值，它们的产出和供给，将有一个确定的均衡量。他还认为在“完全竞争”的均衡条件下，出售一切生产要素的总收入和出售一切消费品的总收入必将相等。该理论的实质是说明资本主义经济可以处于稳定的均衡状态。在资本主义经济中，消费者可以获得最大效用，企业家可以获得最大利润，生产要素的所有者可以得到最大报酬。– 百度百科 有点儿不明觉厉，反正是一个经济上了一个什么理论，果然是通过各种数理知识来解决真正改的经济学问题，大师就是大师，膜拜。帕累托基于这个思路，进一步延伸和拓展了这一理论。 主要成就下面，我们来了解下他的主要成就，仰望下大师 帕累托法则这就是前面说到的那个80/20法则、二八定律 帕累托法则是指在任何大系统中，约80%的结果是由该系统中约20%的变量产生的。 帕累托主要是研究经济学和社会学的，在1906年，他观察了意大利的财富分配情况，发现80%的财富掌握在20%的人手里，好残酷，但现实就是这样，我一定是那个80%里面的80%的里面的80%…… 抽象一下，就变成这样： 在任何特定群体中，重要的因子通常只占少数，而不重要的因子则占多数，因此只要能控制具有重要性的少数因子即能控制全局。 一开始这一理论只限定于经济领域，后来逐步推广到各个领域，而且深为人们认同。根据帕累托法则，延伸出很多的有趣的结论，像： 世界上80%的资源是由15%的人口耗尽的 企业中80%的利润来自于20%的项目或客户 时间管理中，20%的项目或付出可以得来80%的成绩 我们可以从不同的角度去理解和应用这一原则，比如：时间管理，财务管理、员工管理、市场营销等等，都会有意想不到的收获。 帕累托最优帕累托最优(Pareto Optimality)也称为帕累托效率（Pareto Efficiency）、帕雷托最佳配置，是博弈论中的重要概念，并且在经济学， 工程学和社会科学中有着广泛的应用。 帕累托最优是指资源分配的一种理想状态，假定固有的一群人和可分配的资源，从一种分配状态到另一种状态的变化中，在没有使任何人境况变坏的前提下，使得至少一个人变得更好，这就是帕累托改进或帕累托最优化。帕累托最优的状态就是不可能再有更多的帕累托改进的余地；换句话说，帕累托改进是达到帕累托最优的路径和方法。帕累托最优是公平与效率的“理想王国”。 这个也很有意思，直白点儿说就是，你想要过的更好，就一定会有一个人过的更差，你想要挣更多的钱，就一定会有一个人挣更少的钱，又是一个残酷的现实啊。上面理解错了，帕累托最优说的是，在你挣了更多的钱的同时，也没有人因此少挣钱，强调的是不能减少任何一个人的福利。维基百科上有一个例子：说是有假设一个社会上，只有一个百万富翁和一个快饿死的乞丐，如果富翁拿出自己财富的万分之一，就可以让乞丐免于死亡，但是这样就损害了富翁的福利（假设乞丐没有可以回报富翁的服务或资源），这种财富转移，并不是“帕累托改进”，而这种社会被称为帕累托最优的。帕累托最优，说的应该是一种理想社会，社会资源的分配已经非常合理，不会损害任何人的福利，但是这种社会里依然存在着富有与贫穷，依然存在贫富差距。 帕累托图帕累托图又叫排列图、主次图,是按照发生频率大小顺序绘制的直方图，表示有多少结果是由已确认类型或范畴的原因所造成。帕累托图在项目管理中主要用来找出产生大多数问题的关键原因，用来解决大多数问题。通过这个图，就可以看出80%的财富掌握在20%的人手里。 小结上面的内容有些比较难懂，需要慢慢的消化，后续等了解的更多了，会再补充。 参考资料这里整理的时候，综合了“百度百科”，“维基百科”等等网上资料]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-自增列]]></title>
    <url>%2F2017%2F09%2F12%2Fmysql-handbook-10%2F</url>
    <content type="text"><![CDATA[MySQL自增列 什么是自增列自增列就是一个自动增长的列，他没有什么业务含义，一般可能用来做主键，作为唯一标识。自增列一般是一个整数，相比其他的UUID占用的存储更少，网络资源占用也少。如果考虑其他因素的话，UUID使用也很多。实际应用还要考虑很多问题，不能单纯的使用 自增列是使用我们可以再create table的时候，就定义好自增列我们使用关键字 auto_increment 来指定。12345mysql&gt; create table t_book_1( -&gt; id int auto_increment, -&gt; f_name varchar(10), -&gt; primary key(id) -&gt; ); 这里的话，一定要让自增列是主键，不然会报错 123456mysql&gt; create table t_book_2( -&gt; id int auto_increment, -&gt; f_name varchar(10), -&gt; f_other varchar(10) -&gt; );ERROR 1075 (42000): Incorrect table definition; there can be only one auto column and it must be defined as a key 另一种方式就是先建表，后面再修改为auto_increment12345678910111213mysql&gt; create table t_book_2( -&gt; id int , -&gt; f_name varchar(10) -&gt; );Query OK, 0 rows affected (0.01 sec)mysql&gt; alter table t_book_2 add primary key(id);Query OK, 0 rows affected (0.02 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; alter table t_book_2 modify id int auto_increment;Query OK, 0 rows affected (0.02 sec)Records: 0 Duplicates: 0 Warnings: 0 重置自增列一般自增列，都是自动赋值的，我们先插入几条记录试试123456789101112131415161718mysql&gt; insert into t_book_2(f_name) values('aa');Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t_book_2(f_name) values('bb');Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t_book_2(f_name) values('cc');Query OK, 1 row affected (0.01 sec)mysql&gt; select *from t_book_2;+----+--------+| id | f_name |+----+--------+| 1 | aa || 2 | bb || 3 | cc |+----+--------+3 rows in set (0.00 sec) 这时候，自增列已经到3了，如果我们删除了一条数据，123456789101112131415mysql&gt; delete from t_book_2 where id=3;Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t_book_2(f_name) values('dd');Query OK, 1 row affected (0.00 sec)mysql&gt; select *from t_book_2;+----+--------+| id | f_name |+----+--------+| 1 | aa || 2 | bb || 4 | dd |+----+--------+3 rows in set (0.00 sec) 序列并不会重新从3开始，如果我们清空，也是一样的。那怎样可以重置自增列呢？ 使用truncate自动重置12345678910111213mysql&gt; truncate table t_book_2;Query OK, 0 rows affected (0.01 sec)mysql&gt; insert into t_book_2(f_name) values('ee');Query OK, 1 row affected (0.00 sec)mysql&gt; select *from t_book_2;+----+--------+| id | f_name |+----+--------+| 1 | ee |+----+--------+1 row in set (0.00 sec) 手工修改我们可以使用命令123-- 我们可以修改为我们想要的值，但如果表中有数据的话，如果修改-- 的值比当前最大值小，则会重置为最大值+1alter table table_name auto_increment = 1; 12345678910111213141516171819mysql&gt; delete from t_book_2 where id=3;Query OK, 1 row affected (0.00 sec)mysql&gt; alter table t_book_2 auto_increment=3;Query OK, 0 rows affected (0.00 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; insert into t_book_2(f_name) values('hh');Query OK, 1 row affected (0.00 sec)mysql&gt; select *from t_book_2;+----+--------+| id | f_name |+----+--------+| 1 | ee || 2 | ff || 3 | hh |+----+--------+3 rows in set (0.00 sec) 直接drop，重新create这个方法就不练习了， 手动给自增列赋值在insert的时候，手动给自增列赋值，也是可以的，手动赋值后，我们再插入的时候，就会使用当前自增序列的最大值1234567891011121314151617mysql&gt; insert into t_book_2(id,f_name) values(9,'xx');Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t_book_2(f_name) values('yy');Query OK, 1 row affected (0.01 sec)mysql&gt; select *from t_book_2;+----+--------+| id | f_name |+----+--------+| 1 | ee || 2 | ff || 3 | hh || 9 | xx || 10 | yy |+----+--------+5 rows in set (0.00 sec) 附录发现一篇好文章，讲的还不错：数据库自增列]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-聚合函数]]></title>
    <url>%2F2017%2F09%2F11%2Fmysql-handbook-09%2F</url>
    <content type="text"><![CDATA[MySQL聚合函数 聚合函数也是函数的一种，比较常用，这里我们就单独拿出来介绍下。聚合函数一般配合group by来使用，经常是用来对数据集中的数值求和、平均值啊这里类的。 聚合函数的默认特性 忽略NULL值 如果没有匹配的记录，返回NULL 如果没有使用group by，则默认对所有字段进行group by 常用聚合函数这里的测试数据依然使用前面的数据，可以参考前面的文章。 count统计结果集的数量,没有结果时，返回012345678910111213-- 我们以学生表为例，来统计每个班级的学生人数select c_id,count(1), count(s_id), -- 这里学生ID是唯一的，所以是否使用distinct是一样的 count(distinct s_id), -- 统计班级的个数 count(distinct c_id)from t_student group by c_id; 当我们只使用count，不使用group by的时候，相当于对所有字段进行group by12345select count(1)from t_student; 这里，我们再来看下count对于null值得处理1234567891011121314select -- count(*),是包括null值的 count(*), -- count(1),也包括null值 count(1), -- 指定字段的时候，是不包括null值的 count(id), -- 同样也不包括null值 count(DISTINCT id)from ( select 1 as id union select NULL union select 2)x avg、sum计算结果集的平局值和结果集的累加和12345678910-- 统计每个学生的平均分和总分select s_id, avg(score), sum(score) from t_scoregroup by s_id; 我们再来看看avg和sum对null值的处理1234567891011121314select -- count指定字段，是不包括null值的 count(score), -- avg也是不包括null值的 avg(score), -- 如果想要统计score的记录，需要使用ifnull进行判断 avg(IFNULL(score,0)), -- 求和 sum(score)from ( select 10 as score union select NULL union select 20)x; min、max统计结果集的最小值和最大值123456789101112select s_id, -- 最低分 min(score), -- 最高分 max(score)from t_scoregroup by s_id; 如果没有匹配的记录，则返回null；如果结果集中有null值，会忽略null123456789select max(score), min(score)from ( select 10 as score union select NULL union select 20)x; group_concatgroup_concat会将函数聚合后的所有值以逗号分隔，以字符串展现1234GROUP_CONCAT([DISTINCT] expr [,expr ...] [ORDER BY &#123;unsigned_integer | col_name | expr&#125; [ASC | DESC] [,col_name ...]] [SEPARATOR str_val]) 123456789select s_id, -- 将该学生所有的成绩以逗号分隔显示 group_concat(score)from t_scoregroup by s_id; having在聚合函数的使用过程中，通常还会使用having来对聚合后的数据进行过滤12345678910select s_id, sum(score) sum_scorefrom t_scoregroup by s_id-- 总分大于150分having sum_score &gt; 150]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-子查询的使用]]></title>
    <url>%2F2017%2F09%2F11%2Fmysql-handbook-08%2F</url>
    <content type="text"><![CDATA[MySQL变量的使用 什么是子查询子查询是将一个 SELECT 语句的查询结果作为中间结果，供另一个 SQL 语句调用。像这样：12-- 我们将学生表中的所有班级ID当做中间结果select *from t_class where c_id in (select distinct c_id from t_student); 常用比较符子查询最常用的用法： non_subquery_operand comparison_operator (subquery)其中操作符通常为= &gt; &lt; &gt;= &lt;= &lt;&gt; != &lt;=&gt; 其他的都不说了，这里说下这个&lt;=&gt;,以前还真没用过&lt;=&gt;和=比较类似，也是判断是否相等，相等返回1，不相等返回2123456789101112131415mysql&gt; select 1&lt;=&gt;1,1&lt;=&gt;2;+-------+-------+| 1&lt;=&gt;1 | 1&lt;=&gt;2 |+-------+-------+| 1 | 0 |+-------+-------+1 row in setmysql&gt; select 1=1,1=2;+-----+-----+| 1=1 | 1=2 |+-----+-----+| 1 | 0 |+-----+-----+1 row in set 和=不一样的地方，是对NULL的支持，用&lt;=&gt;可以判断是否为null，而等号则是出现null，结果就为null1234567mysql&gt; select 1&lt;=&gt;null,null&lt;=&gt;null,1=null,null=null;+----------+-------------+--------+-----------+| 1&lt;=&gt;null | null&lt;=&gt;null | 1=null | null=null |+----------+-------------+--------+-----------+| 0 | 1 | NULL | NULL |+----------+-------------+--------+-----------+1 row in set any、in、some在子查询中，in平时用的比较多，这个any、some，这里简单说下any和some operand comparison_operator ANY (subquery)operand comparison_operator SOME (subquery)可以为 = > < >= all ()：表示大于所有值，即&gt;子查询中最大值&lt; all() : 表示小于所有值，即&lt; 子查询中的最小值= all(): 返回单个值时和=一样，返回多个值时貌似没啥用&lt;&gt; all(): 和not in 一样 1234select *from t_student where s_id &gt; all( select s_id from t_student where s_id in (105,109)); 标量子查询这种情况下，子查询返回单个值，可以在任何地方使用它。1234567891011121314select c_id, c_name, (select max(s_id) from t_student) as max_s_idfrom t_class;select *from t_class where c_id = (select max(c_id) from t_class); 行子查询上面我们介绍的子查询，都是返回1列多行，行子查询的话，是返回1行多列123-- 查询一班所有男生select *from t_studentwhere (c_id,s_gender) = (901,0); 这里也可以返回多行多列（也叫做表子查询）12select *from t_studentwhere (c_id,s_gender) in (select 901,0 union select 902,0); 参考资料官方文档：https://dev.mysql.com/doc/refman/5.7/en/subqueries.html]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小白学习Tableau-使用计算进行聚焦]]></title>
    <url>%2F2017%2F09%2F10%2FTableau-handbook-05%2F</url>
    <content type="text"><![CDATA[Tableau实例 什么是聚焦聚焦是一种技术，用于根据某个度量的值显示离散阈值。聚焦计算其实是一种可产生离散度量的特殊计算。 其实就是新构造了一个维度，这个维度将度量值进行分段，类似于年龄段这样的维度。这个应用场景很多，比如我们有一个达标值，想要哪些数据达标，哪些未达标。 实例数据源使用Tableau中自带的“示例-超市”，其中的订单表 交叉列表我们来看每个地区，每个子类别的数量 现在，我们想要实现这样的功能，就是我有一个达标值500，只有数量达到500，才达标，怎样展示最好呢？ 创建计算字段我们来创建一个计算字段，如果数量大于500则达标，否则不达标 然后，把他拖到颜色上 到这里，我们就可以看到，已经按照Good、Bad来区分颜色，达标、不达标 使用参数上面，我们已经达成了目的，思考下，这个达标值的问题，这个月是500，下个月可能会浮动为600或者400，但我们在代码中写死了500，不够灵活，我们就可以使用参数来控制。 然后，我们修改上面的计算字段 好了，最后，我们就可以动态的修改这个达标值了 参考官方文档：示例 — 使用计算进行聚焦]]></content>
      <categories>
        <category>Tableau</category>
      </categories>
      <tags>
        <tag>数据可视化</tag>
        <tag>Tableau</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-变量的使用]]></title>
    <url>%2F2017%2F09%2F10%2Fmysql-handbook-07%2F</url>
    <content type="text"><![CDATA[MySQL变量的使用 这里，我们简单介绍下MySQL中变量的使用 变量的作用域在MySQL中，变量的作用域主要有全局变量（Global）和会话级变量（Session）全局变量会影响当前server上的所有操作，而会话级变量则只会影响当前用户的会话，server上其他用户的会话不受影响 系统变量系统变量主要涉及MySQL Server的一些配置参数，系统变量，一般会在配置文件中进行配置，或者在使用命令启动MySQL的使用去指定。当然，在MySQL启动后，也可以动态的去修改系统变量（有些变量是不能动态修改的，可以修改的变量请参照官方文档）。 那我们怎样动态的修改系统变量呢？可以使用 SET GLOBAL or SET SESSION@@global. | @@session. 123456SET GLOBAL max_connections = 1000;SET @@global.max_connections = 1000;SET SESSION sql_mode = 'TRADITIONAL';SET @@session.sql_mode = 'TRADITIONAL';SET @@sql_mode = 'TRADITIONAL'; 查看当前系统变量的值1234567SHOW VARIABLES;SHOW VARIABLES LIKE 'max_join_size';SHOW SESSION VARIABLES LIKE 'max_join_size';SHOW VARIABLES LIKE '%size%';SHOW GLOBAL VARIABLES LIKE '%size%'; 用户自定义变量我们可以定义自己的变量，变量名为 @var_name，用户自定义变量都是会话级的 SET @var_name = expr [, @var_name = expr] …For SET, either = or := can be used as the assignment operator. 我们也可以使用select来使用变量，或者给变量赋值，但必须使用:=12345678910mysql&gt; SET @t1=1, @t2=2, @t3:=4;Query OK, 0 rows affectedmysql&gt; SELECT @t1, @t2, @t3, @t4 := @t1+@t2+@t3;+-----+-----+-----+--------------------+| @t1 | @t2 | @t3 | @t4 := @t1+@t2+@t3 |+-----+-----+-----+--------------------+| 1 | 2 | 4 | 7 |+-----+-----+-----+--------------------+1 row in set 在使用自定义变量的时候，会有一个顺序的问题，12set @p_rank:= 0;select *,@p_rank,@p_rank:=@p_rank+1 from t_student; 这样虽然可以得到我们想要的结果，但是官方并不推荐 自定义变量，是在结果返回到客户端时，才进行处理的，所以我们HAVING, GROUP BY, or ORDER BY中使用的时候，并没有效果。 参考资料官方文档：user-variablesserver-system-variables]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-常用函数]]></title>
    <url>%2F2017%2F09%2F10%2Fmysql-handbook-06%2F</url>
    <content type="text"><![CDATA[MySQL常用函数 这里介绍下，MySQL中常用的函数，函数有太多太多，不一定都需要记住，只需要有个印象，需要的时候去文档中找一下，记住一些常用的就好了 数学函数abs(x)返回x的绝对值1select abs(-10),abs(10) ceil(x)、ceiling(x)向上取整,比该值大的第一个整数1select CEIL(9.3),CEIL(9.5),CEIL(9.6),CEIL(-9.5),CEIL(-9.1) floor(x)向下取整，比该值小的第一个整数 1select FLOOR(9.3),FLOOR(9.5),FLOOR(9.6),FLOOR(-9.5),FLOOR(-9.1) round(x),round(x,y)四舍五入，round(x)，最近的一个整数，round(x,y)，这个y可以指定精度 1select ROUND(9.3),ROUND(9.6),ROUND(9.378,2),ROUND(9.455,2) rand(),rand(x)rand() 返回0~1之间的随机数rand(x) 返回0~1之间的随机数，如果x值相等，则返回值相等 1select RAND(),RAND(),RAND(10),RAND(10) 字符串函数获取字符串长度 length(str)，返回str的长度,这里要注意下中文，占3个长度,这里的长度单位是bytes1234567mysql&gt; select length('abc'),length('中国'),length('hi中国');+---------------+----------------+------------------+| length('abc') | length('中国') | length('hi中国') |+---------------+----------------+------------------+| 3 | 6 | 8 |+---------------+----------------+------------------+1 row in set CHAR_LENGTH(str)，返回str的长度，中文和英文一样，占1个字符，这里长度单位是字符1234567mysql&gt; select char_length('abc'),char_length('中国'),char_length('hi中国');+--------------------+---------------------+-----------------------+| char_length('abc') | char_length('中国') | char_length('hi中国') |+--------------------+---------------------+-----------------------+| 3 | 2 | 4 |+--------------------+---------------------+-----------------------+1 row in set 字符串拼接CONCAT(str1,str2,…)，将str1，str2拼接在一起12345678mysql&gt; select concat('h','e','gogo','中国');+-------------------------------+| concat('h','e','gogo','中国') |+-------------------------------+| hegogo中国 |+-------------------------------+1 row in set 这里要注意下NULL，如果其中有参数为NULL，则结果为NULL1234567mysql&gt; select concat('h','e','gogo',NULL,'中国');+------------------------------------+| concat('h','e','gogo',NULL,'中国') |+------------------------------------+| NULL |+------------------------------------+1 row in set CONCAT_WS(separator,str1,str2,…)，使用指定的separator进行拼接1234567mysql&gt; select concat_ws('^','e','gogo',NULL,'中国');+---------------------------------------+| concat_ws('^','e','gogo',NULL,'中国') |+---------------------------------------+| e^gogo^中国 |+---------------------------------------+1 row in set 这里是如果有NULL，对结果是没有影响的，会直接忽略NULL值 剔除空格或指定字符剔除字符串左右的空格ltrim(str),剔除左侧空格rtrim(str),剔除右侧空格TRIM([{BOTH | LEADING | TRAILING} [remstr] FROM] str), TRIM([remstr FROM] str)trim可以使用参数来控制剔除空格或者是指定的remstr，默认是空格123456789101112131415mysql&gt; select concat(ltrim(' hi '),'oo'),concat(rtrim(' hi '),'oo'),concat(trim(' hi '),'oo');+--------------------------------+--------------------------------+-------------------------------+| concat(ltrim(' hi '),'oo') | concat(rtrim(' hi '),'oo') | concat(trim(' hi '),'oo') |+--------------------------------+--------------------------------+-------------------------------+| hi oo | hioo | hioo |+--------------------------------+--------------------------------+-------------------------------+1 row in setmysql&gt; select trim('a' from 'aabbbccaa'),trim(leading 'a' from 'aabbbccaa'),trim(trailing 'a' from 'aabbbccaa');+----------------------------+------------------------------------+-------------------------------------+| trim('a' from 'aabbbccaa') | trim(leading 'a' from 'aabbbccaa') | trim(trailing 'a' from 'aabbbccaa') |+----------------------------+------------------------------------+-------------------------------------+| bbbcc | bbbccaa | aabbbcc |+----------------------------+------------------------------------+-------------------------------------+1 row in set 字符串填充LPAD(str,len,padstr)，左侧填充RPAD(str,len,padstr)，右侧填充len是指定str的长度，如果不够，则使用padstr填充，如果超了，则进行截取1234567mysql&gt; select lpad('hi',6,'@'),lpad('higogo',4,'@'),rpad('hi',6,'@'),rpad('higogo',4,'@');+------------------+----------------------+------------------+----------------------+| lpad('hi',6,'@') | lpad('higogo',4,'@') | rpad('hi',6,'@') | rpad('higogo',4,'@') |+------------------+----------------------+------------------+----------------------+| @@@@hi | higo | hi@@@@ | higo |+------------------+----------------------+------------------+----------------------+1 row in set 字符串截取LEFT(str,len)，从左侧开始截取len个字符RIGHT(str,len)，从右侧截取len个字符SUBSTR(str,pos), SUBSTR(str FROM pos), SUBSTR(str,pos,len), SUBSTR(str FROM pos FOR len)，从指定pos开始截取len个字符1234567891011121314151617181920212223mysql&gt; select left('hello',1),left('hello',3),right('hello',3);+-----------------+-----------------+------------------+| left('hello',1) | left('hello',3) | right('hello',3) |+-----------------+-----------------+------------------+| h | hel | llo |+-----------------+-----------------+------------------+1 row in setmysql&gt; select substr('hello',1),substr('hello',2),substr('hello' from 2);+-------------------+-------------------+------------------------+| substr('hello',1) | substr('hello',2) | substr('hello' from 2) |+-------------------+-------------------+------------------------+| hello | ello | ello |+-------------------+-------------------+------------------------+1 row in setmysql&gt; select substr('hello',1,3),substr('hello',2,3),substr('hello' from 2 for 2);+---------------------+---------------------+------------------------------+| substr('hello',1,3) | substr('hello',2,3) | substr('hello' from 2 for 2) |+---------------------+---------------------+------------------------------+| hel | ell | el |+---------------------+---------------------+------------------------------+1 row in set 大小写转换LOWER(str) LCASE(str)，将str转为小写UPPER(str) UCASE(str)，将str转为大写1234567mysql&gt; select lower('AppLE'),lcase('AppLE'),upper('AppLE'),ucase('AppLE');+----------------+----------------+----------------+----------------+| lower('AppLE') | lcase('AppLE') | upper('AppLE') | ucase('AppLE') |+----------------+----------------+----------------+----------------+| apple | apple | APPLE | APPLE |+----------------+----------------+----------------+----------------+1 row in set 更多字符串函数参考官网： https://dev.mysql.com/doc/refman/5.7/en/string-functions.html 日期和函数获取当前日期、时间1SELECT CURRENT_DATE,CURRENT_DATE(),CURRENT_TIME,CURRENT_TIME(),CURRENT_TIMESTAMP(),NOW() 时间戳相关函数UNIX_TIMESTAMP() 返回当前时间的时间戳，UNIX_TIMESTAMP(x) 返回指定日期的时间戳FROM_UNIXTIME(x) 将时间戳转为日期FROM_UNIXTIME(x,y) 将时间戳转为指定格式的日期 1234567891011121314151617mysql&gt; select UNIX_TIMESTAMP(),UNIX_TIMESTAMP('2017-09-10');+------------------+------------------------------+| UNIX_TIMESTAMP() | UNIX_TIMESTAMP('2017-09-10') |+------------------+------------------------------+| 1505096065 | 1504972800 |+------------------+------------------------------+1 row in setmysql&gt; select FROM_UNIXTIME(1505096033),FROM_UNIXTIME(1504972800),FROM_UNIXTIME(1505096033,'%Y-%m-%d');+---------------------------+---------------------------+--------------------------------------+| FROM_UNIXTIME(1505096033) | FROM_UNIXTIME(1504972800) | FROM_UNIXTIME(1505096033,'%Y-%m-%d') |+---------------------------+---------------------------+--------------------------------------+| 2017-09-11 10:13:53 | 2017-09-10 00:00:00 | 2017-09-11 |+---------------------------+---------------------------+--------------------------------------+1 row in setmysql&gt; extractEXTRACT(unit FROM date)返回日期/时间的单独部分，比如年、月、日、小时、分钟等等date 参数是合法的日期表达式。unit 参数可以是下列的值： 123456789mysql&gt; select extract(YEAR FROM NOW()),extract(MONTH from now()),extract(HOUR from now());+--------------------------+---------------------------+--------------------------+| extract(YEAR FROM NOW()) | extract(MONTH from now()) | extract(HOUR from now()) |+--------------------------+---------------------------+--------------------------+| 2017 | 9 | 10 |+--------------------------+---------------------------+--------------------------+1 row in setmysql&gt; datediff、timediff、timestampdiffdatediff(expr1,expr2),获取2个日期相差的天数123456789mysql&gt; select datediff('2017-09-11 10:00:00','2017-09-08 00:00:00');+-------------------------------------------------------+| datediff('2017-09-11 10:00:00','2017-09-08 00:00:00') |+-------------------------------------------------------+| 3 |+-------------------------------------------------------+1 row in setmysql&gt; TIMEDIFF(expr1,expr2)，返回expr1-expr2的时间差123456789mysql&gt; select timediff('2017-09-11 10:00:00','2017-09-08 00:00:00');+-------------------------------------------------------+| timediff('2017-09-11 10:00:00','2017-09-08 00:00:00') |+-------------------------------------------------------+| 82:00:00 |+-------------------------------------------------------+1 row in setmysql&gt; TIMESTAMPDIFF(unit,datetime_expr1,datetime_expr2)返回指定unit的datetime_expr2 − datetime_expr1时间差unit可以是MICROSECOND (microseconds), SECOND, MINUTE, HOUR, DAY, WEEK, MONTH, QUARTER, or YEAR.123456789101112131415mysql&gt; select timestampdiff(DAY,'2017-09-11 10:00:00','2017-09-08 00:00:00');+----------------------------------------------------------------+| timestampdiff(DAY,'2017-09-11 10:00:00','2017-09-08 00:00:00') |+----------------------------------------------------------------+| -3 |+----------------------------------------------------------------+1 row in setmysql&gt; select timestampdiff(HOUR,'2017-09-11 10:00:00','2017-09-08 00:00:00');+-----------------------------------------------------------------+| timestampdiff(HOUR,'2017-09-11 10:00:00','2017-09-08 00:00:00') |+-----------------------------------------------------------------+| -82 |+-----------------------------------------------------------------+1 row in set 时间加减函数对日期进行加减操作，有很多方法可以使用，最简单的是直接使用interval1234567mysql&gt; select now(),now() + interval 3 DAY,now()+interval 1 Hour;+---------------------+------------------------+-----------------------+| now() | now() + interval 3 DAY | now()+interval 1 Hour |+---------------------+------------------------+-----------------------+| 2017-09-11 10:57:18 | 2017-09-14 10:57:18 | 2017-09-11 11:57:18 |+---------------------+------------------------+-----------------------+1 row in set 当然也可使用提供的函数 ADDDATE(date,INTERVAL expr unit), ADDDATE(expr,days)DATE_ADD(date,INTERVAL expr unit), DATE_SUB(date,INTERVAL expr unit) 1234567mysql&gt; select now(),date_add(now(),interval 1 Day),adddate(now(),interval 3 Hour);+---------------------+--------------------------------+--------------------------------+| now() | date_add(now(),interval 1 Day) | adddate(now(),interval 3 Hour) |+---------------------+--------------------------------+--------------------------------+| 2017-09-11 10:59:09 | 2017-09-12 10:59:09 | 2017-09-11 13:59:09 |+---------------------+--------------------------------+--------------------------------+1 row in set 更多日期、时间函数参考官方介绍: https://dev.mysql.com/doc/refman/5.7/en/date-and-time-functions.html 条件判断函数if if(expr,v1,v2)如果expr为真，则返回v1，为假，则返回v2123456789mysql&gt; select if(1&gt;0,'ok','no'),if(1=0,'ok','no');+-------------------+-------------------+| if(1&gt;0,'ok','no') | if(1=0,'ok','no') |+-------------------+-------------------+| ok | no |+-------------------+-------------------+1 row in setmysql&gt; ifnull ifnull(v1,v2)如果v1的值为null，则返回v2，如果v1不为null，则返回v1123456789mysql&gt; select ifnull(99,20),ifnull(NULL,99);+---------------+-----------------+| ifnull(99,20) | ifnull(NULL,99) |+---------------+-----------------+| 99 | 99 |+---------------+-----------------+1 row in setmysql&gt; case when 这里可以根据多个条件来判断，在不同的情况下，返回不同的值 123456789101112select s_id,s_name,s_gender, case when s_gender=0 then '男' when s_gender=1 then '女' end gender, s_birthday, s_hobby, c_idfrom t_student;]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-regexp]]></title>
    <url>%2F2017%2F09%2F09%2Fmysql-handbook-04%2F</url>
    <content type="text"><![CDATA[MySQL正则表达式 在前面，我们了解了like的使用，它可以做一些简单的匹配 在like中，我们使用%来代替任意个或多个字符，_表示任意单个字符 但在实际的应用场景中，我们可能还会需要更强大的匹配方式，比如“正则表达式”，这就需要使用regexp。 常用的正则表达式，更多的内容，大家可以百度下 我们就以前面的t_student表为例 查询学生姓路或乔的学生信息12345-- like select *from t_student where s_name like '路%' or s_name like '乔%'-- regexpselect *from t_student where s_name regexp '^(路|乔)' 查询名字是以美结尾的学生信息1select *from t_student where s_name regexp '美$' 查询学生爱好中，有吃肉的学生信息1select *from t_student where s_hobby regexp '吃肉' 查询学生姓乔或者名字中有美字的学生信息1select *from t_student where s_name regexp '(^乔)|美' 小结这里先整理这几个简单的例子，后续会再补充。 可以参考官方的文档练习：https://dev.mysql.com/doc/refman/5.7/en/regexp.html]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-分组排序]]></title>
    <url>%2F2017%2F09%2F09%2Fmysql-handbook-05%2F</url>
    <content type="text"><![CDATA[MySQL分组排序 使用过其他数据库的同学，一定知道那些好用的开窗函数，像什么rank() over(),sum() over() 等等，实现一些功能的时候很好用，但是呢，MySQL中并没有这些函数，那怎样实现这些功能呢？我们可以使用MySQL中的变量来实现这个功能。 我们先来看第一个类型的问题，关于排序的问题 排名就是按照指定的字段，给每一条记录分配一个行号（序号），作为他的排名在postgresql中可以使用像rank() over()这样的函数1234567select course_name, s_id, score, rank() over(order by score desc) rank_scorefrom t_score 但在MySQL中，就没这么方便了，我们需要使用变量，来模拟实现12345678910select course_name, s_id, score, -- 根据排序规则，每条记录增加1 @rn:=@rn+1 as rank_score -- 初始化变量@rn，从0开始from t_score cross join (select @rn:=0) xorder by score desc; 细心的同学，会发现，上面的排名会有些差别，以前2条记录为例，因为都是87，所以postgresql中排名都是1，而在MySQL中呢，依然是自增，一个1，一个2，那我们是否能让他也是2个1并列呢？ 当然可以，我们改一下SQL只要我们判断一下，当前记录的score和上一条记录的score是不是一样就可以了，那怎样才能获取上一条记录的score呢？就是增加一个变量来记录上一条记录的score12345678910111213select course_name, s_id, score, @pre pre_score, -- 判断当前score是否和上一条记录的score相等， -- 如果相等则使用和上一条一样的排名 if(@pre=score ,@rn:=@rn,@rn:=@rn+1) as rank_score, @pre:=score cur_score -- 使用pre来保存上一条记录的scorefrom t_score cross join (select @rn:=0,@pre:=null) xorder by score desc; 上面的排序还是有点儿问题，比如有2个第1之后，依然从第2开始，那我们能不能跳过直接从3开始呢？ 我们回想下，最开始他的排名其实就是3，只是我们上边把它变成了2，那我们再有一个和之前一样的变量就够了。 1234567891011select course_name, s_id, score, @pre pre_score, @rn_1:=@rn_1+1 rank_score_1, if(@pre=score ,@rn:=@rn,@rn:=@rn_1) as rank_score, @pre:=score cur_score from t_score cross join (select @rn:=0,@pre:=null,@rn_1:=0) xorder by score desc 我们使用@rn_1来正常记录排名，当当前记录和上一条记录的score不一样时，我们使用@rn_1的排名信息。 分组排序就是给每个分组中的数据，分别进行排名如果有类似rank() over()的函数12345678-- 在postgresql中select course_name, s_id, score, rank() over(partition by course_name order by score desc) rank_scorefrom t_score 但在MySQL中，就没这么方便了，我们需要使用变量，来模拟实现这里的思路和上面的思路一样，我们需要一个变量来保存上一条记录的课程名称，如果课程名称一样，则继续排名，如果不一样，则重新开始排名1234567891011set @rn:=0;set @pre_course:=null;select course_name, s_id, score, if(@pre_course=course_name,@rn:=@rn+1,@rn:=0), @pre_course:= course_name from t_score order by course_name,score desc;]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL笔试题-MySQL练习题]]></title>
    <url>%2F2017%2F09%2F09%2Fdata-analyst-interview-sql-04%2F</url>
    <content type="text"><![CDATA[SQL笔试题 下面的SQL基于MySQL 下面整理些MySQL学习过程中，基本的练习题，题目来源于网上及个人总结。 测试数据12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970create table t_student( s_id int comment '学生ID', s_name varchar(20) comment '学生姓名', s_gender int comment '学生性别 0-男,1-女', s_birthday date comment '出生日期', s_hobby varchar(100) comment '爱好', c_id int comment '班级ID') comment '学生表';create table t_class( c_id int comment '班级ID', c_name varchar(20) comment '班级名称') comment '班级表';create table t_score( sc_id int comment '成绩ID', s_id int comment '学生ID', course_name varchar(20) comment '课程名称', score numeric(10,0) comment '成绩' ) comment '成绩表';insert into t_class values(901,'一班');insert into t_class values(902,'二班');insert into t_class values(903,'三班');insert into t_class values(905,'五班');insert into t_student values(101,'路飞',0,'1990-01-26','吃肉,睡觉',901);insert into t_student values(102,'娜美',1,'1995-10-05','足球,篮球',901);insert into t_student values(103,'乔巴',0,'1992-08-11','唱歌,吃肉',901);insert into t_student values(104,'鸣人',0,'1991-03-29','拉面,忍术',901);insert into t_student values(105,'卡卡西',1,'1989-05-10','看书,吃肉',902);insert into t_student values(106,'乌索普',1,'1988-02-02','跳舞,篮球',902);insert into t_student values(107,'乔峰',0,'1990-12-12','跑步,羽毛球',902);insert into t_student values(108,'段誉',0,'1990-12-13','吃肉,加班',903);insert into t_student values(109,'虚竹',1,'1991-01-22','看电影,旅行',903);insert into t_student values(110,'杨过',0,'2000-03-04','旅行',903);insert into t_student values(111,'令狐冲',0,'1997-03-04','喝酒',904);insert into t_score values(1,101,'数学',39);insert into t_score values(2,102,'数学',20);insert into t_score values(3,103,'数学',54);insert into t_score values(4,104,'数学',38);insert into t_score values(5,105,'数学',70);insert into t_score values(6,106,'数学',15);insert into t_score values(7,107,'数学',75);insert into t_score values(8,108,'数学',84);insert into t_score values(9,109,'数学',87);insert into t_score values(10,110,'数学',67);insert into t_score values(11,101,'语文',73);insert into t_score values(12,102,'语文',71);insert into t_score values(13,103,'语文',82);insert into t_score values(14,104,'语文',83);insert into t_score values(15,105,'语文',36);insert into t_score values(16,106,'语文',87);insert into t_score values(17,107,'语文',74);insert into t_score values(18,108,'语文',19);insert into t_score values(19,109,'语文',29);insert into t_score values(20,110,'语文',26);insert into t_score values(21,101,'英语',55);insert into t_score values(22,102,'英语',24);insert into t_score values(23,103,'英语',38);insert into t_score values(24,104,'英语',82);insert into t_score values(25,105,'英语',12);insert into t_score values(26,106,'英语',15);insert into t_score values(27,107,'英语',50);insert into t_score values(28,108,'英语',68);insert into t_score values(29,109,'英语',77);insert into t_score values(30,110,'英语',19); 查询所有课程分数都大于50分的学生信息12345678910111213141516select *from t_student where s_id in ( -- 按学生ID分组，看每个学生的最低分数是否大于50分 select s_id from t_score group by s_id having min(score)&gt;=50); 换另一种方式，实现上一题123456789101112select *from t_student where s_id not in ( -- 查询分数小于50分的学生ID select s_id from t_score where score &lt; 50) and s_id in ( -- 加上这个判断，是因为有的学生没有考试成绩 select s_id from t_score); 查询最少有2门课程都&gt;=60分的学生信息123456789101112131415161718select *from t_student where s_id in ( select s_id from t_score where score&gt;=60 group by s_id having count(1)&gt;=2); 查询每个学生的个人信息及班级信息及所有科目分数，按照班级ID升序排列，课程分数降序排列12345678910111213141516select a.*,b.c_name,c.course_name,c.scorefrom t_student ajoin t_class b on b.c_id = a.c_idjoin t_score c on c.s_id = a.s_idorder by a.c_id asc, c.score desc ; 查询每个学生的学生ID、学生姓名、班级名称、总分、平均分，按照班级名称升序、总分降序排列123456789101112131415161718192021222324select a.s_id, a.s_name, b.c_name, sum(c.score) total_score, avg(c.score) avg_scorefrom t_student ajoin t_class b on b.c_id = a.c_idjoin t_score c on c.s_id = a.s_idgroup by a.s_id, a.s_name, b.c_nameorder by b.c_name, total_score desc; 上一题，加上平均分大于60分1234567891011121314151617181920212223242526select a.s_id, a.s_name, b.c_name, sum(c.score) total_score, avg(c.score) avg_scorefrom t_student ajoin t_class b on b.c_id = a.c_idjoin t_score c on c.s_id = a.s_idgroup by a.s_id, a.s_name, b.c_namehaving avg_score &gt;= 60order by b.c_name, total_score desc; 查询数学成绩比语文成绩高的所有学生信息及数学、语文的成绩123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960-- 使用子查询select x.*, a.course_name course_math, a.score math_score, b.course_name course_language , b.score language_scorefrom t_student xjoin ( -- 所有学生的数学成绩 select * from t_score WHERE course_name = '数学') aon x.s_id = a.s_idjoin ( -- 所有学生的语文成绩 select * from t_score WHERE course_name = '语文') b on a.s_id = b.s_idand b.score &lt;= a.score;-- 2. 使用joinselect x.*, y.course_name course_math, y.score math_score, z.course_name course_language , z.score language_scorefrom t_student xjoin t_score yON x.s_id = y.s_idand course_name = '数学'join t_score zon z.course_name = '语文'and z.s_id = y.s_id and z.score &lt;= y.score; 查询英语和语文成绩都大于60的学生信息1234567891011121314151617select x.* from t_student xjoin ( select a.s_id from t_score a where a.score &gt; 60 and a.course_name in ('英语','语文') group by a.s_id having count(1) = 2) y on y.s_id=x.s_id; 查询每个学生的学生ID、学生姓名及平均分,按照平局分降序排列123456789101112131415select b.s_id, b.s_name, avg(a.score) avg_scorefrom t_score ajoin t_student b on b.s_id = a.s_idgroup by b.s_id, b.s_nameorder by avg_score desc 上一题，加上，平局分大于60分1234567891011121314151617select b.s_id, b.s_name, avg(a.score) avg_scorefrom t_score ajoin t_student b on b.s_id = a.s_idgroup by b.s_id, b.s_namehaving avg_score &gt; 60order by avg_score desc 上上一题，加上只查看平均分最高的学生信息1234567891011121314151617select b.s_id, b.s_name, avg(a.score) avg_scorefrom t_score ajoin t_student b on b.s_id = a.s_idgroup by b.s_id, b.s_nameorder by avg_score desclimit 1; 上上上一题，加上平局分第3高的学生信息1234567891011121314151617select b.s_id, b.s_name, avg(a.score) avg_scorefrom t_score ajoin t_student b on b.s_id = a.s_idgroup by b.s_id, b.s_nameorder by avg_score desclimit 1 offset 2; 换一种方式实现上一题12345678910111213141516171819202122232425select x.s_id, x.s_name, x.avg_score, @cur_rank := @cur_rank+1 as score_rankfrom ( select b.s_id, b.s_name, avg(a.score) avg_score from t_score a join t_student b on b.s_id = a.s_id group by b.s_id, b.s_name) xcross join ( select @cur_rank:=0)yorder by x.avg_score desc;]]></content>
      <categories>
        <category>笔试题</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>笔试题</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-关联查询]]></title>
    <url>%2F2017%2F09%2F09%2Fmysql-handbook-03%2F</url>
    <content type="text"><![CDATA[MySQL关联查询 前面，我们介绍的都是单表查询（就是只从一张表中获取数据），而实际应用的时候，我们都会同时查询多张表，这里，我们就介绍下，多表关联查询的使用。 SQL join 用于根据两个或多个表中的列之间的关系，从这些表中查询数据 前置知识 主键（Primary Key）：可以唯一确定一条记录的字段，比如学生表中的学生ID，生活中我们的身份证号外键（Foreign Key）：指向另一张表的主键，比如学生表中的班级ID，班级ID是班级表中的主键，但在学生表中是外键 主键和外键可以在建表的时候指定，他可以在数据库层面，控制你的数据的完整性、一致性。 测试数据参考：http://yuguiyang.github.io/2017/09/09/mysql-handbook-01/ inner joininner join 可以简写为 join，结果集是两张表中 都存在的记录，是一个交集，详情参考上面的图片。比如：在学生表中，有一个班级ID，我们想根据班级ID，在班级表中找到班级信息1234567891011select *from t_student a -- 要关联查询的表join t_class b -- 使用什么字段去关联这两张表on a.c_id = b.c_id; left join左关联，以左边的表为主表，不管外键在右表中是否存在，左表的数据都会存在。比如学生表中，有这样一条记录，他的班级ID是904，但是班级表中并没有904的班级信息，所以，使用join的话是查不到这条记录的1234567891011-- 2. left join -- 学生表为主表，包含所有学生信息select *from t_student a left join t_class b on a.c_id = b.c_id; right join右关联，和做关联类似，但已右表为主表123456789101112-- 3. right join -- 班级表为主表，不管改班级是否有学生信息select *from t_student a right join t_class b on a.c_id = b.c_id; full outer join全关联，mysql没有full join 语法，我们可以通过使用union来实现123456789101112131415161718select *from t_student a left join t_class b on a.c_id = b.c_idUNIONselect *from t_student a right join t_class b on a.c_id = b.c_id; cross joincross join 是对2个表做笛卡尔积123select *from t_class a cross join t_class b order by a.c_id,b.c_id 小结关联查询的话，我们主要是选择好主表，然后找好表与表之间的关联关系，注意多对多、一对多的这种关系，验证号结果数据就行了。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-中文排序]]></title>
    <url>%2F2017%2F09%2F09%2Fmysql-handbook-02%2F</url>
    <content type="text"><![CDATA[MySQL中文排序 测试数据参考：http://yuguiyang.github.io/2017/09/09/mysql-handbook-01/ 以前还真没有关注这个中文排序的问题，这里记录下。 一张学生表1select *from t_student; 我们根据s_name来排序1select *from t_student order by s_name; 这里的中文排序，是不对的，应该是由于字符集的问题，一般情况下，数据库中的编码都是使用UTF-8的，所以，对于中文会有问题。 从网上找到2中解决办法 create table的时候加上binary属性（经测试，不好用）注意下s_name字段，我们添加了binary属性12345678CREATE TABLE `t_student_test` ( `s_id` int(11) DEFAULT NULL COMMENT '学生ID', `s_name` varchar(20) binary DEFAULT NULL COMMENT '学生姓名', `s_gender` int(11) DEFAULT NULL COMMENT '学生性别 0-男,1-女', `s_birthday` date DEFAULT NULL COMMENT '出生日期', `s_hobby` varchar(100) DEFAULT NULL COMMENT '爱好', `c_id` int(11) DEFAULT NULL COMMENT '班级ID') ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='学生表'; 这里，我试验是失败的，中文排序依然不对 在order by 后面，使用 convert函数1select *from t_student order by convert(s_name using gbk); 使用convert函数是可以的，没有问题]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-基本语法介绍]]></title>
    <url>%2F2017%2F09%2F09%2Fmysql-handbook-01%2F</url>
    <content type="text"><![CDATA[MySQL基本语法介绍 1. 什么是SQLSQL（Structured Query Language）结构化查询语言，通过SQL，我们就可以查询数据库中的数据，而数据再数据库中又是以表的形式保存的，所以SQL查询，主要就是对表进行查询。 SQL的语法就和学习英语的语法、汉语拼音一样，满足给定的套路，去使用就可以了。 当我们拿到了数据库的连接信息，连接到一个数据库上，我们就可以开始写SQL了。 2. Navicat的使用MySQL的客户端有很多，通常使用的，可能有Navicat，还有MySQL自带的workbench。Navicat是收费产品，但在网上可以找到XX版，workbench是免费的。 这里以Navicat为例，简单介绍下。 在这里，输入数据库地址、用户名、密码等等就行了。 这一个一个圆柱形的，就是一个数据库实例，下面那些电子表格图标的就是表，数据就存储在表中。 默认是不会看到表结构信息的，我们勾选下面的配置之后，就可以看到了 3. 基本语法数据准备12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970create table t_student( s_id int comment '学生ID', s_name varchar(20) comment '学生姓名', s_gender int comment '学生性别 0-男,1-女', s_birthday date comment '出生日期', s_hobby varchar(100) comment '爱好', c_id int comment '班级ID') comment '学生表';create table t_class( c_id int comment '班级ID', c_name varchar(20) comment '班级名称') comment '班级表';create table t_score( sc_id int comment '成绩ID', s_id int comment '学生ID', course_name varchar(20) comment '课程名称', score numeric(10,0) comment '成绩' ) comment '成绩表';insert into t_class values(901,'一班');insert into t_class values(902,'二班');insert into t_class values(903,'三班');insert into t_student values(101,'路飞',0,'1990-01-26','吃肉,睡觉',901);insert into t_student values(102,'娜美',1,'1995-10-05','足球,篮球',901);insert into t_student values(103,'乔巴',0,'1992-08-11','唱歌,吃肉',901);insert into t_student values(104,'鸣人',0,'1991-03-29','拉面,忍术',901);insert into t_student values(105,'卡卡西',1,'1989-05-10','看书,吃肉',902);insert into t_student values(106,'乌索普',1,'1988-02-02','跳舞,篮球',902);insert into t_student values(107,'乔峰',0,'1990-12-12','跑步,羽毛球',902);insert into t_student values(108,'段誉',0,'1990-12-13','吃肉,加班',903);insert into t_student values(109,'虚竹',1,'1991-01-22','看电影,旅行',903);insert into t_student values(110,'杨过',0,'2000-03-04','旅行',903);insert into t_score values(1,101,'数学',39);insert into t_score values(2,102,'数学',20);insert into t_score values(3,103,'数学',54);insert into t_score values(4,104,'数学',38);insert into t_score values(5,105,'数学',70);insert into t_score values(6,106,'数学',15);insert into t_score values(7,107,'数学',75);insert into t_score values(8,108,'数学',84);insert into t_score values(9,109,'数学',87);insert into t_score values(10,110,'数学',67);insert into t_score values(11,101,'语文',73);insert into t_score values(12,102,'语文',71);insert into t_score values(13,103,'语文',82);insert into t_score values(14,104,'语文',83);insert into t_score values(15,105,'语文',36);insert into t_score values(16,106,'语文',87);insert into t_score values(17,107,'语文',74);insert into t_score values(18,108,'语文',19);insert into t_score values(19,109,'语文',29);insert into t_score values(20,110,'语文',26);insert into t_score values(21,101,'英语',55);insert into t_score values(22,102,'英语',24);insert into t_score values(23,103,'英语',38);insert into t_score values(24,104,'英语',82);insert into t_score values(25,105,'英语',12);insert into t_score values(26,106,'英语',15);insert into t_score values(27,107,'英语',50);insert into t_score values(28,108,'英语',68);insert into t_score values(29,109,'英语',77);insert into t_score values(30,110,'英语',19); select下面，我们来看看，怎样查看一张表的数据；SQL的语法呢，就好比是一个公式，初学的话我们去套用就可以了。 SELECT 列名称 FROM 表名称或者SELECT * FROM 表名称 使用Navicat执行查询12-- 查看学生表数据，指定字段select s_id,s_name from t_student; 12-- 查看所有字段select *from t_student; 排序排序是很常用的功能，我们想要对结果集进行指定的排序，就要使用order by order by 字段名默认升序，可以使用desc降序排列 12-- 学生ID降序排列select *from t_student order by s_id desc; 多字段排序12-- 班级ID升序排列，班级ID一样的按学生ID降序排列select *from t_student order by c_id,s_id desc; limit 指定返回记录的数目 我们上面，都是查询一张表所有的数据，有的时候表的数据量很大，或者我们只想看看排名前3的数据，我们就可以使用limit12-- 学生ID降序排列,取前3条记录select *from t_student order by s_id desc limit 3; where前面，我们可以查看一张表的所有数据、做排序、然后只取前几行，实际使用时，一定会有这样的需求，比如我们只想看学生ID是105的记录，就需要使用where了，它可以对数据进行过滤。 SELECT 列名称 FROM 表名称 WHERE 列 运算符 值 12345678-- 查看学生ID是105的学生信息select *from t_student where s_id = 105;-- 查看学生ID不是105的其他学生信息select *from t_student where s_id &lt;&gt; 105 ;-- 查看学生ID在103和108之间的学生信息select *from t_student where s_id between 103 and 108; 这里还有一个操作符很常用，就是 in 和 not in。in 表示在多个值中存在，加上not则表示不存在12345-- 查看学生ID是103,105，,107的学生信息select *from t_student where s_id in (103,105,107);-- 查看学生ID不在102中的其他学生信息select *from t_student where s_id not in (102); like匹配字符串，像‘xxxx’一样 %： 表示任意个或多个字符_：表示任意单个字符 123-- 查看喜欢吃肉的学生信息select *from t_student where s_hobby like '吃肉%';select *from t_student where s_hobby like '%吃肉%'; and 和 or上面，我们都是一个单独的过滤条件，实际上，我们的会有各种各样的情况，需要同时满足多种过滤条件，这就用到了 and 和 or。 AND 和 OR 可在 WHERE 子语句中把两个或多个条件结合起来。如果第一个条件和第二个条件都成立，则 AND 运算符显示一条记录。如果第一个条件和第二个条件中只要有一个成立，则 OR 运算符显示一条记录。 12345-- 查看班级ID是901的所有男生信息select *from t_student where c_id=901 and s_gender=0;-- 查看班级ID是901或者s_id大于107的学生信息select *from t_student where c_id=901 or s_id &gt; 107;]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL-序(怎样学习MySQL)]]></title>
    <url>%2F2017%2F09%2F09%2Fmysql-handbook-00%2F</url>
    <content type="text"><![CDATA[MySQL怎样学习MySQL 作为一名BI开发工程师，SQL是必须要掌握的一门技能。数据分析师请参考这篇《数据分析师是否要掌握SQL?》 SQL是干嘛的呢？大街上随便拉个人过来，可能都听说过“大数据”，不管大数据、小数据，他这个数据到底在哪儿呢？其中一种方式，就是存储在关系型数据库中（其他的还有什么非关系型数据库、HDFS等等），就像我们把货物都放在仓库里一样，如果我们想要查看数据库中的数据，就用到了SQL。 SQL也是一种编程语言，偏底层，所以学起来会枯燥些，不像Tableau那样可视化效果好，可以托拉拽。只要掌握了SQL的基本语法，他就像数学公式一样，直接去套用就行了。SQL的知识点也非常多，我们不需要都记住，要学会查文档、用Google，而且不同的数据库，他的语法可能不太一样，很容易记混。开发人员平时说的SQL脚本，就是一段或者多段SQL或者存储过程。 怎么学习SQL?随着互联网的发展，现在的学习成本非常的低，网络上的免费资源非常多。评价好些的像w3school的SQL 教程在这，就可以了解SQL的基本语法，记住这些，再配合习题练习就算是入门了。 sqlzoo是一个在线的练习网站,上面有很多SQL的练习题，我们可以直接在线写SQL，而且提交后，还会告诉我们对错。 如果还有时间的话，找一本书《SQL必知必会》、《MySQL必知必会》、《深入浅出SQL》，查漏补缺，可以更好的梳理自己的知识架构。 SQL就像所有的知识一样，学会不用很容易就忘掉，所以最好可以在工作中多用用，而且一定要记住“实践是检验真理的唯一标准”，SQL一定要多练习、多实践。 附录客户端安装及pdf资料：SQL工具及资料后面也整理了一些MySQL的知识点MySQL分类，欢迎大家访问。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书笔记-数据分析实战(3章)]]></title>
    <url>%2F2017%2F09%2F04%2Freading_notes_data_analysis_02%2F</url>
    <content type="text"><![CDATA[读书笔记 《数据分析实战》 从第3章开始，都是从一个实际问题出发，套用，前面的数据分析思路，来进行模拟分析。第3章的主题是“销售额为什么会减少？”：一款社交游戏本月的销售额相较于上月有所下滑，于是想调查下滑的原因，来提升销售额。 现状和预期现状肯定是当月销售额下降，预期肯定是保持上升，等于甚至高于上月销售额，这里的话，要确定销售额下降是不是一个问题，因为该社交游戏一直保持稳定增长，所以突然下滑，一定是不正常的， 是一个问题。 发现问题我们明确了现状和预期，需要从中，找出影响最大的因素。上面说到，有3种方法去发现问题： 观察数据大小 数据分解（指标拆解） 数据对比 这一步，也是一个根据经验来提出假设的过程，我们需要从宏观角度，找到可能影响销售额的因素。我感觉，这一步是数据分析切入的点，比较重要，如果这一步没有发现核心问题，那后面的数据收集和分析都会有问题。 书中，在这一步，提出的问题是“商业宣传上存在问题”，对了，这一步，还需要及时和其他部门去沟通，像这种市场推广、商业宣传，本身我们可能不知道，所以，假设后要去确认是否有这样的情况。 数据的收集和加工数据分析解决对策]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>读书笔记</tag>
        <tag>实例</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析案例-购物篮分析]]></title>
    <url>%2F2017%2F09%2F01%2Fdata-analyst-method-01%2F</url>
    <content type="text"><![CDATA[数据分析案例 说到数据分析、数据挖掘，我们首先想到的可能就是沃尔玛那个“啤酒与尿布”的故事，它告诉我们，世间万物都有着千丝万缕的联系。这其中使用的数据分析方法就是“关联分析”。 什么是购物篮分析购物篮分析（Market Basket Analysis），购物篮就是我们去超市使用的篮子，结账的时候，购物篮中所有的商品都会被一起结算。所谓的购物篮分析就是通过购物篮子所反应的信息来==研究顾客的购买行为== 关联分析要解决的问题是：一群用户购买了很多产品之后，哪些产品同时购买的几率比较高？买A产品的同时，买哪个产品的几率比较高 这里介绍完概念，暂时还没啥写的，后面再补充下怎样实现，这里会有一个Tableau的简单购物篮分析。 参考文章上文的部分理论知识，摘自下面的博客 一起大数据]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>购物篮分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书笔记-数据分析实战(1、2章)]]></title>
    <url>%2F2017%2F09%2F01%2Freading_notes_data_analysis_01%2F</url>
    <content type="text"><![CDATA[读书笔记 《数据分析实战》 1. 什么是数据科学家书中通过“什么是数据”和“数据在商业中的应用”，推导出数据科学家的定义。 人们通过观测数据来推测出某种因果关系，再用这种因果关系来预测未来或者控制原因以达到预期的结果。把从事这种工作的人成为数据科学家。– 书中摘录 上面的定义觉得不是很清晰，就百度上找了找： 数据科学家是指能采用科学方法、运用数据挖掘工具对复杂多量的数字、符号、文字、网址、音频或视频等信息进行数字化重现与认识，并能寻找新的数据洞察的工程师或专家(不同于统计学家或分析师)。一个优秀的数据科学家需要具备的素质有：懂数据采集、懂数学算法、懂数学软件、懂数据分析、懂预测分析、懂市场应用、懂决策分析等。– 百度百科 我觉得数据科学家就是对于数据相关的所有门类都有一个整体的认识，感觉是个“杂家”，精通算法、什么深度学习、机器学习、AI之类的都是信手拈来，对我就是神一样的存在了，努力吧，同学。 2. 3中类型的数据科学家书中将数据科学家分成了3类，主要从所在领域分类： 商业领域出身 统计学出身 工程领域出身 这应该也是数据科学家成长的3条路线，从不同的路线出发，最终殊途同归。当然，这3个领域需要综合，才称得上是合格的数据科学家。 书中的技能配图，可以瞻仰下 数据分析的5个流程书中，将数据分析分为5个步骤，看完后，感觉很靠谱，真的很实用，这里分享下 商业数据分析的目的是解决问题，要解决问题，需要使用统计分析、机器学习、数据挖掘等各种方法。 现状和预期首先我们要确认“什么才是数据分析中的问题”。比如，“某种商品销售额下降”，这是一个现象，但它是不是一个问题呢？如果，该产品不是公司主打商品，并且就要下架了，那销售额下降并不是一个问题，或者，该商品处于正常的波动，或是季节、市场环境的外部因素导致的，可能都不是一个问题；相反，如果该商品是公司主打商品，并且没有其他外部因素导致，那销售额下降就是个问题了。 这里记录下，其实，还需要确认下，销售额取数逻辑是否有问题，确保数据没有问题，并且要知道这个下降是怎么定义的，是和什么商品，或时间段对比发现下降的。 有对比，才会有差距，既然下降了，说明他心里一定有个预期，即现状和预期之间是有差距的 发现问题有了上面的“现状和预期”，我们需要区别”现象和问题“。像“销售额下降”，“顾客流失”，这都是一个现象，我们需要从中去发现问题 现象 前提 预期 是否有问题 销售额下降 销售额比例低 维持现状 无 销售额下降 销售额比例高 将销售额恢复到良好状态 有 销售额上升 广告费用高 降低广告费用 有 销售额上升 广告费用适当 维持现状 无 从3个角度发现问题发现问题的关键是思考并理解现状和预期之间的差距。那怎样发现、理解这个差距呢？ 观察数据大小首先考虑有哪些因素会导致这些差距，并明确这些因素的影响程度大小，即找到影响最大的因素。 将数据分解后观察指从多个角度观察发生的现象，分解出构成这种现象的因素。在分解的时候，必须遵循MECE原则： Mutually 相互性 Exclusive 排重性 Collectively 完整性 Exhaustive 全面性 我感觉这个很抽象，不是很理解，书上有一个例子，说的还不错，常用的拆分方法是因数分解，比如： 销售额=人均销售额*购买人数 拆解后，找到容易调控的因子，才方面后面去解决问题 将数据比较后观察指的是将发生问题是的数据和没发生问题时的数据相互比较，并找出问题出现的原因。比如，按时间对比，看看同比、环比（使用时间序列） 昨天和今天比较 上周和本周比较 同一个商业活动前、后比较 与竞争对手数据比较 公司内部服务之间利益比较 年龄段差异 性别差异 地域差异 3.3 数据收集和整理通过前面，对现状和预期的对比，发现影响最大的因素后，我们就需要开始收集数据，来验证问题。数据收集的话，还会涉及到怎样去采集数据，比如想要的数据，并没有保存下来。已保存下来的数据，通常会保存在文件、数据库或者是Hadoop（HDFS）中收集完数据，我们就需要对数据进行加工，变成我们后面分析需要的格式，比如使用SQL进行处理，或者Python、R进行整合；我们再加工数据的同时，为了方便我们后面的分析，可能还需要增加一下自定义的变量，比如一些标志位，像“已消费（1），未消费（0）”；或者是一些离散变量，类似于区间段： 消费金额较大用户（1） 消费金额一般用户（2） 消费金额较小用户（3） 3.4 数据分析书中把数据分析按目的，分为两大类：“决策支持和自动化、最优化”。其中，“决策支持”使用简单求和、交叉列表的方式分析，还会涉及预测模型；“自动化、最优化”则涉及机器学习、构建算法。 3.5 解决对策通过上面两种分析思路，我们需要针对分析的结果，来判断是否要采取对应的决策，不同的对策， 又会产生不同的沟通成本。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>读书笔记</tag>
        <tag>实例</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL笔试题-连续登录天数]]></title>
    <url>%2F2017%2F08%2F31%2Fdata-analyst-interview-sql-03%2F</url>
    <content type="text"><![CDATA[SQL笔试题 下面的SQL基于PostgreSQL 1.用户连续登录天数背景描述现在我们有一张用户登录日志表，记录用户每天的登录时间，我们想要统计一下，用户每次连续登录的开始日期和结束日期，以及连续登录天数。 用户ID 登录日期 1001 2017-01-01 1001 2017-01-02 1001 2017-01-04 1001 2017-01-06 1002 2017-01-02 1002 2017-01-03 同学们先思考下，整理下思路，如果没有思路或者某几个点不了解，就可以继续往下看了。 测试数据1234567891011121314151617181920212223242526272829303132333435363738CREATE TABLE interview.tm_login_log( user_id integer, login_date date)WITH ( OIDS=FALSE);-- 这里的数据是最简化的情况，每个用户每天只有一条登录信息，insert into interview.tm_login_log values(1001,'2017-01-01');insert into interview.tm_login_log values(1001,'2017-01-02');insert into interview.tm_login_log values(1001,'2017-01-04');insert into interview.tm_login_log values(1001,'2017-01-05');insert into interview.tm_login_log values(1001,'2017-01-06');insert into interview.tm_login_log values(1001,'2017-01-07');insert into interview.tm_login_log values(1001,'2017-01-08');insert into interview.tm_login_log values(1001,'2017-01-09');insert into interview.tm_login_log values(1001,'2017-01-10');insert into interview.tm_login_log values(1001,'2017-01-12');insert into interview.tm_login_log values(1001,'2017-01-13');insert into interview.tm_login_log values(1001,'2017-01-15');insert into interview.tm_login_log values(1001,'2017-01-16');insert into interview.tm_login_log values(1002,'2017-01-01');insert into interview.tm_login_log values(1002,'2017-01-02');insert into interview.tm_login_log values(1002,'2017-01-03');insert into interview.tm_login_log values(1002,'2017-01-04');insert into interview.tm_login_log values(1002,'2017-01-05');insert into interview.tm_login_log values(1002,'2017-01-06');insert into interview.tm_login_log values(1002,'2017-01-07');insert into interview.tm_login_log values(1002,'2017-01-08');insert into interview.tm_login_log values(1002,'2017-01-09');insert into interview.tm_login_log values(1002,'2017-01-10');insert into interview.tm_login_log values(1002,'2017-01-11');insert into interview.tm_login_log values(1002,'2017-01-12');insert into interview.tm_login_log values(1002,'2017-01-13');insert into interview.tm_login_log values(1002,'2017-01-16');insert into interview.tm_login_log values(1002,'2017-01-17'); 步骤拆解我们首先要思考，怎样才算连续登录呢？就是1号登录，2号也登录了，这样就连续2天登录，那我们怎么知道2号他有没有登录呢？一种思路是根据排序来判断：我们来根据日期来排个名1234567select user_id, login_date, row_number() over(partition by user_id order by login_date) day_rankfrom interview.tm_login_log; 现在，我们根据用户ID，对他的登录日期做了排序，但是我们还是没有办法知道，他是不是连续的。我们根据这个排序再思考一下，对于一个用户来说，他的登录日期排序已经是连续的了，如果登录日期也是个数字，那我们根据每行的差值，就可以判断登录日期是否连续了。我们换个角度，我们找一个起始日期，来计算一个相差的天数，用它去和排序相对比，就可以了。12345678select user_id, login_date, date_part('day',login_date::timestamp - timestamp'2017-01-01') day_interval, -- 间隔天数 row_number() over(partition by user_id order by login_date) day_rank -- 日期排序from interview.tm_login_log; 我们观察下数据，因为日期排序是连续的，我们统计的间隔天数都是一个起始日期，所以，如果登录日期是连续的，那么，排序-间隔天数的差值也应该是一样的 12345678910111213select user_id, login_date, date_part('day',login_date::timestamp - timestamp'2017-01-01') day_interval, -- 间隔天数 row_number() over(partition by user_id order by login_date) day_rank, -- 日期排序 ( row_number() over(partition by user_id order by login_date) ) - ( date_part('day',login_date::timestamp - timestamp'2017-01-01') ) diff_valuefrom interview.tm_login_log; 差值一样的记录，就是连续登录的日期 好了，连续登录的判断标准，我们已经确定了，下面就是把题目中要的数据查出来即可1234567891011121314151617181920212223select user_id, --diff_value, --差值 min(login_date) start_date, --开始日期 max(login_date) end_date, --结束日期 count(1) running_days --连续登录天数from ( select user_id, login_date, date_part('day',login_date::timestamp - timestamp'2017-01-01') day_interval, -- 间隔天数 row_number() over(partition by user_id order by login_date) day_rank, -- 日期排序 ( row_number() over(partition by user_id order by login_date) ) - ( date_part('day',login_date::timestamp - timestamp'2017-01-01') ) diff_value from interview.tm_login_log) basegroup by user_id,diff_valueorder by user_id,start_date 拓展：获取用户最大的连续登录天数及开始日期和结束日期123456789101112131415161718192021222324252627282930313233with tmp as (select user_id, diff_value, --差值 min(login_date) start_date, --开始日期 max(login_date) end_date, --结束日期 count(1) running_days --连续登录天数from ( select user_id, login_date, date_part('day',login_date::timestamp - timestamp'2017-01-01') day_interval, -- 间隔天数 row_number() over(partition by user_id order by login_date) day_rank, -- 日期排序 ( row_number() over(partition by user_id order by login_date) ) - ( date_part('day',login_date::timestamp - timestamp'2017-01-01') ) diff_value from interview.tm_login_log) basegroup by user_id,diff_value) select a.user_id,a.start_date,a.end_date,a.running_daysfrom tmp ajoin ( select user_id,max(running_days) running_days from tmp group by user_id) b on a.user_id = b.user_idand a.running_days = b.running_days; 连续5天登录用户这里补充另一个类似的问题，这里，我们想看连续登录5天的用户，使用上面的方法可以实现，这里介绍一个更快的方法：是使用一个函数123456789101112向前取n位lag(value anyelement [, offset integer [, default anyelement ]])select *from ( select a.user_id, a.login_date, --5天前的登录日期 lag(a.login_date,4) over(partition by a.user_id order by a.login_date) pre_five_day from interview.tm_login_log a )xwhere date_part('day',x.login_date::timestamp - pre_five_day::timestamp)=4 这样取连续登录的话，比较方便 思考题：连续7天未登录用户这里留一个类似的小问题，大家自行练习下 小结我们简单整理下思路，上面的例子，我认为主要是一个思路的介绍，核心就是我们要找到一个判断连续的方法，找到方法后，SQL自然就一步一步想出来了。上面只是一种思路，一定还有更优的解法，欢迎大家反馈分享。]]></content>
      <categories>
        <category>笔试题</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>笔试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL笔试题-行转列]]></title>
    <url>%2F2017%2F08%2F28%2Fdata-analyst-interview-sql-02%2F</url>
    <content type="text"><![CDATA[SQL笔试题 下面的SQL基于PostgreSQL 1. 行转列背景我们写SQL的时候，经常会遇到一些列转行、行转列的情况，有的时候是为了展现需要，有的时候是代码里就得这样转一下。总之嘞，得掌握这个技巧。下面就开始我们的练习。 测试数据1234567891011121314151617181920CREATE TABLE interview.tm_score( stu_name character varying(20), -- 学生名称 course_name character varying(20), -- 课程名称 score numeric(10,0) -- 分数)WITH ( OIDS=FALSE);-- 初始化数据insert into interview.tm_score values('路飞','数学',100);insert into interview.tm_score values('路飞','语文',62);insert into interview.tm_score values('路飞','英语',98);insert into interview.tm_score values('索隆','数学',40);insert into interview.tm_score values('索隆','语文',57);insert into interview.tm_score values('索隆','英语',40);insert into interview.tm_score values('娜美','数学',42);insert into interview.tm_score values('娜美','语文',44);insert into interview.tm_score values('娜美','英语',28); 我们先来思考第一个问题：我们怎样可以将课程变成列呢，类似交叉表那样？最容易想到的方法，就是使用case when了 case when1234567891011select stu_name, max(case course_name when '数学' then score else 0 end) as "数学", max(case course_name when '语文' then score else 0 end) as "语文", max(case course_name when '英语' then score else 0 end) as "英语" from interview.tm_scoregroup by stu_name; crosstab在新版本的PostgreSQL中有一个extension，可以方便的实现行转列我们需要先安装这个扩展，我看了下，PostgreSQL8.3以后的版本都可以安装1create extension tablefunc 官网地址：https://www.postgresql.org/docs/9.5/static/tablefunc.html 安装完后，会有这么几个函数 我们可以使用crosstab来实现上面的行转列123select *from crosstab( 'select stu_name,course_name,score from interview.tm_score') as ct(stu_name varchar(20) ,"数学" numeric(10,0),"语文" numeric(10,0), "英语" numeric(10,0)) 结果也是一样的，我们传入一个SQL，SQL里面返回3列（这3列都是默认处理的，第一列是主字段，第2列是要拆成列的字段，第3列是要显示的值），最后因为返回值是record，所以我们要定义一下类型。 这里，顺便看看这个函数的用法 要转成列的字段有Null是这样一种情况：不是所有的同学都有3门课程的分数我们删掉了几条记录来模拟 这时候使用crosstab，结果会有问题 数据会自动从第一列开始，导致错误的数据12345-- crosstab(text source_sql, text category_sql)select *from crosstab( 'select stu_name,course_name,score from interview.tm_score','select distinct course_name from interview.tm_score') as ct(stu_name varchar(20) ,"数学" numeric(10,0),"语文" numeric(10,0), "英语" numeric(10,0)) 这时候，数据就对了 source_sql 多于3列这种情况是，我们查询的结果不单单有上面说的3列，可能还有其他字段，比如，我们可以在上面的测试数据上加一个班级列我们同样也是使用上面的方式解决 12345678alter table interview.tm_score add column stu_class varchar(10);update interview.tm_score set stu_class='一班' where stu_name in ('路飞','索隆');update interview.tm_score set stu_class='二班' where stu_name not in ('路飞','索隆');select *from crosstab( 'select stu_name,stu_class,course_name,score from interview.tm_score','select distinct course_name from interview.tm_score') as ct(stu_name varchar(20) ,stu_class varchar(10),"数学" numeric(10,0),"语文" numeric(10,0), "英语" numeric(10,0)) 这里我们六个思考题，有这样一组数据： 我们想查询出这样格式的数据 大家可以练习下这个SQL该怎么写 列转行我们可以把行转成列，那要怎样把列转成行呢？ union all最简单的方法是直接union all,既然要放到一列里面，那字段类型肯定要一样，所以直接根据不同字段去union all 就好了 12345select stu_name,"数学" from xxxunion all select stu_name,"语文" from xxxunion allselect stu_name,"英语" from xxx 小结这里简单介绍了几种常见的处理方法，实际使用中，一定还有更好地方法、或一些特殊的问题，欢迎大家分享、反馈。]]></content>
      <categories>
        <category>笔试题</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>笔试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL笔试题-累计值（月累计、年累计）]]></title>
    <url>%2F2017%2F08%2F28%2Fdata-analyst-interview-sql-01%2F</url>
    <content type="text"><![CDATA[SQL笔试题 下面的SQL基于PostgreSQL 1. 累计值（月累计、年累计）背景描述比如说，我们有这样一份数据,记录的是图书每天的销量情况： 日期 图书名称 销量 2017-01-01 解忧杂货店 90 2017-01-03 解忧杂货店 50 2017-01-05 解忧杂货店 100 2017-01-01 雪落香杉树 100 2017-01-03 雪落香杉树 44 2017-01-04 雪落香杉树 99 现在，我们要统计每本书，当月的累计销量？即1号是1号的销量，2号是1号+2号当天的销量（注意：这里2号当天虽然没有销量，但是应该为1号的90+2号的0，为90）。大家先思考下，如果可以很快解答，就不需要接着读啦，有疑问的同学可以继续往下看。 测试数据123456789101112131415161718192021222324252627282930313233343536373839-- 图书的销量表CREATE TABLE interview.tm_book_sales( calendar_date date, -- 日期 book_name character varying(100), -- 图书名称 sales numeric(10,0) -- 销量)WITH ( OIDS=FALSE);-- 测试数据insert into tm_book_sales values('2017-01-01','解忧杂货店',56);insert into tm_book_sales values('2017-01-02','解忧杂货店',100);insert into tm_book_sales values('2017-01-03','解忧杂货店',70);insert into tm_book_sales values('2017-01-06','解忧杂货店',11);insert into tm_book_sales values('2017-01-07','解忧杂货店',65);insert into tm_book_sales values('2017-01-08','解忧杂货店',9);insert into tm_book_sales values('2017-01-09','解忧杂货店',30);insert into tm_book_sales values('2017-01-10','解忧杂货店',56);insert into tm_book_sales values('2017-01-01','雪落香杉树',18);insert into tm_book_sales values('2017-01-02','雪落香杉树',40);insert into tm_book_sales values('2017-01-03','雪落香杉树',2);insert into tm_book_sales values('2017-01-04','雪落香杉树',22);insert into tm_book_sales values('2017-01-05','雪落香杉树',48);insert into tm_book_sales values('2017-01-07','雪落香杉树',71);insert into tm_book_sales values('2017-01-09','雪落香杉树',73);insert into tm_book_sales values('2017-01-10','雪落香杉树',37);insert into tm_book_sales values('2017-02-03','解忧杂货店',5);insert into tm_book_sales values('2017-02-05','解忧杂货店',46);insert into tm_book_sales values('2017-02-06','解忧杂货店',35);insert into tm_book_sales values('2017-02-07','解忧杂货店',10);insert into tm_book_sales values('2017-02-09','解忧杂货店',30);insert into tm_book_sales values('2017-02-10','解忧杂货店',12);insert into tm_book_sales values('2017-02-13','解忧杂货店',43);insert into tm_book_sales values('2017-02-01','雪落香杉树',10);insert into tm_book_sales values('2017-02-04','雪落香杉树',78);insert into tm_book_sales values('2017-02-10','雪落香杉树',50);insert into tm_book_sales values('2017-02-20','雪落香杉树',93); 现在呢，我们有了图书每天的销量数据，下面，我们思考1个问题：我想要统计每本图书的当月累计销量，应该怎么做呢？ 如果只是单纯的统计每本书每个月的销量，熟悉SQL的同学，一定可以很快想到12345678910111213select to_char(calendar_date,'yyyy-mm') month_name, book_name, sum(sales) from interview.tm_book_sales group by to_char(calendar_date,'yyyy-mm'), book_nameorder by book_name, to_char(calendar_date,'yyyy-mm'); 下面，我们来想下，这个月累计怎么做？月累计值，其实就是当天的销量再加上当天之前的销量 自关联通过 interview.tm_book_sales 表，我们可以获取每一天的销量，那要怎样获取每天历史的销量呢？最简单的方式就是自关联了。其实就是自己和自己去关联，来获取历史的销量123456789101112131415161718192021222324select t_today.calendar_date, t_today.book_name, sum(t_his.sales) sales_mtdfrom interview.tm_book_sales t_todayleft join interview.tm_book_sales t_hison -- 图书名称相等 t_his.book_name = t_today.book_name and -- 月份相等，只统计当月 to_char(t_his.calendar_date,'yyyy-mm') = to_char(t_today.calendar_date,'yyyy-mm')and -- 获取当天之前的历史日期 t_his.calendar_date &lt;= t_today.calendar_dategroup by t_today.calendar_date, t_today.book_nameorder by t_today.book_name, t_today.calendar_date; 好了，上面，我们通过自关联，获取了每本图书的月累计销量，不要太高兴，我们观察下，就会发现些问题。我们看看日期，就会发现，有些日期是没有销量的，比如：《解忧杂货店》2017-01-04，2017-01-05 就没有销量，但实际上，如果是累计值得花，他是应该有数据的，因为1号、2号、3号都有数据，就算4号当天没有销量，月累计也应该要算上前3天的销量，所以我们的SQL并不严谨，还得修改。 补全没有销量的日期我们需要想办法补全缺失的日期，如果，t_today里面含有每一天每本书的数据就好了，这就要我们手动构造一个了。123456789101112select t_day.calendar_date, t_book.book_namefrom -- 日期维度表，就是存放每一天 base.dm_calendar t_daycross join -- 所有的图书信息 (select distinct book_name from interview.tm_book_sales) t_bookwhere t_day.month_id=201701; 日期维度表的话，其实是数仓中必备的地基础维度中的一个，她里面就是存放了每一天的数据，和其他一些我们会常用的字段，后面写一篇文章详细介绍下。我们通过笛卡尔积，生成了一张包含每一天每本的图书的一个全维度表。1234567891011121314151617181920212223242526272829303132333435363738394041with t_dim as ( select t_day.calendar_date, t_book.book_name from base.dm_calendar t_day cross join (select distinct book_name from interview.tm_book_sales) t_book where t_day.month_id=201701)select t_dim.calendar_date, t_dim.book_name, sum(t_his.sales) sales_mtdfrom t_dimleft join interview.tm_book_sales t_todayon t_today.calendar_date = t_dim.calendar_dateand t_today.book_name = t_dim.book_nameleft join interview.tm_book_sales t_hison -- 图书名称相等 t_his.book_name = t_dim.book_name and -- 月份相等，只统计当月 to_char(t_his.calendar_date,'yyyy-mm') = to_char(t_dim.calendar_date,'yyyy-mm')and -- 获取当天之前的历史日期 t_his.calendar_date &lt;= t_dim.calendar_dategroup by t_dim.calendar_date, t_dim.book_nameorder by t_dim.book_name, t_dim.calendar_date; 好啦，补全了日期信息后，我们的月累计算是完成了，手工。 总结简单总结下，通过上面的例子，我们要掌握什么呢？首先是对业务的理解，比如上面的月累计的统计方法；然后根据统计方法，使用SQL去实现，一步步完善；还有对日期维度表的一个综合使用。年累计的实现也是一样的，同学们可以自行练习下，有问题可以反馈。]]></content>
      <categories>
        <category>笔试题</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>笔试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[词云图之《战狼2》影评]]></title>
    <url>%2F2017%2F08%2F22%2Fpython-handbook-01%2F</url>
    <content type="text"><![CDATA[下午看了社区里的一篇文章《Python 爬虫实践：《战狼2》豆瓣影评分析》，感谢分享。最近也是在学习爬虫，周末刚好看了词云图，这里就自己也来实现下。 周末的词云图介绍《word_cloud-用Python之作个性化词云图》 豆瓣影评页面分析我们到豆瓣电影模块，选择《战狼2》，找到下面的短评 页面地址：https://movie.douban.com/subject/26363254/comments?status=P 通过FireBug，观察页面，可以发现，评论信息还是很好拿的 然后，我们看看下一页数据是怎么获取的 这里是直接用参数传的，多点几次观察，就会发现规律 这里有个小疑问，他这个参数start，短评每页20条没有问题，但是这个start，并不是0，20,40开始的，会跳跃，不知道为啥，而且，这个limit貌似是假的，我改成100都没用，还是显示20条而且，不登录的话，并不能看完所有的短评，后面会报错，说没有权限。1234567891011121314151617#解析当前页面 def parseCurrentPage(html): soup = BeautifulSoup(html, "html.parser") #获取评论信息 p_comments = soup.select('div#comments div.comment p') p_lines=[] for com in p_comments: p_lines.append(com.contents[0].strip()+'\n') #获取下一页信息，可以通过这个获取数据 p_next = soup.select_one('div#paginator a.next') #print(p_next) print(p_next['href']) return p_lines 生成词云图这里的方法还是和周末的那一篇类似，这里多了一个stopwords的概念，就是剔除了一些没有用的词语，貌似网上可以找到通用的一些，我这里直接根据测试，手动剔除的。 原文是自己使用pandas统计的词频，我这里直接就传给Wordcloud了，后面再试试12345#指定需要提出的词语 stopwords = &#123;u'个',u'一个',u'这个',u'个人',u'不是',u'就是',u'一部',u'这部' ,u'我们',u'所以',u'不会',u'这种',u'没有',u'各种',u'觉得' ,u'真的',u'知道',u'还是',u'但是',u'可以',u'这么',u'因为',u'很多'&#125; print('stopwords',stopwords) 实例代码刚刚看了下导出的评论文件，发现有重复数据，一定是哪里有问题 刚试了下，这个影评的返回结果有毒啊1234https://movie.douban.com/subject/26363254/comments?start=20&amp;limit=20&amp;sort=new_score&amp;status=P和https://movie.douban.com/subject/26363254/comments?start=26&amp;limit=20&amp;sort=new_score&amp;status=P这2个显示的内容居然是一样的。。看来还是得通过每次下一页的href属性去获取下一页地址 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788# -*- coding: utf-8 -*-"""Created on Tue Aug 22 16:13:24 2017@author: yuguiyang"""import osimport urllibfrom bs4 import BeautifulSoupimport jiebafrom wordcloud import WordCloudimport matplotlib.pyplot as pltfile_name = 'douban_movie_zhanlang.txt'#根据URL，获取页面源码def getHtml(url): req = urllib.request.Request(url) page = urllib.request.urlopen(req).read().decode('utf-8') return page#删除文件def clearFile(targetFile): if os.path.exists(targetFile): os.remove(targetFile) #将数据保存到文件def save2file(lines,targetFile): with open(targetFile,'a') as file: file.writelines(lines)#解析当前页面 def parseCurrentPage(html): soup = BeautifulSoup(html, "html.parser") #获取评论信息 p_comments = soup.select('div#comments div.comment p') p_lines=[] for com in p_comments: p_lines.append(com.contents[0].strip()+'\n') #获取下一页信息，可以通过这个获取数据 p_next = soup.select_one('div#paginator a.next') #print(p_next) print(p_next['href']) return p_lines def showWordCloud(targetFile): #指定字体，是为了显示中文 font = r'C:\Windows\Fonts\simsun.ttc' with open(targetFile) as file: comments = file.read() text_cut = jieba.cut(comments , cut_all=False) #指定需要提出的词语 stopwords = &#123;u'个',u'一个',u'这个',u'个人',u'不是',u'就是',u'一部',u'这部' ,u'我们',u'所以',u'不会',u'这种',u'没有',u'各种',u'觉得' ,u'真的',u'知道',u'还是',u'但是',u'可以',u'这么',u'因为',u'很多'&#125; print('stopwords',stopwords) wordcloud = WordCloud(font_path=font,width=800,height=400, background_color='white',stopwords=stopwords ).generate(' '.join(text_cut)) plt.imshow(wordcloud, interpolation='bilinear') plt.axis("off") def main(): url = 'https://movie.douban.com/subject/26363254/comments?start=&#123;0&#125;&amp;limit=20&amp;sort=new_score&amp;status=P' clearFile(file_name) #直接遍历200条评论，这里要注意，超过多少页后，需要登录才可以，这里暂时还没有做，就到200 for i in range(0,200,20): print(url.format(i)) html = getHtml(url.format(i)) movie_comments = parseCurrentPage(html) save2file(movie_comments,file_name) #显示词云图 showWordCloud(file_name)if __name__=='__main__': main()]]></content>
      <categories>
        <category>数据可视化-Python&amp;R</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>可视化</tag>
        <tag>词云图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib手册(10)-用pyplot实现“房间里100个人玩游戏的例子”]]></title>
    <url>%2F2017%2F08%2F18%2Fmatplotlib-handbook-01%2F</url>
    <content type="text"><![CDATA[之前有篇文章，说房间里有100个人，每人100块钱，的那个原文介绍：用数据分析告诉你这个世界很有意思 觉得挺有意思的，昨天发现pyplot也可以绘制动画，就来试试，主要是实现动画效果，其他的暂时先不考虑了目前跑起来是可以，就是比较慢，还没找到原因整体想法，就是x轴表示玩家的序号，财富值用y轴来表示 存款为0后，不在支付给别人，但可以收到别人给的钱1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import numpy as np import matplotlib.pyplot as plt import matplotlib.animation as animation #初始状态，共100人total_people = 100 #每个人的x轴坐标x = np.arange(100)#每人初始100块people = [100]*total_people#局数game_round = 0game_max_round = 17000#初始绘图fig,axes = plt.subplots()axes.bar(x,people,facecolor='green') axes.set_title(u'Round: '+str(game_round))axes.grid(True,axis='y')#根据下标i,随机返回另一个下标def give_to(i): i_to = i while i == i_to: i_to = np.random.randint(0,100) #print('from',i,'to',i_to) return i_to#重新绘制图形#1.当拥有的钱为0，则不再支出，但可以收入def game(obj): global people global game_round #还不知道咋让循环停止，就在这判断下 if game_round &lt; game_max_round: #清空当前轴 plt.cla() #遍历100个人 for i in range(total_people): #判断，当前人是否有钱 if people[i] &gt; 0 : #每个人拿出1块钱，给另一个人 people[i] = people[i] - 1 people_to = give_to(i) people[people_to] = people[people_to] + 1 #print(people) else : pass game_round += 1 #重新绘图 axes.set_title(u'Round: '+str(game_round)) axes.bar(x,sorted(people),facecolor='green') axes.grid(True,axis='y') else : pass#循环调用游戏ani = animation.FuncAnimation(fig, game, interval=0.1) plt.show() 不知道是不是代码有问题，最后有1个人那么高代码跑的很慢，随机那块不知道有没有问题，等再研究下看看优化下 可以贷款，即存款为负数12345678910111213141516171819202122232425#2.允许借贷的情况，及拥有的钱可以为负def game2(obj): global people global game_round #还不知道咋让循环停止，就在这判断下 if game_round &lt; game_max_round: #清空当前轴 plt.cla() #遍历100个人 for i in range(total_people): #每个人拿出1块钱，给另一个人 people[i] = people[i] - 1 people_to = give_to(i) people[people_to] = people[people_to] + 1 game_round += 1 #重新绘图 axes.set_title(u'Round: '+str(game_round)) axes.bar(x,sorted(people),facecolor='green') axes.grid(True,axis='y') else : pass 35岁破产后，人生的走势这里就以第6500轮游戏为基准， # -*- coding: utf-8 -*- """ Created on Fri Aug 18 11:08:44 2017 @author: yuguiyang """ import numpy as np import matplotlib.pyplot as plt import matplotlib.animation as animation #定义一个people类 class People: __data = 100 __color = 'green' def __init__(self,data=100,color='green'): self.__data = data self.__color = color def __str__(self): return str(self.__data)+','+self.__color def set_data(self,data): self.__data = data def set_color(self,color): self.__color = color def get_data(self): return self.__data def get_color(self): return self.__color def give_money(self,money=1): self.__data = self.__data - money def rcv_money(self,money=1): self.__data = self.__data + money #对数据进行排序后返回数据 def parse_people_data(): global peoples people_data = [] people_color = [] for p in sorted(peoples, key=lambda p: p.get_data()): people_data.append(p.get_data()) people_color.append(p.get_color()) return people_data,people_color #################################################### #初始状态，共100人 total_people = 100 #每个人的x轴坐标 x = np.arange(total_people) #每人初始100块,颜色是绿色 peoples = [] for i in range(total_people): peoples.append(People()) #局数 game_round = 0 game_max_round = 17000 #初始绘图 fig,axes = plt.subplots() axes.bar(x,parse_people_data()[0],color=parse_people_data()[1]) axes.set_title(u'Round: '+str(game_round)) axes.grid(True,axis='y') #根据下标i,随机返回另一个下标 def give_to(i): i_to = i while i == i_to: i_to = np.random.randint(0,100) return i_to #重新绘制图形 #1.当拥有的钱为0，则不再支出，但可以收入 #参考plt_flash_demo2 #2.允许借贷的情况，及拥有的钱可以为负 def game2(obj): global peoples global game_round #还不知道咋让循环停止，就在这判断下 if game_round &lt; game_max_round: #清空当前轴 plt.cla() #遍历100个人 for i in range(total_people): #每个人拿出1块钱，给另一个人 peoples[i].give_money() people_to = give_to(i) peoples[people_to].rcv_money() #在第6500次游戏，修改负债者的颜色 if game_round == 6500: for p in peoples : if p.get_data()&lt;0: p.set_color('red') else : pass game_round += 1 #重新绘图 axes.set_title(u'Round: '+str(game_round)) axes.bar(x,parse_people_data()[0],color=parse_people_data()[1]) axes.grid(True,axis='y') else : pass #循环调用游戏 ani = animation.FuncAnimation(fig, game2, interval=1) plt.show() 6500忘记截图了， 挺好玩儿的，还是有2个人逆袭的这里，定义了一个People类来绑定财富值和颜色，感觉可以直接用pandas DataFrame就可以解决，一会儿找时间改下看看后续再补充下其他情况]]></content>
      <categories>
        <category>数据可视化-Python&amp;R</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib手册(9) - 绘制动画]]></title>
    <url>%2F2017%2F08%2F17%2Fmatplotlib-base-flash-09%2F</url>
    <content type="text"><![CDATA[matplotlib手册(9) 绘制动画 前面，我们介绍了很多绘图的方法，matplotlib不单单可以绘制静态的图，还可以制作动态的图，下面，我们就来学习下。 我们主要参考matplotlib官网的例子http://matplotlib.org/api/animation_api.html 创建动画最简单的方式，就是使用Animation的子类,就是下面的这2个 1. FuncAnimation函数介绍及主要参数1234567891011class matplotlib.animation.FuncAnimation(fig, func, frames=None, init_func=None, fargs=None, save_count=None, **kwargs)fig : matplotlib.figure.Figurefunc : callableframes : iterable, int, generator function, or None, optionalinit_func : callable, optionalfargs : tuple or None, optionalinterval : number, optionalrepeat_delay : number, optionalrepeat : bool, optional 小栗子1234567891011121314151617181920212223242526272829# -*- coding: utf-8 -*-"""Created on Thu Aug 17 18:19:08 2017@author: yuguiyang"""import numpy as np import matplotlib.pyplot as plt import matplotlib.animation as animation from datetime import datetime fig,axes = plt.subplots() axes.plot(np.random.rand(10)) #重新绘制图形def update_line(data): print(datetime.now(),'--',data) #清空当前轴 plt.cla() #重新绘图 axes.plot(np.random.rand(10))#传入的fig中，调用update_line函数，将range(3)作为参数传给update_line，1秒调用一次ani = animation.FuncAnimation(fig, update_line, 3, interval=1000) plt.show() 上面的代码，我们定义了一个函数update_line，他会清空axes，并重新绘图；FuncAnimation会每个1秒调用一次这个函数 这里记录一个小问题，暂时还没有解决 frames 参数的问题上面的例子里，我们给的是一个常量3，按照官方的介绍，是按照range(3),来一次传给函数的，但实际测试下来，发现他的调用会有问题。我们看下上面的那个输出 刚刚发现了导致这个问题的原因，注意看这个：123init_func : callable, optional A function used to draw a clear frame. If not given, the results of drawing from the first item in the frames sequence will be used. This function will be called once before the first frame. 上面，因为我们没有指定初始化函数，所以导致，会调用一次update_line，用它返回值作为初始状态，我们改下脚本再看123456def init(): print('init') #清空当前轴 plt.cla() ani = animation.FuncAnimation(fig, update_line, 3, repeat=False, interval=3000,init_func=init) 这回输出就正常了， repeat、repeat_delay这2个参数一般会配合使用，repeat默认是true，所以上面的例子会一直循环下去，如果我们改为false,第一次循环完之后就会停止。 2. ArtistAnimation12345class matplotlib.animation.ArtistAnimation(fig, artists, *args, **kwargs)artists : list Each list entry a collection of artists that represent what needs to be enabled on each frame. These will be disabled for other frames. 使用起来和上面的差不多，这里不会调用函数，而是传入一个list，123456789101112131415import numpy as np import matplotlib.pyplot as plt import matplotlib.animation as animation fig,axes = plt.subplots() ims= []for i in range(5): ims.append(axes.plot(np.random.rand(10)))im_ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=3000, blit=True)print(ims)plt.show()]]></content>
      <categories>
        <category>数据可视化-Python&amp;R</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>数据可视化</tag>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas手册（12）- 时间序列]]></title>
    <url>%2F2017%2F08%2F17%2Fpandas-handbook-12%2F</url>
    <content type="text"><![CDATA[PythonPandas 在数据分析中，时间序列应该很常见，这里，我们看看在pandas里面的使用 1. 日期和时间数据类型经常使用的datetime，time，及calendar模块123456789101112131415from datetime import datetimenow = datetime.now()nowOut[33]: datetime.datetime(2017, 8, 18, 9, 43, 46, 360886)now.yearOut[34]: 2017now.monthOut[35]: 8now.dayOut[36]: 18 datetime.timedelta表示2个datetime对象的时间差12345678910a = datetime(2018,8,18) - datetime(2018,8,17)aOut[38]: datetime.timedelta(1)a.daysOut[39]: 1a.secondsOut[40]: 0 12class datetime.timedelta(days=0, seconds=0, microseconds=0, milliseconds=0, minutes=0, hours=0, weeks=0)A timedelta object represents a duration, the difference between two dates or times. 我们可以给datetime加上或者减去一个或多个timedelta123456789from datetime import timedeltastart = datetime(2017,8,18)start + timedelta(days=-12)Out[44]: datetime.datetime(2017, 8, 6, 0, 0)start + timedelta(seconds=600)Out[45]: datetime.datetime(2017, 8, 18, 0, 10) 2. 日期转换1pandas.to_datetime(arg, errors='raise', dayfirst=False, yearfirst=False, utc=None, box=True, format=None, exact=True, unit=None, infer_datetime_format=False, origin='unix') dayfirst参数1234567#比如有时候，我们的格式，可能是day在前面，默认的话，会转换为 月/日/年pd.to_datetime(['06/08/2017','07/08/2017'])Out[29]: DatetimeIndex(['2017-06-08', '2017-07-08'], dtype='datetime64[ns]', freq=None)#使用dayfirst，指定日期，日/月/年pd.to_datetime(['06/08/2017','07/08/2017'],dayfirst=True)Out[30]: DatetimeIndex(['2017-08-06', '2017-08-07'], dtype='datetime64[ns]', freq=None) 我们可以将一个包含日期信息的DataFrame封装成datetime123456789101112131415df = pd.DataFrame(&#123;'year': [2015, 2016], 'month': [2, 3], 'day': [4, 5]&#125;)dfOut[32]: day month year0 4 2 20151 5 3 2016pd.to_datetime(df)Out[33]: 0 2015-02-041 2016-03-05dtype: datetime64[ns] 这样转换的时候，是一定需要，最少满足年月日3列，不然会报错 ValueError: to assemble mappings requires at least that [year, month, day] be specified: [day,month] is missing errors参数12345errors : &#123;‘ignore’, ‘raise’, ‘coerce’&#125;, default ‘raise’ If ‘raise’, then invalid parsing will raise an exception If ‘coerce’, then invalid parsing will be set as NaT If ‘ignore’, then invalid parsing will return the input 如果一个日期不符合规范，默认是会报错的，我们可以通过errors参数来控制12345678pd.to_datetime('13/13/2017')ValueError: month must be in 1..12pd.to_datetime('13/13/2017',errors='ignore')Out[38]: '13/13/2017'pd.to_datetime('13/13/2017',errors='coerce')Out[39]: NaT 这个NaT，就是Not a Time，在时间序列里面表示NA值123456789101112131415161718192021222324252627282930313233pd.to_datetime(1)Out[21]: Timestamp('1970-01-01 00:00:00.000000001')pd.to_datetime(10)Out[22]: Timestamp('1970-01-01 00:00:00.000000010')s1 = pd.Series(['2017-01-01','2017-02-01','2017-03-01']*3)s1Out[24]: 0 2017-01-011 2017-02-012 2017-03-013 2017-01-014 2017-02-015 2017-03-016 2017-01-017 2017-02-018 2017-03-01dtype: objectpd.to_datetime(s1)Out[25]: 0 2017-01-011 2017-02-012 2017-03-013 2017-01-014 2017-02-015 2017-03-016 2017-01-017 2017-02-018 2017-03-01dtype: datetime64[ns] 3. 时间序列基础最基本的时间序列类型，就是以时间戳为索引的Series1234567891011121314s = pd.Series(np.random.randn(3),index=[datetime(2018,8,1),datetime(2018,8,2),datetime(2018,8,3)])sOut[52]: 2018-08-01 0.3601372018-08-02 1.4225332018-08-03 -1.079172dtype: float64s.indexOut[53]: DatetimeIndex(['2018-08-01', '2018-08-02', '2018-08-03'], dtype='datetime64[ns]', freq=None)type(s)Out[54]: pandas.core.series.Series 不同索引的时间序列之间的算术运算会按时间自动对齐12345678910111213141516171819s+s[1:]Out[56]: 2018-08-01 NaN2018-08-02 2.8450662018-08-03 -2.158345dtype: float64sOut[57]: 2018-08-01 0.3601372018-08-02 1.4225332018-08-03 -1.079172dtype: float64s[1:]Out[58]: 2018-08-02 1.4225332018-08-03 -1.079172dtype: float64 4. 索引、选取、子集构造对于时间序列的处理，有很方便的方式123456789101112131415161718192021222324252627longer_ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))longer_ts.head()Out[76]: 2000-01-01 -0.6176522000-01-02 1.3312102000-01-03 0.3510122000-01-04 -1.6861232000-01-05 -0.426614Freq: D, dtype: float64longer_ts['2000-01-01':'2000-01-04']Out[77]: 2000-01-01 -0.6176522000-01-02 1.3312102000-01-03 0.3510122000-01-04 -1.686123Freq: D, dtype: float64longer_ts['01/01/2000':'01/04/2000']Out[78]: 2000-01-01 -0.6176522000-01-02 1.3312102000-01-03 0.3510122000-01-04 -1.686123Freq: D, dtype: float64 对于DataFrame来说，也是一样的1234pandas.date_range(start=None, end=None, periods=None, freq='D', tz=None, normalize=False, name=None, closed=None, **kwargs)Return a fixed frequency datetime index, with day (calendar) as the default frequencyfreq参数可以参考网址：http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases 1234567891011121314151617181920212223#从2017-01-01开始，每个周三，生成100个dates = pd.date_range('1/1/2017', periods=100, freq='W-WED')long_df = pd.DataFrame(np.random.randn(100, 4), index=dates, columns=['Colorado', 'Texas', 'New York', 'Ohio'])long_df.head()Out[92]: Colorado Texas New York Ohio2017-01-04 -1.061954 0.324256 1.532633 -0.3105992017-01-11 1.211990 -0.241169 -0.488018 -0.4730852017-01-18 0.112726 -0.021406 -0.903449 -1.1918922017-01-25 0.247537 -0.086301 1.252001 1.1196872017-02-01 -0.058160 -0.856956 -0.783473 0.081420#选取1月份所有数据long_df.loc['2017-01']Out[94]: Colorado Texas New York Ohio2017-01-04 -1.061954 0.324256 1.532633 -0.3105992017-01-11 1.211990 -0.241169 -0.488018 -0.4730852017-01-18 0.112726 -0.021406 -0.903449 -1.1918922017-01-25 0.247537 -0.086301 1.252001 1.119687]]></content>
      <categories>
        <category>Python-Pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas手册（11）- groupby]]></title>
    <url>%2F2017%2F08%2F16%2Fpandas-handbook-11%2F</url>
    <content type="text"><![CDATA[PythonPandas 这里，我们整理下pandas中关于groupby的使用，和SQL中一样，就是对数据进行聚合可以参考官方：http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.htmlhttp://pandas.pydata.org/pandas-docs/stable/groupby.html 1. groupby基本使用12DataFrame.groupby(by=None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=False, **kwargs)Group series using mapper (dict or key function, apply given function to group, return result as series) or by a series of columns. 12345678910111213141516import pandas as pdimport numpy as npdf = pd.DataFrame(&#123;'key1' : ['a', 'a', 'b', 'b', 'a'], 'key2' : ['one', 'two', 'one', 'two', 'one'], 'data1' : np.random.randint(0,10,5), 'data2' : np.random.randint(0,10,5)&#125;)dfOut[158]: data1 data2 key1 key20 2 4 a one1 3 8 a two2 6 5 b one3 7 3 b two4 7 6 a one 我们现在，根据key1来groupby1234a = df.groupby(by=['key1'])aOut[170]: &lt;pandas.core.groupby.DataFrameGroupBy object at 0x000000000B8317F0&gt; 我们可以看到，返回值是一个DataFrameGroupBy对象，这只是一个中间数据，还没有进行真正的聚合这里有一个概念”split-apply-combine”，拆分-应用-合并，感觉和MapReduce的概念差不多，这个的groupby就是做了拆分我们可以遍历DataFrameGroupBy，12345678910111213for k,v in a: print('k:',k) print('v:',v) k: av: data1 data2 key1 key20 2 4 a one1 3 8 a two4 7 6 a onek: bv: data1 data2 key1 key22 6 5 b one3 7 3 b two 这个就是将内容进行了拆分,当我们在调用统计函数时，才会执行应用和合并12345678910111213141516171819202122232425262728293031323334a.sum()Out[172]: data1 data2key1 a 12 18b 13 8a.size()Out[173]: key1a 3b 2dtype: int64a.count()Out[174]: data1 data2 key2key1 a 3 3 3b 2 2 2a.max()Out[175]: data1 data2 key2key1 a 7 8 twob 7 5 twoa.mean()Out[176]: data1 data2key1 a 4.0 6.0b 6.5 4.0 我们可以按2个值进行聚合1234567891011121314151617dfOut[188]: data1 data2 key1 key20 2 4 a one1 3 8 a two2 6 5 b one3 7 3 b two4 7 6 a onedf.groupby(by=['key1','key2']).sum()Out[189]: data1 data2key1 key2 a one 9 10 two 3 8b one 6 5 two 7 3 默认的话，会将数值类型的字段做聚合，我们也可以选择1234567891011121314151617df.groupby(by=['key1','key2'])['data1'].sum()Out[190]: key1 key2a one 9 two 3b one 6 two 7Name: data1, dtype: int32df.groupby(by=['key1','key2'])['data1','data2'].sum()Out[191]: data1 data2key1 key2 a one 9 10 two 3 8b one 6 5 two 7 3 下面的写法也是同样的，前面我们是直接传入的列名，这里我们传入series也可以123456789101112131415df.groupby(by=df['key1']).sum()Out[197]: data1 data2key1 a 12 18b 13 8df.groupby(by=[df['key1'],df['key2']]).sum()Out[198]: data1 data2key1 key2 a one 9 10 two 3 8b one 6 5 two 7 3 上面我们传入的都是当前df的序列，这里也可以传入新的，这里只要长度符合就行了，感觉就是把它当成新列来处理12345df.groupby(by=['lufei','lufei','lufei','lufei','namei']).sum()Out[199]: data1 data2lufei 18 20namei 7 6 123by : mapping, function, str, or iterable Used to determine the groups for the groupby. If by is a function, it’s called on each value of the object’s index. If a dict or Series is passed, the Series or dict VALUES will be used to determine the groups (the Series’ values are first aligned; see .align() method). If an ndarray is passed, the values are used as-is determine the groups. A str or list of strs may be passed to group by the columns in self 这个by参数，还可以接收一个dict，像这样：1234567891011121314151617181920212223242526dfOut[204]: data1 data2 key1 key20 2 4 a one1 3 8 a two2 6 5 b one3 7 3 b two4 7 6 a onedf.indexOut[205]: RangeIndex(start=0, stop=5, step=1)dfOut[206]: data1 data2 key1 key20 2 4 a one1 3 8 a two2 6 5 b one3 7 3 b two4 7 6 a one#默认是根据啊axis=0，所以groupby之前会先将index和dict进行映射，df.groupby(&#123;0:'a',1:'a',2:'a',3:'a',4:'a'&#125;).sum()Out[207]: data1 data2a 25 26 这里，对于series也是一样的，series也有index，更厉害的是，这里还可以使用函数进行分组，函数会在各个索引值上调用一次，然后根据返回值来用作分组名称123456789101112131415161718dfOut[216]: data1 data2 key1 key20 2 4 a one1 3 8 a two2 6 5 b one3 7 3 b two4 7 6 a one#会把每一个index的值加10，然后再聚合df.groupby(lambda x:x+10).sum()Out[217]: data1 data210 2 411 3 812 6 513 7 314 7 6 —————update at 2017-08-23这里继续整理下pandas中groupby的使用 2. 面向列的多函数应用 上面，我们再对列做聚合的时候，都是使用使用统一的函数，比如sum(),count(),都是一起的，在pandas中，我们可以同时调用多个函数，主要是使用agg12345678910111213DataFrameGroupBy.agg(arg, *args, **kwargs)Aggregate using callable, string, dict, or list of string/callablesfunc : callable, string, dictionary, or list of string/callables Function to use for aggregating the data. If a function, must either work when passed a DataFrame or when passed to DataFrame.apply. For a DataFrame, can pass a dict, if the keys are DataFrame column names. Accepted Combinations are: string function name function list of functions dict of column names -&gt; functions (or list of functions) 小栗子12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152df = pd.DataFrame(&#123;'A': [1, 1, 2, 2], 'B': [1, 2, 3, 4], 'C': np.random.randn(4)&#125;)dfOut[64]: A B C0 1 1 0.4330761 1 2 0.5097642 2 3 -1.0913183 2 4 -0.696079df.groupby(by=['A']).min()Out[65]: B CA 1 1 0.4330762 3 -1.091318#使用agg，调用min函数，和直接调用时等价的df.groupby(by=['A']).agg('min')Out[66]: B CA 1 1 0.4330762 3 -1.091318#还可以传入一个函数数组，同时调用min和maxdf.groupby(by=['A']).agg(['min','max'])Out[67]: B C min max min maxA 1 1 2 0.433076 0.5097642 3 4 -1.091318 -0.696079df.groupby(by=['A'])['B'].agg(['min','max'])Out[68]: min maxA 1 1 22 3 4#还可以通过传入一个dict，来对不同的列做不同的操作，列名为key，func为valuedf.groupby(by=['A']).agg(&#123;'B':['min','max'],'C':['sum']&#125;)Out[69]: B C min max sumA 1 1 2 0.9428402 3 4 -1.787397 上面，我们传入函数，默认会用我们的函数名来做列名，但，有时我们想要自定义，我们通过传入一个（name,function）的列表12345678910111213141516df.groupby(by=['A']).agg([('the_min_data','min'),('the_max_data','max')])Out[73]: B C the_min_data the_max_data the_min_data the_max_dataA 1 1 2 0.433076 0.5097642 3 4 -1.091318 -0.696079#可以随意组合df.groupby(by=['A']).agg(&#123;'B':['min','max'],'C':[('hey_sum','sum')]&#125;)Out[74]: B C min max hey_sumA 1 1 2 0.9428402 3 4 -1.787397 3. 已无索引形式返回聚合数据前面，我们groupby之后，都是用聚合建来当做index，我们可以通过参数as_index=False,来取消123456789101112131415161718#会默认生成一个新indexdf.groupby(by=['A','B'],as_index=False).max()Out[80]: A B C0 1 1 0.4330761 1 2 0.5097642 2 3 -1.0913183 2 4 -0.696079df.groupby(by=['A','B'],as_index=True).max()Out[81]: CA B 1 1 0.433076 2 0.5097642 3 -1.091318 4 -0.696079 做了练习之后，这里发现，直接调用函数是好用的，但是，如果使用agg来调用，是不好用的这个参数123456789df.groupby(by=['A','B'],as_index=False).agg(['min','max'])Out[79]: C min maxA B 1 1 0.433076 0.433076 2 0.509764 0.5097642 3 -1.091318 -1.091318 4 -0.696079 -0.696079]]></content>
      <categories>
        <category>Python-Pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas手册（10）- 数据转换]]></title>
    <url>%2F2017%2F08%2F15%2Fpandas-handbook-10%2F</url>
    <content type="text"><![CDATA[PythonPandas 这里接着上一篇，继续记录下pandas中数据处理方面的函数 1. 重复数据结果集中，可能会有重复数据，有函数可以做去重操作 1234567#判断数据是否重复DataFrame.duplicated(subset=None, keep='first')Return boolean Series denoting duplicate rows, optionally only considering certain columns#删除重复记录DataFrame.drop_duplicates(subset=None, keep='first', inplace=False)Return DataFrame with duplicate rows removed, optionally only considering certain columns 1234567891011121314151617181920212223242526272829303132333435363738394041424344data = pd.DataFrame(&#123;'K1':['one']*3+['two']*4,'k2':[1,1,2,3,3,4,4]&#125;)dataOut[100]: K1 k20 one 11 one 12 one 23 two 34 two 35 two 46 two 4#keep参数，默认是first，会以第一个为准，后面再出现就是Truedata.duplicated()Out[103]: 0 False1 True2 False3 False4 True5 False6 Truedtype: booldata.duplicated(keep='last')Out[116]: 0 True1 False2 False3 True4 False5 True6 Falsedtype: bool#这里drop也是一样的，可以用keep控制保留第一次出现的，还是最后1次出现的data.drop_duplicates()Out[104]: K1 k20 one 12 one 23 two 35 two 4 这2个函数，默认都是判断全部的列，也可以指定列12345678910111213141516171819202122232425262728293031323334353637383940data['v1']=range(7)dataOut[118]: K1 k2 v10 one 1 01 one 1 12 one 2 23 two 3 34 two 3 45 two 4 56 two 4 6data.duplicated()Out[119]: 0 False1 False2 False3 False4 False5 False6 Falsedtype: booldata.duplicated(subset=['K1'])Out[120]: 0 False1 True2 True3 False4 True5 True6 Truedtype: booldata.drop_duplicates(subset=['K1'])Out[121]: K1 k2 v10 one 1 03 two 3 3 2. 利用函数或映射进行数据转换有的时候，我们想要对Series和dataframe中的每个元素做些转换，就用到了这个函数，和Python内置的map函数类似123456Series.map(arg, na_action=None)Map values of Series using input correspondence (which can be a dict, Series, or function)arg : function, dict, or SeriesDataFrame.applymap(func)Apply a function to a DataFrame that is intended to operate elementwise, i.e. like doing map(func, series) for each series in the DataFrame 12345678910111213141516171819202122232425262728293031323334353637383940414243s1 = pd.Series(range(10))s1Out[133]: 0 01 12 23 34 45 56 67 78 89 9dtype: int32s1.map(lambda x:x+10)Out[134]: 0 101 112 123 134 145 156 167 178 189 19dtype: int64s1.map(lambda x:x*-1)Out[135]: 0 01 -12 -23 -34 -45 -56 -67 -78 -89 -9dtype: int64 这个map，不单单可以是函数，还可以是 Series或dict123456789101112131415161718192021222324252627282930313233x = pd.Series([1,2,3], index=['one', 'two', 'three'])y = pd.Series(['foo', 'bar', 'baz'], index=[1,2,3])xOut[142]: one 1two 2three 3dtype: int64yOut[143]: 1 foo2 bar3 bazdtype: objectx.map(y)Out[144]: one footwo barthree bazdtype: objectz = &#123;1: 'A', 2: 'B', 3: 'C'&#125;x.map(z)Out[146]: one Atwo Bthree Cdtype: object 3.替换值在字符串中，我们经常会用到replace函数，用来替换指定的值，pandas中也有123456DataFrame.replace(to_replace=None, value=None, inplace=False, limit=None, regex=False, method='pad', axis=None)Replace values given in ‘to_replace’ with ‘value’.to_replace : str, regex, list, dict, Series, numeric, or NoneSeries.replace(to_replace=None, value=None, inplace=False, limit=None, regex=False, method='pad', axis=None)Replace values given in ‘to_replace’ with ‘value’. 12345678910111213141516171819202122232425262728293031323334353637383940414243data = pd.Series([1., -999., 2., -999., -1000., 3.])dataOut[150]: 0 1.01 -999.02 2.03 -999.04 -1000.05 3.0dtype: float64#我们将-999这个异常值替换掉data.replace(-999,np.nan)Out[151]: 0 1.01 NaN2 2.03 NaN4 -1000.05 3.0dtype: float64#同时替换多个值，也可以data.replace([-999,-1000],np.nan)Out[152]: 0 1.01 NaN2 2.03 NaN4 NaN5 3.0dtype: float64data.replace([-999,-1000],[np.nan,0])Out[153]: 0 1.01 NaN2 2.03 NaN4 0.05 3.0dtype: float64]]></content>
      <categories>
        <category>Python-Pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最新行政区信息获取]]></title>
    <url>%2F2017%2F08%2F14%2Fthe-newest-area-info-01%2F</url>
    <content type="text"><![CDATA[这是之前记录的，这里顺便分享下。 之前想要获取官方的省市区的代码code，就找了下。 官方地址：http://www.stats.gov.cn/tjsj/tjbz/xzqhdm/ 我们可以直接将数据复制到Excel中，简单处理下，导入到数据库中 里面会有换行，先去个重，然后trim一下，再分列就可以了 最后，我们可以将数据组织成我们想要的维度表格式 ————–update at 2017-08-15 上面呢，我是直接在Excel里面生成insert脚本插入数据库的，但是目前的表使用起来应该不是很方便，这里我们改造下。 这个区域信息呢，是很常用的一张维度表，通常他可能会是这个样子的123456789101112131415CREATE TABLE base.dm_area( area_id integer, area_name character varying(50), province_id integer, province_name character varying(50), city_id integer, city_name character varying(50), country_id integer, country_name character varying(50), flevel integer)WITH ( OIDS=FALSE); 这里我们就简单处理下，变成这种格式。 其实一开始的时候，我们可以通过数据的格式区分他是省份、城市、还是区县，下面我们用的是一个套路 主要是观察法，因为这个code都是有规律的，所以我们处理起来方便很多 code一共是6位，前两位是省份、中间2位是城市，后2位是区县1select *from public.b_area where area_id||'' like '00' 下面，我们来初始化数据12345678910111213141516171819202122232425262728293031323334353637INSERT INTO base.dm_area( area_id, area_name)select *from public.b_area; -- 初始化flevel 1:省份,2:城市,3:区县update base.dm_area set flevel=1 where area_id||'' like '00';update base.dm_area set flevel=2 where area_id||'' like '' and flevel is null;update base.dm_area set flevel=3 where flevel is null;-- 更新省份信息update base.dm_area a set province_id = b.area_id, province_name = b.area_namefrom base.dm_area bwhere left(a.area_id||'',2)=left(b.area_id||'',2) and b.flevel=1;-- 更新城市信息update base.dm_area a set city_id = c.area_id, city_name = c.area_name from base.dm_area c where left(a.area_id||'',4)=left(c.area_id||'',4) and c.flevel=2;-- 更新区县信息update base.dm_area a set country_id = area_id, country_name = area_name where a.flevel=3; 到这里，flevel=3的数据已经没有问题了，再对省份、城市做些优化就行了12345678910111213-- 补全数据update base.dm_area set city_id=-area_id,city_name=area_name||'XT', country_id=-area_id,country_name=area_name||'XT'where flevel=1;update base.dm_area set country_id=-area_id,country_name=area_name||'XT'where flevel=2; 好了，我们就处理好这个区域维度了，正常使用的话，估计就用flevel=3的就够了]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas手册（9）- 数据合并与连接]]></title>
    <url>%2F2017%2F08%2F14%2Fpandas-handbook-09%2F</url>
    <content type="text"><![CDATA[PythonPandas 这里来看一下，pandas中数据转换与合并的使用方法，刚刚学习了一下，很好用，就跟SQL里面一样。 #1. 合并数据集就是说，我们有2个数据集，想要将他们合并一下，就是SQL里面的关联查询，pandas里面用一个函数就行了12345DataFrame.merge(right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=True, indicator=False)Merge DataFrame objects by performing a database-style join operation by columns or indexes.If joining columns on columns, the DataFrame indexes will be ignored. Otherwise if joining indexes on indexes or indexes on a column or columns, the index will be passed on. 熟练掌握几个参数就足够了，下面会依次介绍下小例子123456789101112131415161718192021222324import pandas as pdimport numpy as npa = pd.DataFrame(&#123;'key':list('bbccaa'),'data1':np.random.randint(0,10,size=6)&#125;)b = pd.DataFrame(&#123;'key':list('abc'),'data2':np.random.randint(0,10,size=3)&#125;)aOut[18]: data1 key0 8 b1 0 b2 5 c3 2 c4 3 a5 6 abOut[19]: data2 key0 0 a1 3 b2 2 c 上面是我们的原始数据集，一个a，一个b，key是相同的字段，可以用来关联，123456789a.merge(b)Out[20]: data1 key data20 8 b 31 0 b 32 5 c 23 2 c 24 3 a 05 6 a 0 这个翻译成SQL，就是a join b on a.key=b.key（因为我们没有指定根据什么字段去关联，所以会使用a、b中名字一样的字段去关联）我们当然可以手动指定关联的字段1234567891011on : label or list Field names to join on. Must be found in both DataFrames. If on is None and not merging on indexes, then it merges on the intersection of the columns by default.left_on : label or list, or array-like Field names to join on in left DataFrame. Can be a vector or list of vectors of the length of the DataFrame to use a particular vector as the join key instead of columnsright_on : label or list, or array-like Field names to join on in right DataFrame or vector/list of vectors per left_on docs 如果，数据集中，关联字段名称一样，直接使用on就行了，如果不一样，就可以分别使用left_on 和right_on12345678910111213141516171819a.merge(b,on='key')Out[21]: data1 key data20 8 b 31 0 b 32 5 c 23 2 c 24 3 a 05 6 a 0a.merge(b,left_on='key',right_on='key')Out[22]: data1 key data20 8 b 31 0 b 32 5 c 23 2 c 24 3 a 05 6 a 0 如果关联字段又多个，就指定为数组就行了123456789101112131415161718192021222324252627282930313233343536373839404142c = pd.DataFrame(&#123;'lkey1':list('ab'),'lkey2':list('xy'),'ldata1':np.random.randint(0,10,size=2)&#125;)d = pd.DataFrame(&#123;'rkey1':list('ab'),'rkey2':list('xy'),'ldata1':np.random.randint(0,10,size=2)&#125;)cOut[25]: ldata1 lkey1 lkey20 8 a x1 2 b ydOut[26]: ldata1 rkey1 rkey20 5 a x1 7 b y#因为找不到名字一样的字段做关联，所以是空的结果（有一个字段是一样的，ldata1，但是都一样，刚刚d的名字忘改了......）#正常找不到关联字段，是会报错的c.merge(d)Out[27]: Empty DataFrameColumns: [ldata1, lkey1, lkey2, rkey1, rkey2]Index: []#我们将组合建传入，即可c.merge(d,left_on=['lkey1','lkey2'],right_on=['rkey1','rkey2'])Out[28]: ldata1_x lkey1 lkey2 ldata1_y rkey1 rkey20 8 a x 5 a x1 2 b y 7 b y#看下结果集，我们会发现，c和d中有一个同名的字段ldata1，这里默认加了后缀 _x，_y来表示区分#我们也可以通过参数来指定后缀名称suffixes : 2-length sequence (tuple, list, ...) Suffix to apply to overlapping column names in the left and right side, respectivelyc.merge(d,left_on=['lkey1','lkey2'],right_on=['rkey1','rkey2'],suffixes=['_left','_right'])Out[29]: ldata1_left lkey1 lkey2 ldata1_right rkey1 rkey20 8 a x 5 a x1 2 b y 7 b y 我们再看下一个例子123456789101112131415161718192021222324252627282930313233e = pd.DataFrame(&#123;'key':list('bbccaadd'),'data':np.random.randint(0,10,size=8)&#125;)f = pd.DataFrame(&#123;'key':list('abcx'),'data':np.random.randint(0,10,size=4)&#125;)eOut[32]: data key0 4 b1 5 b2 1 c3 2 c4 7 a5 7 a6 7 d7 9 dfOut[33]: data key0 6 a1 9 b2 2 c3 8 xe.merge(f,on='key')Out[34]: data_x key data_y0 4 b 91 5 b 92 1 c 23 2 c 24 7 a 65 7 a 6 熟悉SQL的同学，会发现，上面的结果集市inner join之后的结果，SQL中，还有什么left join、right join之类的，pandas中也有的123456how : &#123;‘left’, ‘right’, ‘outer’, ‘inner’&#125;, default ‘inner’ left: use only keys from left frame, similar to a SQL left outer join; preserve key order right: use only keys from right frame, similar to a SQL right outer join; preserve key order outer: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically inner: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys 123456789101112131415161718192021222324252627282930313233343536373839#左关联，e为主即包含全部数据，不管是否可以和f关联上e.merge(f,on='key',how='left')Out[35]: data_x key data_y0 4 b 9.01 5 b 9.02 1 c 2.03 2 c 2.04 7 a 6.05 7 a 6.06 7 d NaN7 9 d NaN#右为主，即f为主，e.merge(f,on='key',how='right')Out[36]: data_x key data_y0 4.0 b 91 5.0 b 92 1.0 c 23 2.0 c 24 7.0 a 65 7.0 a 66 NaN x 8#所有数据都包含e.merge(f,on='key',how='outer')Out[37]: data_x key data_y0 4.0 b 9.01 5.0 b 9.02 1.0 c 2.03 2.0 c 2.04 7.0 a 6.05 7.0 a 6.06 7.0 d NaN7 9.0 d NaN8 NaN x 8.0 前面，我们的例子，都是通过columns来关联的，有的时候，我们可能需要使用index来关联，者就用到了另2个参数1234567left_index : boolean, default False Use the index from the left DataFrame as the join key(s). If it is a MultiIndex, the number of keys in the other DataFrame (either the index or a number of columns) must match the number of levelsright_index : boolean, default False Use the index from the right DataFrame as the join key. Same caveats as left_index 12345678910111213141516171819202122232425262728293031eOut[38]: data key0 4 b1 5 b2 1 c3 2 c4 7 a5 7 a6 7 d7 9 dg = pd.DataFrame(&#123;'data':np.random.randint(0,10,size=3)&#125;,index=list('abc'))gOut[40]: dataa 9b 4c 0#指定right_index=True，使用index去关联e.merge(g,left_on='key',right_index=True)Out[41]: data_x key data_y0 4 b 41 5 b 42 1 c 03 2 c 04 7 a 95 7 a 9 2. 轴向连接这里主要是介绍pandas中另一个函数的使用，pd.concat，concat一看上去，感觉是做拼接用的12345pandas.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False, keys=None, levels=None, names=None, verify_integrity=False, copy=True)Concatenate pandas objects along a particular axis with optional set logic along the other axes.Can also add a layer of hierarchical indexing on the concatenation axis, which may be useful if the labels are the same (or overlapping) on the passed axis number. 我们先来看例子1234567891011121314151617181920212223s1 = pd.Series(['a', 'b'])s2 = pd.Series(['c', 'd'])s1Out[51]: 0 a1 bdtype: objects2Out[52]: 0 c1 ddtype: objectpd.concat([s1,s2])Out[53]: 0 a1 b0 c1 ddtype: object 默认，是按纵轴进行拼接的，我们可以设置 123456#横轴进行拼接pd.concat([s1,s2],axis=1)Out[54]: 0 10 a c1 b d 这里要注意下axis=1时，，如果index不一样，拼接的时候，是会合并的，如下面的例子12345678910111213141516171819202122232425262728293031323334353637383940414243444546s1 = pd.Series([0,1],index=['a','b'])s2 = pd.Series([2,3,4],index=['c','d','e'])s3 = pd.Series([5,6],index=['f','g'])pd.concat([s1,s2,s3])Out[58]: a 0b 1c 2d 3e 4f 5g 6dtype: int64s1Out[60]: a 0b 1dtype: int64s2Out[61]: c 2d 3e 4dtype: int64s3Out[62]: f 5g 6dtype: int64pd.concat([s1,s2,s3],axis=1)Out[59]: 0 1 2a 0.0 NaN NaNb 1.0 NaN NaNc NaN 2.0 NaNd NaN 3.0 NaNe NaN 4.0 NaNf NaN NaN 5.0g NaN NaN 6.0 这里，我们看个常用的参数，join，可以选择是取交集还是并集123join : &#123;‘inner’, ‘outer’&#125;, default ‘outer’ How to handle indexes on other axis(es) 12345678910111213141516171819202122232425262728293031323334353637#他们并没哟并集，所以是空pd.concat([s1,s2,s3],axis=1,join='inner')Out[73]: Empty DataFrameColumns: [0, 1, 2]Index: []s4 = pd.concat([s1*5,s3])s1Out[75]: a 0b 1dtype: int64s4Out[76]: a 0b 5f 5g 6dtype: int64pd.concat([s1,s4],axis=1)Out[77]: 0 1a 0.0 0b 1.0 5f NaN 5g NaN 6#只返回了交集部分pd.concat([s1,s4],axis=1,join='inner')Out[78]: 0 1a 0 0b 1 5 我们也可以指明其他轴要使用的索引，要显示的index1234567891011join_axes : list of Index objects Specific indexes to use for the other n - 1 axes instead of performing inner/outer set logicpd.concat([s1,s4],axis=1,join_axes=[['a','b','c','d']])Out[81]: 0 1a 0.0 0.0b 1.0 5.0c NaN NaNd NaN NaN 3. 合并重叠数据这里是另一个函数的使用介绍 combine_first，类似于numpy中where，123DataFrame.combine_first(other)Combine two DataFrame objects and default to non-null values in frame calling the method. Result index columns will be the union of the respective indexes and columns 12345678910111213141516171819202122232425262728293031323334353637383940414243a = pd.Series([np.nan,2.5,np.nan,3.5,4.5,np.nan] ,index=['f','e','d','c','b','a'])b = pd.Series(np.arange(len(a),dtype=np.float64) ,index=['f','e','d','c','b','a'])b[-1]=np.nanaOut[85]: f NaNe 2.5d NaNc 3.5b 4.5a NaNdtype: float64bOut[88]: f 0.0e 1.0d 2.0c 3.0b 4.0a NaNdtype: float64#如果a的字段为nan，则用b的数据np.where(a.isnull(),b,a)Out[92]: array([ 0. , 2.5, 2. , 3.5, 4.5, nan])a.combine_first(b)Out[93]: f 0.0e 2.5d 2.0c 3.5b 4.5a NaNdtype: float64 combine_first还会做数据对齐的操作12345678910111213141516171819202122232425b[:-2]Out[95]: f 0.0e 1.0d 2.0c 3.0dtype: float64a[2:]Out[96]: d NaNc 3.5b 4.5a NaNdtype: float64b[:-2].combine_first(a[2:])Out[94]: a NaNb 4.5c 3.0d 2.0e 1.0f 0.0dtype: float64]]></content>
      <categories>
        <category>Python-Pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas手册（8）- 常见绘图]]></title>
    <url>%2F2017%2F08%2F12%2Fpandas-handbook-08%2F</url>
    <content type="text"><![CDATA[PythonPandas 前面，我们大概了解了matplotlib中基本的绘图方式，现在，我们来看看在pandas中绘图的方式，pandas做好了封装，我们用起来会很方便的。123456789101112131415Series.plot(kind='line', ax=None, figsize=None, use_index=True, title=None, grid=None, legend=False, style=None, logx=False, logy=False, loglog=False, xticks=None, yticks=None, xlim=None, ylim=None, rot=None, fontsize=None, colormap=None, table=False, yerr=None, xerr=None, label=None, secondary_y=False, **kwds)#这个kind可以指定图表类型 ‘line’ : line plot (default) ‘bar’ : vertical bar plot ‘barh’ : horizontal bar plot ‘hist’ : histogram ‘box’ : boxplot ‘kde’ : Kernel Density Estimation plot ‘density’ : same as ‘kde’ ‘area’ : area plot ‘pie’ : pie plotDataFrame.plot(x=None, y=None, kind='line', ax=None, subplots=False, sharex=None, sharey=False, layout=None, figsize=None, use_index=True, title=None, grid=None, legend=True, style=None, logx=False, logy=False, loglog=False, xticks=None, yticks=None, xlim=None, ylim=None, rot=None, fontsize=None, colormap=None, table=False, yerr=None, xerr=None, secondary_y=False, sort_columns=False, **kwds) 1. 线形图12345678import pandas as pdimport numpy as nps = pd.Series(np.random.randint(0,100,size=10))print(s)s.plot(title='demo-series',label='count',legend=True) 123456789import pandas as pdimport numpy as npdf = pd.DataFrame(np.random.randn(10,4)*100,index=np.arange(0,100,10), columns=list('ABCD'))print(df)df.plot() DataFrame绘图的时候，会把每一列单独绘制 2. 柱状图123456789import pandas as pdimport numpy as nps = pd.Series(np.random.randint(0,100,size=10))print(s)s.plot(title='demo-series',label='line',legend=True)s.plot(kind='bar',colormap='Oranges_r',label='bar',legend=True) 我们设置kind=’bar’，就可以画柱状图了 123456789101112import pandas as pdimport numpy as npimport matplotlib.pyplot as pltdf = pd.DataFrame(np.random.randn(10,4)*100,index=np.arange(0,100,10), columns=list('ABCD'))print(df)f,axes = plt.subplots(2,1)df.plot(kind='bar',ax=axes[0])df.plot(kind='barh',ax=axes[1]) 在pandas里画图非常容易，很多都可以是默认转换，index、columns可以自动转换为x轴、y轴标签12345678910111213import pandas as pdimport numpy as npimport matplotlib.pyplot as pltdf = pd.DataFrame(np.random.rand(6,4)*10,index=['one','two','three','four','five','six'], columns=list('ABCD'))print(df)#通过ax参数，可以在不同的subplot上绘图f,axes = plt.subplots(2,1)df.plot(kind='bar',ax=axes[0])df.plot(kind='barh',ax=axes[1]) 在DataFrame中，另一个好用的参数，就是stacked，可以很方便的绘制堆叠图1df.plot(kind='bar',ax=axes[0],stacked=True)]]></content>
      <categories>
        <category>Python-Pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy手册(5)-random模块]]></title>
    <url>%2F2017%2F08%2F10%2Fnumpy-handbook-05%2F</url>
    <content type="text"><![CDATA[PythonNumpy知识总结 numpy的random模块应该很常用，这里整理一下，参考文章：http://www.mamicode.com/info-detail-507676.htmlhttps://docs.scipy.org/doc/numpy/reference/routines.random.html 简单随机数据123456789101112131415161718192021222324252627numpy.random.rand(d0, d1, ..., dn)Random values in a given shape.Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1)生成给定形状的随机值，随机值在[0,1)import numpy as npnp.random.rand(10)Out[2]: array([ 0.41103285, 0.19043225, 0.30385602, 0.19330136, 0.09727556, 0.96518049, 0.29930132, 0.00633969, 0.64269577, 0.79953589])np.random.rand(2,3)Out[3]: array([[ 0.86213038, 0.56657202, 0.83083843], [ 0.48660386, 0.20508572, 0.4927877 ]])np.random.rand(2,3,1)Out[4]: array([[[ 0.06676746], [ 0.55548283], [ 0.04411342]], [[ 0.18659571], [ 0.02209355], [ 0.83529269]]]) 12345678910111213141516171819202122numpy.random.randn(d0, d1, ..., dn)返回指定形状的状态分布样本For random samples from N(\mu, \sigma^2), use:sigma * np.random.randn(...) + munp.random.randn(2,1)Out[6]: array([[-0.29088142], [ 1.29634911]])np.random.randn(2,2)Out[7]: array([[ 0.24125164, 1.62201226], [ 0.10129715, -1.62001598]])Two-by-four array of samples from N(3, 6.25):2.5 * np.random.randn(2, 4) + 3Out[8]: array([[ 5.09295036, 2.2706219 , 3.26392307, 0.86550482], [ 7.59911261, 5.22543816, 2.0441248 , 1.03322082]]) 1234567891011121314numpy.random.randint(low, high=None, size=None, dtype='l')返回随机整数，左闭右开[low,high)np.random.randint(low=1,high=10,size=8)Out[10]: array([4, 2, 6, 7, 2, 4, 3, 8])#high为空的话，直接[0,low)np.random.randint(10,size=5)Out[11]: array([1, 3, 7, 9, 9])np.random.randint(low=1,high=10,size=(2,3))Out[12]: array([[8, 3, 6], [4, 1, 9]]) 12345678numpy.random.random_integers(low, high=None, size=None)返回随机整数，闭区间[low,high]这个和random.randint类似，已经不推荐使用了np.random.random_integers(low=1,high=5,size=5)__main__:1: DeprecationWarning: This function is deprecated. Please call randint(1, 5 + 1) insteadOut[13]: array([3, 2, 2, 4, 5]) 123456789101112131415numpy.random.random_sample(size=None)numpy.random.random(size=None)numpy.random.ranf(size=None)numpy.random.sample(size=None)返回随机的浮点值，左闭右开区间[0.0, 1.0)np.random.random_sample(8)Out[14]: array([ 0.70353035, 0.79018004, 0.50390916, 0.46261548, 0.85556642, 0.68129238, 0.07098945, 0.65927063])np.random.random_sample([2,3])Out[16]: array([[ 0.37546444, 0.50352846, 0.3496647 ], [ 0.02849239, 0.6035842 , 0.32514876]]) 排列123456789101112numpy.random.shuffle(x)就地修改序列的顺序，类似于洗牌，打乱顺序a = np.random.randint(low=1,high=10,size=10)aOut[18]: array([6, 2, 5, 5, 2, 2, 4, 9, 7, 8])np.random.shuffle(a)aOut[20]: array([2, 8, 4, 7, 5, 2, 5, 6, 2, 9]) 123456789numpy.random.permutation(x)返回一个随机排列If x is an integer, randomly permute np.arange(x). If x is an array, make a copy and shuffle the elements randomly.np.random.permutation(10)Out[21]: array([8, 9, 7, 0, 6, 1, 2, 5, 3, 4])np.random.permutation([1, 4, 9, 12, 15])Out[22]: array([ 4, 12, 1, 9, 15])]]></content>
      <categories>
        <category>Python-Numpy</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫小实例学习篇-猫眼电影]]></title>
    <url>%2F2017%2F08%2F09%2Fcrawler-demo-01%2F</url>
    <content type="text"><![CDATA[这里参考了论坛里一位同学分享的博客：猫眼电影TOP100爬取练习，感谢分享。 学习要从模仿开始，学习了上面的博客之后，自己做下练习，正好最近看了selenium，就用了这个。 原作者的正则用的太溜了，等后面有时间再研究下，这里就简单的使用CSS Selector来实现了。 原文代码很精彩，我这个代码就粗糙很多了，先来个初始版，后面再慢慢优化。 大体思路和Python基础（7）- Selenium使用 里面的豆瓣读书例子差不多， 代码（2017-08-09版）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123import csvfrom selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.common.exceptions import TimeoutException#打开浏览器browser = webdriver.Firefox()#设置等待时长，最长等待10swait = WebDriverWait(browser,10)#定义电影排名comment_index = 0#定义一个movieclass Movie: __movie_id = 0 __movie_name = '' __movie_star = '' __movie_releasetime = '' __movie_score = '' def __init__(self , movie_id,movie_name,movie_star,movie_releasetime,movie_score): self.__movie_id = movie_id self.__movie_name = movie_name self.__movie_star = movie_star self.__movie_releasetime = movie_releasetime self.__movie_score = movie_score def show(self): print('影片排名: ', self.__movie_id) print('影片名称: ', self.__movie_name) print(self.__movie_star) print(self.__movie_releasetime) print('影片评分', self.__movie_score) print('') def simple_list(self): return [self.__movie_id, self.__movie_name, self.__movie_star, self.__movie_releasetime, self.__movie_score] def save2csv(movie_list): with open('movie.csv', 'a', newline='',encoding='utf-8') as csvfile: csv_writer = csv.writer(csvfile) for m in movie_list: csv_writer.writerow(m.simple_list()) csvfile.close() def main(): #打开URL browser.get('http://maoyan.com/board/4') #输出浏览器标题 print('browser title: ',browser.title) #数据更新信息 p_update_info = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,'div.main p.update-time'))) print('更新信息: ',p_update_info.text) #输出当前页信息 show_current_page() def show_current_page(): print('-----------------------------------') print('current url: ',browser.current_url) #获取当前页信息 div_page = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR,'div.pager-main li.active'))) print('current page: ',div_page.text) #当前页总数量 div_items = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR,'div.main div.board-item-main div.board-item-content'))) print('total count: ',len(div_items)) movie_list = [] #结果集有很多结果，我们获取 for item in div_items: #调用全局变量 global comment_index comment_index += 1 #获取我们需要的信息 p_name = item.find_element_by_css_selector('div.movie-item-info p.name') p_star = item.find_element_by_css_selector('div.movie-item-info p.star') p_releasetime = item.find_element_by_css_selector('div.movie-item-info p.releasetime') p_score_integer = item.find_element_by_css_selector('div.movie-item-number p.score i.integer') p_score_fraction = item.find_element_by_css_selector('div.movie-item-number p.score i.fraction') #初始化movie对象 m = Movie(comment_index , p_name.text , p_star.text, p_releasetime.text, p_score_integer.text+p_score_fraction.text) movie_list.append(m) save2csv(movie_list) #获取下一页 show_next_page()def show_next_page(): try: #获取下一页的标签 #最后1页，没有下一页标签 a_next = wait.until(EC.presence_of_element_located((By.PARTIAL_LINK_TEXT,'下一页'))) a_next.click() show_current_page() except TimeoutException: #找不到下一页 print('get all movies.') #也有可能真是网络异常 finally: browser.quit() if __name__=='__main__': main() 这里就简单记录遇到的一些问题和后面需要优化的地方： 获取影片信息的时候，数据没有清洗好，像这个“主演”，“上映时间”还没有剔除掉；那个地点也可以拆分出来单独一个字段 csv编码问题，一开始默认使用gbk（在Windows下开发的），会报错，说是有异常的字符无法保存，改为UTF-8后，就可以了，但是使用CSV打开前，先用notepad++转了编码，才用CSV打开 使用正则去获取元素 4.异常问题的处理]]></content>
      <categories>
        <category>Python-爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy手册(4)-ufunc]]></title>
    <url>%2F2017%2F08%2F08%2Fnumpy-handbook-04%2F</url>
    <content type="text"><![CDATA[PythonNumpy知识总结 这里，我们说下对数组操作的常用函数 常用函数我们先说下接收一个参数的一元函数，比如 np.sqrt 开方函数1234567891011a = np.arange(10)np.sqrt(a)Out[45]: array([ 0. , 1. , 1.41421356, 1.73205081, 2. , 2.23606798, 2.44948974, 2.64575131, 2.82842712, 3. ])a**0.5Out[46]: array([ 0. , 1. , 1.41421356, 1.73205081, 2. , 2.23606798, 2.44948974, 2.64575131, 2.82842712, 3. ]) 常用的一元函数 还有些常用的二元函数，比如 add,subtract123456789101112131415a = np.array([1,2,3])b = np.array([4,5,6])aOut[49]: array([1, 2, 3])bOut[50]: array([4, 5, 6])np.add(a,b)Out[51]: array([5, 7, 9])np.subtract(a,b)Out[52]: array([-3, -3, -3]) np.wherenp.where 是三元表达式 x if condition else y 的矢量化版本12345numpy.where(condition[, x, y])Return elements, either from x or y, depending on condition.If only condition is given, return condition.nonzero(). 我们根据condition的值，来确定是返回x的值，还是y的值12345678x = np.array([1.1,1.2,1.3,1.4,1.5])y = np.array([2.1,2.2,2.3,2.4,2.5])cond = np.array([True,False,True,True,False])np.where(cond,x,y)Out[56]: array([ 1.1, 2.2, 1.3, 1.4, 2.5]) np.where的第2个，第3个参数不一定是数组，也可以是标量值；比如，有一个矩阵，我们想要将所有正值替换为2，负值替换为-21234567891011121314151617181920212223a = np.random.randn(4,4)aOut[58]: array([[-0.62737481, -0.69252389, 0.34290602, -0.54297339], [ 0.57164788, -0.74841413, 1.02406934, 0.3089722 ], [-0.46170713, 2.17671732, -0.51607955, -0.44006653], [-0.13365017, 0.67350363, -0.51877754, -0.382468 ]])np.where(a&gt;0,2,-2)Out[59]: array([[-2, -2, 2, -2], [ 2, -2, 2, 2], [-2, 2, -2, -2], [-2, 2, -2, -2]])#负值的话，我们使用原来的值np.where(a&gt;0,2,a)Out[60]: array([[-0.62737481, -0.69252389, 2. , -0.54297339], [ 2. , -0.74841413, 2. , 2. ], [-0.46170713, 2. , -0.51607955, -0.44006653], [-0.13365017, 2. , -0.51877754, -0.382468 ]]) 数组统计方法我们可以统计数组或某个轴上的数据进行统计计算123456789101112131415a = np.array([1,2,3])a.sum()Out[68]: 6a.max()Out[69]: 3a.min()Out[70]: 1a.mean()Out[71]: 2.0numpy.sum(a, axis=None, dtype=None, out=None, keepdims=&lt;class numpy._globals._NoValue at 0x40ba726c&gt;) 这类聚合函数，可以接收一个axis参数，指定要聚合的轴123456789101112131415a = np.arange(15).reshape(3,5)aOut[74]: array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]])#在横轴上聚合a.sum(axis=1)Out[75]: array([10, 35, 60])#在列上聚合a.sum(axis=0)Out[76]: array([15, 18, 21, 24, 27]) 常用的统计函数 用于布尔数组的方法 对于布尔数组来说，执行上面的统计函数，会是将True转成1，False转成0123456789101112b = np.array([True,False,True,True])b.sum()Out[78]: 3#b中有Trueb.any()Out[79]: True#b中不都为Trueb.all()Out[80]: False 还有2个函数any和all，可以判断数组中是否存在一个或多个True 排序我们可以对数组进行排序12345678910111213141516ndarray.sort(axis=-1, kind='quicksort', order=None)Sort an array, in-place.a = np.random.randn(10)aOut[82]: array([ 0.02418202, -1.86975588, 0.00273745, 0.22470742, 1.10362729, 0.75344308, -0.89005284, -0.94833805, 1.37111527, 1.22149417])a.sort()aOut[84]: array([-1.86975588, -0.94833805, -0.89005284, 0.00273745, 0.02418202, 0.22470742, 0.75344308, 1.10362729, 1.22149417, 1.37111527]) ndarray是就地排序，直接排序原数组；np.sort则是返回一个排序后的数组 其他集合函数比如unique，可以获取数组的唯一值1234567a = np.array([1,3,3,3,5,5,2,1])aOut[90]: array([1, 3, 3, 3, 5, 5, 2, 1])np.unique(a)Out[92]: array([1, 2, 3, 5])]]></content>
      <categories>
        <category>Python-Numpy</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas手册（7）- pandas数据加载]]></title>
    <url>%2F2017%2F08%2F08%2Fpandas-handbook-07%2F</url>
    <content type="text"><![CDATA[PythonPandas 这里整理下，pandas中数据加载的几个方法，前面，我们也有用过，read_csv，下面，我们整理下 1.pandas读取数据方法1pandas.read_csv(filepath_or_buffer, sep=', ', delimiter=None, header='infer', names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, iterator=False, chunksize=None, compression='infer', thousands=None, decimal=b'.', lineterminator=None, quotechar='"', quoting=0, escapechar=None, comment=None, encoding=None, dialect=None, tupleize_cols=False, error_bad_lines=True, warn_bad_lines=True, skipfooter=0, skip_footer=0, doublequote=True, delim_whitespace=False, as_recarray=False, compact_ints=False, use_unsigned=False, low_memory=True, buffer_lines=None, memory_map=False, float_precision=None) 我们可以看到这个参数非常多，基本上可以解决我们文件读取时的常见问题。 下面就是小例子csv文件内容是这样的：数据以逗号分隔，没有其他特殊的情况 123456789101112131415161718import pandas as pddf = pd.read_csv(r'D:\document\python_demo\pydata-book-master\ch06\ex1.csv')#使用read_table需要，手动指定下分隔符#df = pd.read_table(r'D:\document\python_demo\pydata-book-master\ch06\ex1.csv',sep=',')print(df)print(df.index)print(df.columns)runfile('D:/document/python_demo/pandas_csv.py', wdir='D:/document/python_demo') a b c d message0 1 2 3 4 hello1 5 6 7 8 world2 9 10 11 12 fooRangeIndex(start=0, stop=3, step=1)Index(['a', 'b', 'c', 'd', 'message'], dtype='object') 我们可以看到，这里默认把第一行当做columns了，我们可以通过header=None，来自动指定标题1234567891011121314151617181920df = pd.read_csv(r'D:\document\python_demo\pydata-book-master\ch06\ex1.csv',header=None) 0 1 2 3 40 a b c d message1 1 2 3 4 hello2 5 6 7 8 world3 9 10 11 12 fooRangeIndex(start=0, stop=4, step=1)Int64Index([0, 1, 2, 3, 4], dtype='int64')#通过names，来自定义columnsdf = pd.read_csv(r'D:\document\python_demo\pydata-book-master\ch06\ex1.csv',header=None,names=list('opqrxy')) o p q r x y0 a b c d message NaN1 1 2 3 4 hello NaN2 5 6 7 8 world NaN3 9 10 11 12 foo NaNRangeIndex(start=0, stop=4, step=1)Index(['o', 'p', 'q', 'r', 'x', 'y'], dtype='object') 现在的行索引是自动初始化的，我们可以指定存在的列为索引12345678910111213path = r'D:\document\python_demo\pydata-book-master\ch06\ex1.csv'df = pd.read_csv(path,header=None, names=list('opqrxy'), index_col='x') o p q r yx message a b c d NaNhello 1 2 3 4 NaNworld 5 6 7 8 NaNfoo 9 10 11 12 NaNIndex(['message', 'hello', 'world', 'foo'], dtype='object', name='x')Index(['o', 'p', 'q', 'r', 'y'], dtype='object') 比如，有这样一份数据，字段间是通过一个或多个空格来分隔的 我们直接使用read_csv去读取，会发现，列索引有些不友好，我们可以使用正则表达式去分隔123456789101112131415161718df = pd.read_csv(path) A B C0 aaa -0.264438 -1.026059 -0.6195001 bbb 0.927272 0.302904 -0.0323992 ccc -0.264273 -0.386314 -0.2176013 ddd -0.871858 -0.348382 1.100491RangeIndex(start=0, stop=4, step=1)Index([' A B C'], dtype='object')#\s表示空格，+表示1个或多个，就是说分隔符是1个或多个空格df = pd.read_table(path,sep='\s+') A B Caaa -0.264438 -1.026059 -0.619500bbb 0.927272 0.302904 -0.032399ccc -0.264273 -0.386314 -0.217601ddd -0.871858 -0.348382 1.100491Index(['aaa', 'bbb', 'ccc', 'ddd'], dtype='object')Index(['A', 'B', 'C'], dtype='object') 还有很多其他常用的参数，比如skiprows，可以跳过指定行下面，再说个填充缺失值的方法，na_values可以将其他我们指定的值也当成NaN处理12345678910111213141516171819202122232425262728na_values : scalar, str, list-like, or dict, default None Additional strings to recognize as NA/NaN. If dict passed, specific per-column NA values. By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, ‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, ‘nan’`.#原始数据是这样的， something a b c d message0 one 1 2 3.0 4 NaN1 two 5 6 NaN 8 world2 three 9 10 11.0 12 foo#这里，我们将1，2，4也当成NaN值处理df = pd.read_csv(path,na_values=[1,2,4])#加载后，数据就会变成这样 something a b c d message0 one NaN NaN 3.0 NaN NaN1 two 5.0 6.0 NaN 8.0 world2 three 9.0 10.0 11.0 12.0 foo#我们还可以用一个dict来对不同的列指定NaN值df = pd.read_csv(path,na_values=&#123;'something':['one','three'],'d':[8,12]&#125;) something a b c d message0 NaN 1 2 3.0 4.0 NaN1 two 5 6 NaN NaN world2 NaN 9 10 11.0 NaN foo #2.pandas导出数据我们使用read_csv读取数据，处理完之后，我们可能还需要将数据存储起来，还有一个to_csv的函数1234567891011121314151617181920212223242526272829DataFrame.to_csv(path_or_buf=None, sep=', ', na_rep='', float_format=None, columns=None, header=True, index=True, index_label=None, mode='w', encoding=None, compression=None, quoting=None, quotechar='"', line_terminator='\n', chunksize=None, tupleize_cols=False, date_format=None, doublequote=True, escapechar=None, decimal='.')##我们就使用这份数据，先读取后，再保存something,a,b,c,d,messageone,1,2,3,4,NAtwo,5,6,,8,worldthree,9,10,11,12,foodf.to_csv(sys.stdout,sep='^')^something^a^b^c^d^message0^one^1^2^3.0^4^1^two^5^6^^8^world2^three^9^10^11.0^12^foo#我们看到这个输出，发现，NaN值，导出时，转为了空字符串，这里呢，我们可以手工指定na_rep : string, default ‘’df.to_csv(sys.stdout,sep='^',na_rep='NaN')^something^a^b^c^d^message0^one^1^2^3.0^4^NaN1^two^5^6^NaN^8^world2^three^9^10^11.0^12^foo#有时候，行列标签可能也不需要，我们也可以禁用df.to_csv(sys.stdout,sep='^',na_rep='NaN',index=False,header=False)one^1^2^3.0^4^NaNtwo^5^6^NaN^8^worldthree^9^10^11.0^12^foo #3.附录pandas中还有其他的加载数据方式，像读取HTML，JSON，xml等等，常用的可能还是和数据库取连接，这块后面会再补充，这里就先到这里。]]></content>
      <categories>
        <category>Python-Pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy手册(3)-Datetimes and Timedeltas]]></title>
    <url>%2F2017%2F08%2F07%2Fnumpy-handbook-03%2F</url>
    <content type="text"><![CDATA[PythonNumpy知识总结 这里说下numpy中日期、时间相关的使用。主要参考：https://docs.scipy.org/doc/numpy-dev/reference/arrays.datetime.html 基本使用在numpy中，我们很方便的讲字符串转换成日期类型123456789101112131415161718192021222324252627import numpy as npnp.datetime64('2017-08-06')Out[3]: numpy.datetime64('2017-08-06')np.datetime64('2017-08')Out[4]: numpy.datetime64('2017-08')#我们可以通过参数，强制将数据格式化为我们想要的粒度np.datetime64('2017-08' , 'D')Out[5]: numpy.datetime64('2017-08-01')np.datetime64('2017-08' , 'Y')Out[6]: numpy.datetime64('2017')a = np.array(['2017-07-01','2017-07-15','2017-08-01'] , dtype=np.datetime64)aOut[10]: array(['2017-07-01', '2017-07-15', '2017-08-01'], dtype='datetime64[D]')#我们也可以使用arange函数初始化数组b = np.arange('2017-08-01','2017-09-01',dtype=np.datetime64)bOut[12]: array(['2017-08-01', '2017-08-02', '2017-08-03', ..., '2017-08-29', '2017-08-30', '2017-08-31'], dtype='datetime64[D]') 日期计算在numpy中，我们可以进行简单的日期计算 1234567891011121314151617181920#2个日期相减，会得到相差的天数np.datetime64('2017-08-03') - np.datetime64('2017-07-15')Out[27]: numpy.timedelta64(19,'D')#这里日期可以直接减去对应的天数np.datetime64('2017-08-03') - np.timedelta64(20,'D')Out[28]: numpy.datetime64('2017-07-14')#这里粒度要一样，一个是D,不可以和M相减np.datetime64('2017-08-03') - np.timedelta64(1,'M')Traceback (most recent call last): File "&lt;ipython-input-29-61beff16fb05&gt;", line 1, in &lt;module&gt; np.datetime64('2017-08-03') - np.timedelta64(1,'M')TypeError: Cannot get a common metadata divisor for NumPy datetime metadata [D] and [M] because they have incompatible nonlinear base time unitsnp.datetime64('2017-08') - np.timedelta64(1,'M')Out[30]: numpy.datetime64('2017-07') 工作日判断numpy中提供了一个一些工作日判断的函数，比如，通常周一到周五是工作日123456789101112numpy.busday_offset(dates, offsets, roll='raise', weekmask='1111100', holidays=None, busdaycal=None, out=None)First adjusts the date to fall on a valid day according to the roll rule, then applies offsets to the given dates counted in valid days.#2017-08-01是周二#2017-08-01的下一个工作日是2017-08-02np.busday_offset('2017-08-01',1)Out[32]: numpy.datetime64('2017-08-02')#2017-08-01的下2个工作日是2017-08-03np.busday_offset('2017-08-01',2)Out[33]: numpy.datetime64('2017-08-03') 这时候，如果传入的日期是周末，就会报错了12345678#2017-08-05是周六np.busday_offset('2017-08-05',2)Traceback (most recent call last): File "&lt;ipython-input-34-9f767204127b&gt;", line 1, in &lt;module&gt; np.busday_offset('2017-08-05',2)ValueError: Non-business day date in busday_offset 当然，我们可以通过参数来避免错误123456789101112roll : &#123;‘raise’, ‘nat’, ‘forward’, ‘following’, ‘backward’, ‘preceding’, ‘modifiedfollowing’, ‘modifiedpreceding’&#125;, optional How to treat dates that do not fall on a valid day. The default is ‘raise’. ‘raise’ means to raise an exception for an invalid day. ‘nat’ means to return a NaT (not-a-time) for an invalid day. ‘forward’ and ‘following’ mean to take the first valid day later in time. ‘backward’ and ‘preceding’ mean to take the first valid day earlier in time. ‘modifiedfollowing’ means to take the first valid day later in time unless it is across a Month boundary, in which case to take the first valid day earlier in time. ‘modifiedpreceding’ means to take the first valid day earlier in time unless it is across a Month boundary, in which case to take the first valid day later in time. 常用的可能是这个forward和backward一个是向前取第一个有效的工作日，一个是向后取第一个有效的工作日 123456789101112131415161718192021222324252627282930np.busday_offset('2017-08-05',2,roll='forward')Out[35]: numpy.datetime64('2017-08-09')np.busday_offset('2017-08-05',2,roll='backward')Out[36]: numpy.datetime64('2017-08-08')np.busday_offset('2017-08-05',0,roll='forward')Out[37]: numpy.datetime64('2017-08-07')np.busday_offset('2017-08-05',0,roll='backward')Out[38]: numpy.datetime64('2017-08-04')#判断是否为工作日np.is_busday()np.is_busday(np.datetime64('2017-08-05'))Out[39]: Falsenp.is_busday(np.datetime64('2017-08-01'))Out[40]: True#判断时间段内，工作日天数np.busday_count()np.busday_count(np.datetime64('2017-08-01'),np.datetime64('2017-08-06'))Out[41]: 4np.busday_count(np.datetime64('2017-08-06'),np.datetime64('2017-08-01'))Out[42]: -4]]></content>
      <categories>
        <category>Python-Numpy</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas手册（6）- 用pandas完成excel中常见任务]]></title>
    <url>%2F2017%2F08%2F07%2Fpandas-handbook-06%2F</url>
    <content type="text"><![CDATA[PythonPandas 这里整理下pandas常用的操作，为什么要写这个呢？有本书《利用Python进行数据分析》一边看一遍记录下。 1. 重新索引(reindex)就是重构一下索引，在重构的同时，我们可以做一些其他操作12345DataFrame.reindex(index=None, columns=None, **kwargs)Conform DataFrame to new index with optional filling logic, placing NA/NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=FalseSeries.reindex(index=None, **kwargs)Conform Series to new index with optional filling logic, placing NA/NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False 一个小例子123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172obj = pd.Series([4.5, 7.2, -5.3, 3.6], index=['d', 'b', 'a', 'c'])objOut[156]: d 4.5b 7.2a -5.3c 3.6dtype: float64#reindex后，没有的值，默认会用NaN填充obj.reindex(['a','b','c','d','e'])Out[157]: a -5.3b 7.2c 3.6d 4.5e NaNdtype: float64#fill_value，常用的参数，表示没有数据时默认填充的值obj.reindex(['a','b','c','d','e'] , fill_value=9.9)Out[159]: a -5.3b 7.2c 3.6d 4.5e 9.9dtype: float64#method,常用参数，在递增或递减index中，填充空值的方法obj3 = pd.Series(['blue', 'purple', 'yellow'], index=[0, 2, 4])obj3Out[165]: 0 blue2 purple4 yellowdtype: objectobj3.reindex(range(6))Out[170]: 0 blue1 NaN2 purple3 NaN4 yellow5 NaNdtype: object#ffill，前向填充obj3.reindex(range(6),method='ffill')Out[167]: 0 blue1 blue2 purple3 purple4 yellow5 yellowdtype: object#bfill，后向填充obj3.reindex(range(6),method='bfill')Out[171]: 0 blue1 purple2 purple3 yellow4 yellow5 NaNdtype: object 对于DataFrame来说，用起来也是差不多的 2. 丢弃指定轴上的项主要就是drop方法的使用12DataFrame.drop(labels, axis=0, level=None, inplace=False, errors='raise')Return new object with labels in requested axis removed. 小例子12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455obj = pd.Series(np.arange(5.), index=['a', 'b', 'c', 'd', 'e'])objOut[174]: a 0.0b 1.0c 2.0d 3.0e 4.0dtype: float64obj.drop('c')Out[175]: a 0.0b 1.0d 3.0e 4.0dtype: float64obj.drop(['b','d'])Out[176]: a 0.0c 2.0e 4.0dtype: float64#DataFramedata = pd.DataFrame(np.arange(16).reshape((4, 4)), index=['Ohio', 'Colorado', 'Utah', 'New York'], columns=['one', 'two', 'three', 'four'])dataOut[178]: one two three fourOhio 0 1 2 3Colorado 4 5 6 7Utah 8 9 10 11New York 12 13 14 15#默认是横轴，data.drop(['Ohio','Utah'])Out[179]: one two three fourColorado 4 5 6 7New York 12 13 14 15#我们可以指定axis，在columns上删除data.drop(['two','four'],axis=1)Out[180]: one threeOhio 0 2Colorado 4 6Utah 8 10New York 12 14 3. 算术运算和数据对齐在numpy和pandas中好像都会看到这个词，数据对齐，就是说2个对象在运算的时候，会取一个并集，然后在自动对齐的时候，不重叠的部分就会填充NaN 小例子先看看123456789101112131415161718192021222324252627s1 = pd.Series([7.3, -2.5, 3.4, 1.5], index=['a', 'c', 'd', 'e'])s2 = pd.Series([-2.1, 3.6, -1.5, 4, 3.1], index=['a', 'c', 'e', 'f', 'g'])#index不重叠的地方，会填充NaNs1+s2Out[188]: a 5.2c 1.1d NaNe 0.0f NaNg NaNdtype: float64#使用自带的add方法，就可以填充默认值了，这个和我们上面reindex时的思想是一样的#Series.add(other, level=None, fill_value=None, axis=0)s1.add(s2,fill_value=0)Out[189]: a 5.2c 1.1d 3.4e 0.0f 4.0g 3.1dtype: float64 4.DataFrame和Series之间的运算这里用到了一个广播的思想，就是指不同形状的数组之间的算术运算的执行方式，很强大的功能，这里，我们先简单了解下。小例子1234567891011121314151617arr = np.arange(12.).reshape((3, 4))arrOut[191]: array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]])arr[0]Out[192]: array([ 0., 1., 2., 3.])#3行4列的数组，减1行4列的数组，这就是广播arr - arr[0]Out[193]: array([[ 0., 0., 0., 0.], [ 4., 4., 4., 4.], [ 8., 8., 8., 8.]]) DataFrame和Series之间的计算也是这样12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364frame = pd.DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'), index=['Utah', 'Ohio', 'Texas', 'Oregon'])frameOut[195]: b d eUtah 0.0 1.0 2.0Ohio 3.0 4.0 5.0Texas 6.0 7.0 8.0Oregon 9.0 10.0 11.0s = frame.iloc[0]sOut[197]: b 0.0d 1.0e 2.0Name: Utah, dtype: float64frame - sOut[198]: b d eUtah 0.0 0.0 0.0Ohio 3.0 3.0 3.0Texas 6.0 6.0 6.0Oregon 9.0 9.0 9.0s = pd.Series(range(3),index=list('abc'))frameOut[223]: b d eUtah 0.0 1.0 2.0Ohio 3.0 4.0 5.0Texas 6.0 7.0 8.0Oregon 9.0 10.0 11.0sOut[224]: a 0b 1c 2dtype: int32frame.add(s)Out[225]: a b c d eUtah NaN 1.0 NaN NaN NaNOhio NaN 4.0 NaN NaN NaNTexas NaN 7.0 NaN NaN NaNOregon NaN 10.0 NaN NaN NaN#我们可以通过axis控制在哪个方向上去广播frame.add(s,axis=0)Out[227]: b d eOhio NaN NaN NaNOregon NaN NaN NaNTexas NaN NaN NaNUtah NaN NaN NaNa NaN NaN NaNb NaN NaN NaNc NaN NaN NaN 在这里，不能使用fill_value填充默认值，还不知道为啥，总是报错，说不支持 5. 函数应用和映射这里主要是介绍DataFrame中的一个函数使用，apply，就是对DataFrame中的每一个元素执行传入的函数12DataFrame.apply(func, axis=0, broadcast=False, raw=False, reduce=None, args=(), **kwds)Applies function along input axis of DataFrame. 小例子1234567891011121314151617181920212223242526272829f = lambda x: x+10#每一个单元格都会加10frame.apply(f)Out[230]: b d eUtah 10.0 11.0 12.0Ohio 13.0 14.0 15.0Texas 16.0 17.0 18.0Oregon 19.0 20.0 21.0f = lambda x: x.max() - x.min()frame.apply(f)Out[232]: b 9.0d 9.0e 9.0dtype: float64#我们可以指定轴，去执行函数frame.apply(f,axis=1)Out[233]: Utah 2.0Ohio 2.0Texas 2.0Oregon 2.0dtype: float64 这里还有一个applymap函数123DataFrame.applymap(func)Apply a function to a DataFrame that is intended to operate elementwise, i.e. like doing map(func, series) for each series in the DataFrame 这里得注意下，这2个函数的区别；目前的理解是，applymap是元素级的，apply在轴上进行操作（貌似不太顺，等明白了再记录下）123456789101112131415161718f = lambda x: '$&#123;:,.3f&#125;'.format(x)frameOut[237]: b d eUtah 0.0 1.0 2.0Ohio 3.0 4.0 5.0Texas 6.0 7.0 8.0Oregon 9.0 10.0 11.0#前面，我们有用过，格式化内容的frame.applymap(f)Out[238]: b d eUtah $0.000 $1.000 $2.000Ohio $3.000 $4.000 $5.000Texas $6.000 $7.000 $8.000Oregon $9.000 $10.000 $11.000 6.处理缺失数据在pandas中处理缺失数据非常容易，pandas使用浮点值NaN（Not a Number）表示缺失值。前面，我们说过使用isnull来判断是否有NaN值小例子12345678910111213141516171819202122232425262728293031323334353637383940414243444546a = pd.Series(['one','two',np.nan,'three'])aOut[240]: 0 one1 two2 NaN3 threedtype: objecta.isnull()Out[241]: 0 False1 False2 True3 Falsedtype: boola.notnull()Out[242]: 0 True1 True2 False3 Truedtype: bool#Python内置的None也会被当做NaN处理a[4]=NoneaOut[247]: 0 one1 two2 NaN3 three4 Nonedtype: objecta.isnull()Out[248]: 0 False1 False2 True3 False4 Truedtype: bool 对于这种数据，我们要怎样处理呢？有的时候，我们可能会初始化为默认值，或者直接剔除掉我们可以使用dropna函数来剔除掉，或者布尔类型索引1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556DataFrame.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)aOut[249]: 0 one1 two2 NaN3 three4 Nonedtype: objecta.dropna()Out[250]: 0 one1 two3 threedtype: objecta[a.notnull()]Out[251]: 0 one1 two3 threedtype: object##dataframedata = pd.DataFrame([[1., 6.5, 3.], [1., np.nan, np.nan], [np.nan, np.nan, np.nan], [np.nan, 6.5, 3.]])dataOut[254]: 0 1 20 1.0 6.5 3.01 1.0 NaN NaN2 NaN NaN NaN3 NaN 6.5 3.0#默认的话，会将行、列含有NaN的都剔除掉data.dropna()Out[255]: 0 1 20 1.0 6.5 3.0#我们可以使用参数how来控制how : &#123;‘any’, ‘all’&#125; any : if any NA values are present, drop that label all : if all values are NA, drop that labeldata.dropna(how='all')Out[257]: 0 1 20 1.0 6.5 3.01 1.0 NaN NaN3 NaN 6.5 3.0 有的时候，我们想要做填充而不是剔除，像我们前面使用的参数fill_value12345678910111213141516171819202122232425262728DataFrame.fillna(value=None, method=None, axis=None, inplace=False, limit=None, downcast=None, **kwargs)Fill NA/NaN values using the specified methodmethod : &#123;‘backfill’, ‘bfill’, ‘pad’, ‘ffill’, None&#125;, default NonedataOut[261]: 0 1 20 1.0 6.5 3.01 1.0 NaN NaN2 NaN NaN NaN3 NaN 6.5 3.0data.fillna(9.9)Out[259]: 0 1 20 1.0 6.5 3.01 1.0 9.9 9.92 9.9 9.9 9.93 9.9 6.5 3.0#使用method，和前面reindex的时候是一个道理data.fillna(method='ffill')Out[262]: 0 1 20 1.0 6.5 3.01 1.0 6.5 3.02 1.0 6.5 3.03 1.0 6.5 3.0]]></content>
      <categories>
        <category>Python-Pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy手册(2)-常用操作杂记]]></title>
    <url>%2F2017%2F08%2F06%2Fnumpy-handbook-02%2F</url>
    <content type="text"><![CDATA[PythonNumpy知识总结 这里记录下numpy常用的一些操作，一些散乱的知识点。 数组和标量之间的运算就是对数组进行批量的运算123456789101112131415161718192021import numpy as npa = np.arange(15).reshape(3,5)aOut[3]: array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]])a+2Out[4]: array([[ 2, 3, 4, 5, 6], [ 7, 8, 9, 10, 11], [12, 13, 14, 15, 16]])a+aOut[5]: array([[ 0, 2, 4, 6, 8], [10, 12, 14, 16, 18], [20, 22, 24, 26, 28]]) 基本的索引和切片上一篇，我们大概介绍过数组的切片，这里的切片操作都是原数组的一个视图，即原数组变化后，视图也会变化123456789101112131415161718192021222324252627aOut[7]: array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]])a[1]Out[8]: array([5, 6, 7, 8, 9])a[1][2]Out[9]: 7a[1:2]Out[10]: array([[5, 6, 7, 8, 9]])b=a[1:2]a[1:2]=0bOut[13]: array([[0, 0, 0, 0, 0]])aOut[14]: array([[ 0, 1, 2, 3, 4], [ 0, 0, 0, 0, 0], [10, 11, 12, 13, 14]]) 花式索引花式索引，就是利用整数数组进行索引。123456789101112131415161718192021x = np.empty((8,4))for i in range(8): x[i]=i xOut[18]: array([[ 0., 0., 0., 0.], [ 1., 1., 1., 1.], [ 2., 2., 2., 2.], ..., [ 5., 5., 5., 5.], [ 6., 6., 6., 6.], [ 7., 7., 7., 7.]])x[[3,0,5]]Out[19]: array([[ 3., 3., 3., 3.], [ 0., 0., 0., 0.], [ 5., 5., 5., 5.]]) 如果使用负数，会从末尾开始选取1234x[[-1,-2]]Out[20]: array([[ 7., 7., 7., 7.], [ 6., 6., 6., 6.]]) 数组的转置12345678910111213141516xOut[21]: array([[ 0., 0., 0., 0.], [ 1., 1., 1., 1.], [ 2., 2., 2., 2.], ..., [ 5., 5., 5., 5.], [ 6., 6., 6., 6.], [ 7., 7., 7., 7.]])x.TOut[22]: array([[ 0., 1., 2., ..., 5., 6., 7.], [ 0., 1., 2., ..., 5., 6., 7.], [ 0., 1., 2., ..., 5., 6., 7.], [ 0., 1., 2., ..., 5., 6., 7.]]) 上面的主要是轴对换，转置的话，看下这个例子（大学学的矩阵都忘了，得回顾下…） 一开始真迷糊了，找了个解释，明天再研究下这个函数]]></content>
      <categories>
        <category>Python-Numpy</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy手册(1)-ndarray]]></title>
    <url>%2F2017%2F08%2F02%2Fnumpy-handbook-01%2F</url>
    <content type="text"><![CDATA[前面我们算是简单入门了Pandas，numpy也是数据分析中常用的，这里我们也来简单学习下。 1.numpy基本介绍numpy是Python的一种开源数值计算扩展，这种工具可以用来存储和处理大型矩阵。一个用Python实现的科学计算包。from 百度百科 numpy有2种基本对象，1ndarray（N-dimensional array object）和 ufunc（universal function object） ndarray是存储单一数据类型的多维数组，ufunc是能够对数组进行处理的函数。 2.ndarray我们先来看看这个数组首先，我们得引入numpy1import numpy as np 2.1 创建数组初始化的话有很多方式：Array creation routines我们可以直接使用list来初始化，array有很多的属性，比如大小，维度，元素个数123456789import numpy as npa = np.array([1,2,3])b = np.array([4,5,6])c = np.array([[1,2,3],[4,5,6],[7,8,9]])print(a,type(a),',shape:',a.shape,',ndim:',a.ndim,',size:',a.size)print(b,type(b),',shape:',b.shape,',ndim:',b.ndim,',size:',b.size)print(c,type(c),',shape:',c.shape,',ndim:',c.ndim,',size:',c.size) 这里呢，我们定义了一维数组和二维数组，比如c，是3行3列的2维数组，元素个数是9个1numpy.array(object, dtype=None, copy=True, order='K', subok=False, ndmin=0) 这里，我们再说下这个shape，这个属性可以修改123456789#原来是4行3列c = np.array([[1,2,3],[4,5,6],[7,8,9],[0,0,7]])print(c)#我们改为3行4列c.shape=(3,4)print(c)#改为2行6列c.shape=(2,6)print(c) 这里需要注意下，如果某个轴的元素为-1，将根据数组元素的个数，自动计算长度1234c.shape=(1,-1)print(c)c.shape=(-1,1)print(c) 这里的shape是改变原来的数组，另一个method，可以创建一个改变shape的新数组，而原数组保持不变12345c = np.array([[1,2,3],[4,5,6],[7,8,9],[0,0,7]])print('c:',c)d = c.reshape(2,6)print('c:',c)print('d:',d) 这里要注意的是，c和d共享内存数据存储内存区域，c变了，d也会变12345print(c[0])#修改c[0]c[0]=[-9,-8,-3]print('c:',c)print('d:',d) 我们可以通过dtype来获取元素的类型，我们可以在初始化的时候，指定dtype12345c = np.array([1,2,3])print(c.dtype) #int32d = np.array([1.1,3.3])print(d.dtype) #float64 下面，我们来看看常用的初始化方法 arange通过指定开始值，结束值和步长来创建一维数组，这里不包过终值1234567arange([start,] stop[, step,], dtype=None)np.arange(3)Out[51]: array([0, 1, 2])np.arange(1,10,3)Out[52]: array([1, 4, 7]) linspace通过指定开始值，终值和元素个数，来创建数组，这里包括终值12345np.linspace(1,10,5)Out[53]: array([ 1. , 3.25, 5.5 , 7.75, 10. ])np.linspace(1,2,3)Out[54]: array([ 1. , 1.5, 2. ]) np.zeros,np.ones这2个函数可以初始化指定长度或形状的全0或全1的数组1234567891011121314151617np.ones(3)Out[202]: array([ 1., 1., 1.])np.ones([2,2])Out[203]: array([[ 1., 1.], [ 1., 1.]])np.zeros(5)Out[204]: array([ 0., 0., 0., 0., 0.])np.zeros([4,3])Out[205]: array([[ 0., 0., 0.], [ 0., 0., 0.], [ 0., 0., 0.], [ 0., 0., 0.]]) np.empty可以创建一个没有任何具体值得数组1234567891011np.empty(2)Out[211]: array([ 7.74860419e-304, 7.74860419e-304])np.empty(2,dtype=int)Out[214]: array([ -1, 2147483647])np.empty((3,3),dtype=np.float64)Out[215]: array([[ 4.94065646e-324, 9.88131292e-324, 1.48219694e-323], [ 1.97626258e-323, 2.47032823e-323, 2.96439388e-323], [ 3.45845952e-323, 3.95252517e-323, 4.44659081e-323]]) 这要注意下，empty初始化的都是没有意思的值，不一定会初始化为0 2.2 存取元素这里直接粘贴一个例子，原始教程在这：http://old.sebug.net/paper/books/scipydoc/numpy_intro.html123456789101112131415161718&gt;&gt;&gt; a = np.arange(10)&gt;&gt;&gt; a[5] # 用整数作为下标可以获取数组中的某个元素5&gt;&gt;&gt; a[3:5] # 用范围作为下标获取数组的一个切片，包括a[3]不包括a[5]array([3, 4])&gt;&gt;&gt; a[:5] # 省略开始下标，表示从a[0]开始array([0, 1, 2, 3, 4])&gt;&gt;&gt; a[:-1] # 下标可以使用负数，表示从数组后往前数array([0, 1, 2, 3, 4, 5, 6, 7, 8])&gt;&gt;&gt; a[2:4] = 100,101 # 下标还可以用来修改元素的值&gt;&gt;&gt; aarray([ 0, 1, 100, 101, 4, 5, 6, 7, 8, 9])&gt;&gt;&gt; a[1:-1:2] # 范围中的第三个参数表示步长，2表示隔一个元素取一个元素array([ 1, 101, 5, 7])&gt;&gt;&gt; a[::-1] # 省略范围的开始下标和结束下标，步长为-1，整个数组头尾颠倒array([ 9, 8, 7, 6, 5, 4, 101, 100, 1, 0])&gt;&gt;&gt; a[5:1:-2] # 步长为负数时，开始下标必须大于结束下标array([ 5, 101]) 就2维数组来说 这是基本的获取方式，还有些高级的方法 使用整数序列这里简单来2个练习，原文例子很多，就是通过下标来筛选数据12345678910111213141516171819202122a = np.arange(-5,5,1)aOut[68]: array([-5, -4, -3, -2, -1, 0, 1, 2, 3, 4])a[[1,3,5]]Out[69]: array([-4, -2, 0])### 使用布尔数组按照传入的布尔数组，只有为True的才返回，也叫布尔型索引``` pythona=np.array([-3,1,5])aOut[72]: array([-3, 1, 5])a[[False,True,False]]Out[73]: array([1])a[[True,False,True]]Out[74]: array([-3, 5]) 3.附录（参考资料）文档：https://docs.scipy.org/doc/numpy-dev/reference/index.html#reference numpy快速处理数据]]></content>
      <categories>
        <category>Python-Numpy</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cognos资料汇总贴]]></title>
    <url>%2F2017%2F08%2F01%2Fcognos-doc-main%2F</url>
    <content type="text"><![CDATA[以前搞过Cognos，写过很多基础的教程，应该是14年的样子，都在CSDN上，这里贴个汇总贴吧，想要看的同学可以去看看，希望有帮助。 ReportStudio入门教程：http://blog.csdn.net/column/details/ygy-reportstudio.html Framework Manage入门教程：http://blog.csdn.net/column/details/ygy-frameworkmanager.html Cognos函数手册：http://blog.csdn.net/column/details/ygy-cognos-function.html Cognos相关的其他资料（主页不同的类别下看看）：http://blog.csdn.net/yuguiyang1990 好了，感兴趣的同学，可以自行去看看，好久不搞了，估计有疑问也解决不了了…]]></content>
      <categories>
        <category>数据可视化-Cognos</category>
      </categories>
      <tags>
        <tag>数据可视化</tag>
        <tag>Cognos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小白学习Tableau-购物篮分析]]></title>
    <url>%2F2017%2F08%2F01%2FTableau-handbook-04%2F</url>
    <content type="text"><![CDATA[Tableau实例 关于购物篮的介绍可以参考数据分析案例-购物篮分析 目标我们想要分析的是顾客在购买商品的时候，哪些商品会同时购买。 下面，我们直接开始开发Tableau 数据源我们就是用Tableau默认自带的“示例-超市” 拖入两张订单表因为我们需要分析的是每个顾客，在购买A商品的时候，还会购买哪些商品。只使用一张订单表，不是很容易看出来，所以我们需要拖入2张订单表，以此来更方便的处理数据。因为我们要分析的是每个顾客，所以我们使用顾客ID去关联 拖子类别进行分析我们按照下图所示，进行拖拽 剔除无效的值手工剔除，好麻烦，如果数据是在数据库中，直接用SQL处理掉就可以了 这里，我们就快速的实现了购物篮分析，可以看到在购买某种商品时，同时购买其他商品的数量。 参考文章人人都是可视化分析师系列：用Tableau玩转购物篮分析]]></content>
      <categories>
        <category>Tableau</category>
      </categories>
      <tags>
        <tag>数据可视化</tag>
        <tag>Tableau</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas手册（5）- 用pandas完成excel中常见任务]]></title>
    <url>%2F2017%2F08%2F01%2Fpandas-handbook-05%2F</url>
    <content type="text"><![CDATA[PythonPandas 发现了一篇很好的教程，介绍一些Excel中的常用操作，怎样在pandas中实现，很不错，这里学习，顺便分享下。原文地址：用Pandas完成Excel中常见的任务， 这个是翻译的，再原文是：Common Excel Tasks Demonstrated in Pandas 好了，下面，我们开始学习下。 基础数据这个是从网上找的一个成绩单，拿了一部分数据 首先呢，我们想要，在加一列，显示总分，Excel中很方便 在pandas中呢，其实，我们就是需要“数学”，“语文”，“英语”这3列加在一起，我们怎样获取这3列呢？前面，我们说过在DataFrame中，怎样去筛选数据 12345678import pandas as pdimport numpy as npdf = pd.read_excel(r'D:\document\tableau_data\data_stu.xlsx',sheetname=0)print(df)print(df['数学'])print(df['语文']) 那我们只需要新增一列，把已知的3列加起来就行12df['总分'] = df['数学'] + df['语文'] + df['英语']print(df) 也是很方便，按照思路，直接相加就行了下面呢，我们来统计下，数学的总分、语文的总分，就是把每一列的数据都相加 DataFrame中有很多的聚合函数，这里简单介绍下12345678#数学的最大值print(df['数学'].max())#数学的最小值print(df['数学'].min())#数学的平均值print(df['数学'].mean())#数学的总分print(df['数学'].sum()) 这个和SQL里面一样，Excel里也是这样的，他会从这一列中，获取最大值、最小值等等下面，我们算个列的总分1234df['总分'] = df['数学'] + df['语文'] + df['英语']sum_data = df[['数学','语文','英语','总分']].sum()print(sum_data)print(type(sum_data)) 这样，我们构造了一个Series前面呢，我们知道，Series可以初始化一个DataFrame12df2 = pd.DataFrame(sum_data)print(df2) 初始化之后呢，是这样的，但是，结构不太一样，我们可以做一下行列转换12df2 = pd.DataFrame(sum_data).Tprint(df2) 这回样子像一些了，但是DataFrame中，需要保持结构一致，我们还需要填充几列我们可以用到reindex函数，重构一下索引12345DataFrame.reindex(index=None, columns=None, **kwargs)df2 = pd.DataFrame(sum_data).Tdf2 = df2.reindex(columns=df.columns)print(df2) 到了这里，我们只要将这个DataFrame插入到原来的DataFrame中就行了1DataFrame.append(other, ignore_index=False, verify_integrity=False)Append rows of other to the end of this frame, returning a new object. Columns not in this frame are added as new columns. 刚试了下，发现，前面不重构索引的话，也是可以的，这里会自动补全12df3 = df.append(df2,ignore_index=True)print(df3) 原文中还有些模糊匹配的例子，这里就不练习了，下面，我们看个分类汇总的小问题，这里又增加了一个班级列，要不不好测试 Excel里面实现，应该是这样的，在pandas中，我们要使用groupby这个函数1print(df[['班级','数学','语文','英语']].groupby(by=['班级']).sum() ) 原文中，还有一个格式化和rename index的问题，格式化还没搞明白，后面再说下吧1234567DataFrame.rename(index=None, columns=None, **kwargs)df2 = df[['班级','数学','语文','英语']].groupby(by=['班级']).sum()print(df2)df2 = df2.rename(index=&#123;'1班':'1班-汇总','2班':'2班-汇总'&#125;)print(df2) 我们可以通过一个dict，来替换索引的名字 好了，今天的分享就是这些，总结下呢，主要是对DataFrame中函数的理解和使用，还是得多多的练习才可以。 – 这里回头试下那个格式化的问题，刚刚学习了下，可以参考：Python基础（2）- 格式化format 主要就是format那个函数的使用，还有DataFrame中那个applymap的使用1DataFrame.applymap(func)Apply a function to a DataFrame that is intended to operate elementwise, i.e. like doing map(func, series) for each series in the DataFrame 123456789101112131415import pandas as pdimport numpy as npdf = pd.read_excel(r'D:\document\tableau_data\data_stu.xlsx',sheetname=0)print(df)df2 = df[['班级','数学','语文','英语']].groupby(by=['班级']).sum()print(df2)df2 = df2.rename(index=&#123;'1班':'1班-汇总','2班':'2班-汇总'&#125;)print(df2)def score(s): return '@&#123;:.2f&#125;@分'.format(s)df3 = df2.applymap(score)print(df3)]]></content>
      <categories>
        <category>Python-Pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas手册（4）- 对数据进行筛选和排序]]></title>
    <url>%2F2017%2F07%2F31%2Fpandas-handbook-04%2F</url>
    <content type="text"><![CDATA[PythonPandas 前几天看了篇教程：使用Pandas对数据进行筛选和排序 里面主要介绍了，我们在使用Pandas时，对数据进行筛选和排序的介绍这里简单总结分享下自己。 排序可能是版本的问题，原文中的sort函数没有了，变成了2个常用的函数 sort_index和sort_value 12345DataFrame.sort_index(axis=0, level=None, ascending=True, inplace=False, kind='quicksort', na_position='last', sort_remaining=True, by=None)Sort object by labels (along an axis)DataFrame.sort_values(by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')Sort by the values along either axis sort_index：按照索引排序，及列标签或行标签，axis=0是列标签，axis=1是行标签1234567df3 = pd.DataFrame(np.random.randn(6,4), index=list(range(0,12,2)), columns=list(range(0,8,2)))print(df3)print(df3.sort_index(axis=0,ascending=False))print(df3.sort_index(axis=1,ascending=False)) sort_value：按值进行排序，这个估计用的会多些，按数据内容进行排序123print(df3)print(df3.sort_values(by=[0],axis=0,ascending=True))print(df3.sort_values(by=[2],axis=0,ascending=False)) 排序后呢，我们可能有，只需要看到前10、后10这类的需求，需要用到另一个函数 head()、tail() 筛选筛选呢，我们上一篇介绍了loc和ilocPandas手册（3）-DataFrame-Selection By Label/Position 这里，我们实际应用下12345678910111213141516171819import pandas as pdimport numpy as npdf = pd.read_excel(r'D:\document\tableau_data\data_stu.xlsx',sheetname=0)print(df)#按照数学、语文，降序排列print(df.sort_values(by=['数学','语文'],ascending=False))#按照数学、语文，降序排列，取前3print(df.sort_values(by=['数学','语文'],ascending=False).head(3))#按照数学、语文，降序排列，取后3print(df.sort_values(by=['数学','语文'],ascending=False).tail(3))#筛选数学大于90的print(df.loc[df['数学']&gt;=90])#筛选数学大于等于90，且语文小于60的print(df.loc[(df['数学']&gt;=90) &amp; (df['语文']&lt;60)])#筛选数学或语文大于60分，爱英语排序print(df.loc[(df['数学']&gt;=60) | (df['语文']&gt;=60)].sort_values(by=['英语'])) 数据如下， 我们主要是多种过滤条件的整合使用，大家多练习就可以掌握了 附录（参考资料）使用Pandas对数据进行筛选和排序]]></content>
      <categories>
        <category>Python-Pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas手册（3）- DataFrame-Selection By Label/Position]]></title>
    <url>%2F2017%2F07%2F31%2Fpandas-handbook-03%2F</url>
    <content type="text"><![CDATA[PythonPandas 序这里主要介绍下，在DataFrame中一些筛选的操作，常用的有下面这些 熟练掌握上面的几个方法，操作DataFrame应该就足够了123456789101112import pandas as pdimport numpy as npd = &#123;'one' : pd.Series([1., 2., 3.], index=['a', 'b', 'c']), 'two' : pd.Series([1., 2., 3., 4.], index=['a', 'b', 'c', 'd'])&#125;df = pd.DataFrame(d)print('原始数据：\n',df)print('index 为a的数据：\n',df.loc['a'])print('index下标为2的数据：\n', df.iloc[2]) .loc函数，主要就是通过label来获取row数据 前面的例子，都是通过label来输出指定的行数据，其实也可以控制输出指定的列12345df2 = pd.DataFrame(np.random.randn(6,4),index=list('abcdef'), columns=list('ABCD'))print(df2)print(df2.loc['c':])print(df2.loc['d':,['A','D']]) 我们还可以实现更复杂的筛选我们只输出指定的列，label 为a的行数值大于-1且小于0的列1print(df2.loc[:,(df2.loc['a']&gt;-1) &amp; (df2.loc['a']&lt;0)]) #输出指定单元格数据print(df2.loc[‘a’,’C’]) .iloc函数就是通过下标来筛选数据 123456789df3 = pd.DataFrame(np.random.randn(6,4), index=list(range(0,12,2)), columns=list(range(0,8,2)))print(df3)#输出第2行print(df3.iloc[1])print(df3.iloc[:3])print(df3.iloc[3:5,1:3])print(df3.iloc[[1, 3, 5], [1, 3]]) 附录（参考资料）Indexing and Selecting DataSelection By Label]]></content>
      <categories>
        <category>Python-Pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas手册（2）- DataFrame]]></title>
    <url>%2F2017%2F07%2F31%2Fpandas-handbook-02%2F</url>
    <content type="text"><![CDATA[PythonPandas 序DataFrame是2维的标签数组，可以把他当成电子表格（Excel），数据库里的表，a dict of Series。DataFrame初始化，也可以有不同的输入， 在Series中呢，我们有一个index的概念，在DataFrame中，我们除了index，还有一个columns的概念index：行标签columns：列标签 DataFrame初始化1class pandas.DataFrame(data=None, index=None, columns=None, dtype=None, copy=False) 我们看到，这里有data，index，columns我们可以只初始化data，其他都默认123456import pandas as pdimport numpy as npdf = pd.DataFrame(np.random.randn(5))print(df) 我们看到，index，依然是下标从0开始，columns呢，也是从0开始的我们可以，初始化index，和columns12df = pd.DataFrame(np.random.randn(5),index=['i1','i2','i3','i4','i5'], columns=['a']) From dict of Series or dicts1234d = &#123;'one' : pd.Series([1., 2., 3.], index=['a', 'b', 'c']), 'two' : pd.Series([1., 2., 3., 4.], index=['a', 'b', 'c', 'd'])&#125;df = pd.DataFrame(d) 这里，我们定义一个dict，value是Series，就是我们给2个一维数组都加了一个key，然后把他们拼到一起，就成了一个DataFrame，key就变成了columns 为了让数据均匀，空的数据会默认为NaN这些都是默认的，我们也可以显示初始化index和columns1df = pd.DataFrame(d,index=['a','c','p','q']) 我们显示初始化index的话，会用我们指定的index去和data中Series的index去匹配，匹配上了就使用，匹配不上就剔除掉了，如上，只有’a’,’c’是有的，所以其他的都剔除掉了，像’p’,’q’是data中没有的，所以就是用NAN代替了columns也是同样的道理1df = pd.DataFrame(d,index=['a','b','c'],columns=['id','one','age']) 我们可以查看DataFrame的index和columns12print(df.index)print(df.columns) From dict of ndarray/lists123d = &#123;'one' : [1., 2., 3., 4.],'two' : [4., 3., 2., 1.]&#125;df = pd.DataFrame(d) index会默认初始化，range(n)这时，我们显示初始化index的时候，index的长度一定要和dict中ndarray的长度一样，不然，会报错1df = pd.DataFrame(d,index=['a','b','c','d','e']) From structured or record array12345data = np.zeros((2,), dtype=[('A', 'i4'),('B', 'f4'),('C', 'a10')])data[:] = [(1,2.,'Hello'), (2,3.,"World")]df = pd.DataFrame(data)print(df) 这里先定义了一个数据结构，（对numpy还不熟，希望没说错），2行，3列，分别指定了每一列的数据类型；然后进行初始化 这里的index是默认range(n)初始化的，columns是data中指定的名字，numpy中的数组后面也得学习下1df = pd.DataFrame(data,index=['ff','ss']) 我们显示初始化index的话，一定要和data中的行数一样，columns的话，会取data中名字一样的1df = pd.DataFrame(data, columns=['one','two','C','B','A']) From a list of dicts123data2 = [&#123;'a': 1, 'b': 2&#125;, &#123;'a': 5, 'b': 10, 'c': 20&#125;]df = pd.DataFrame(data2)print(df) 这里会将list中的元素做一个union，合并到一起去，如果显示初始化index，columns的话，index的长度一个要和list的长度一致，columns 的话，则会自动处理，有则显示，无则显示NAN（有无表示是否和dict中的key匹配的上）1df = pd.DataFrame(data2,index=['x','y'],columns=['a','b','c','d']) From a dict of tuples123456df2 = pd.DataFrame(&#123;('a', 'b'): &#123;('A', 'B'): 1, ('A', 'C'): 2&#125;, ('a', 'a'): &#123;('A', 'C'): 3, ('A', 'B'): 4&#125;, ('a', 'c'): &#123;('A', 'B'): 5, ('A', 'C'): 6&#125;, ('b', 'a'): &#123;('A', 'C'): 7, ('A', 'B'): 8&#125;, ('b', 'b'): &#123;('A', 'D'): 9, ('A', 'B'): 10&#125;&#125;)print(df2) 这个感觉类似Excel里面合并单元格的操作，就不多做练习了，啥时候用上了再说 From a Series这个和前面都比较类似，我们看个例子1234s = pd.Series(np.random.randn(5),index=['a','b','c','d','e'])print(s)df3 = pd.DataFrame(s,index=['x','y','a','b'])print(df3) 常用构造函数这里就不多说了，大家可以自行学习下 Column selection,addition,deletion这里主要是说对DataFrame中对column的一些操作123456789101112d = &#123;'one' : pd.Series([1., 2., 3.], index=['a', 'b', 'c']), 'two' : pd.Series([1., 2., 3., 4.], index=['a', 'b', 'c', 'd'])&#125;df = pd.DataFrame(d)print(df)#输出列名为one的数据print(df['one'])#给列three赋值，one列*two列df['three'] = df['one']*df['two']#给列flag赋值，one列的值是否大于2df['flag'] = df['one'] &gt; 2print(df) 删除操作123del df['flag']df.pop('three')print(df) 1DataFrame.insert(loc, column, value, allow_duplicates=False)[source] 关于DataFrame的其他操作，后面会再详细说明，这里就简单说到这了。 附录（参考资料）官方教程：DataFrame ———update at 2017-08-07 DataFrame使用后记 记录下DataFrame使用的小技巧 索引对象 pandas中的索引对象负责管理轴标签和其他元数据（比如轴名称）。构建Series或DataFrame时，所用到的任何数组或其他序列的标签都会转换成一个index。 索引创建后是不可以修改的12345678910111213141516171819202122232425262728obj = pd.Series([4, 7, -5, 3])objOut[144]: 0 41 72 -53 3dtype: int64obj.indexOut[145]: RangeIndex(start=0, stop=4, step=1)o_ind = obj.indexo_indOut[147]: RangeIndex(start=0, stop=4, step=1)o_ind[0]=9Traceback (most recent call last): File "&lt;ipython-input-148-f7ee6348297a&gt;", line 1, in &lt;module&gt; o_ind[0]=9 File "D:\Users\yugui\Anaconda3\lib\site-packages\pandas\core\indexes\base.py", line 1620, in __setitem__ raise TypeError("Index does not support mutable operations")TypeError: Index does not support mutable operations]]></content>
      <categories>
        <category>Python-Pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas手册（1）- Series]]></title>
    <url>%2F2017%2F07%2F30%2Fpandas-handbook-01%2F</url>
    <content type="text"><![CDATA[PythonPandas 要学习pandas了，，看官网上的资料还是很多的，就根据找到的资料简单总结下吧。这里也有很多同学分享的资料，这里都整理下，按照自己的理解整理下。 序这里的主要内容，参考官方教程：http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dsintropandas里面有3个基本的数据结构， 我们可以把Series，理解成一维数组，但是又和常规的一维数组不太一样。 Series是一维的标签数组，可以存储任意的数据类型（integers，strings，floating point numbers，Python objs，etc.)这里为什么是标签数组呢？因为他多了一个轴的概念，类似索引，我们往下看下就知道了。 Series初始化引入必要的类12import pandas as pdimport numpy as np 基本初始化语法：1s = pd.Series(data, index=index) 这个data，就是我们要初始化的数据，index，就是那个标签了，即索引data呢，常规可以为： from ndarray如果data是ndarray，index的长度必须和data的长度一样，或者保持默认，index会自动初始化，就是下标从0开始1s = pd.Series(np.random.randn(5), index=['a', 'b', 'c', 'd', 'e']) 这里呢，我们data的长度是5，我们index的长度也是5如果不是5呢，我们试试 这里是会报错的，少了不行，那多了呢？ 也是不行的，所以，如果初始化index的话，长度一定要和data一样当然，默认是可以的，1s = pd.Series(np.random.randn(5)) index默认初始化，从0开始 from dict如果data为dict，因为dict是key，value的，所以，默认初始化时，会使用key来初始化index 当然，我们也可以，显式初始化index 通过上面的例子，我们发现，如果指定的index没有包括所有的data中的key，那么就只显示index中有的；如果指定的index中有data中key没有的，那么就用NAN来赋值 from scalar value如果data是常量，那么我们必须初始化index 刚试了下，好像也不用，默认会初始化一个长度的 Series使用Series使用起来也很方便 Series is ndarray-like我们可以使用下标，1s = pd.Series(np.random.randn(5), index=['a', 'b', 'c', 'd', 'e']) Series is dict-like我们也可以像dict一样，使用index来操作Series 附录（参考资料）博客：1.1 pandas数据结构Series官方教程：Intro to Data Structures ——update at 2017-08-07 Series使用后记这里记录些Series使用上的心得Series使用起来，不仅可以使用下标来获取元素，也可以使用index来获取12345678910111213141516171819202122232425262728293031323334353637383940414243s = pd.Series(np.random.randn(5) , index=list('abcde'))sOut[89]: a -0.434789b -0.047950c -0.826720d 1.493415e 0.806696dtype: float64s[0]Out[90]: -0.43478889663783105s['a']Out[91]: -0.43478889663783105s[1:3]Out[92]: b -0.04795c -0.82672dtype: float64s['b':'d']Out[93]: b -0.047950c -0.826720d 1.493415dtype: float64s[[3,2,1]]Out[94]: d 1.493415c -0.826720b -0.047950dtype: float64s[['b','c','a']]Out[95]: b -0.047950c -0.826720a -0.434789dtype: float64 对于NaN值得处理,我们可以使用isnull，notnull来判断是否有NaN值123456789101112131415161718192021222324252627a = &#123;'lufei':10,'namei':30,'qiaoba':40&#125;s = pd.Series(a,index=['lufei','namei','qiaoba','suolong'])sOut[98]: lufei 10.0namei 30.0qiaoba 40.0suolong NaNdtype: float64s.isnull()Out[99]: lufei Falsenamei Falseqiaoba Falsesuolong Truedtype: bools.notnull()Out[100]: lufei Truenamei Trueqiaoba Truesuolong Falsedtype: bool Series的索引就可就地修改，直接使用s.index123456789101112s.indexOut[104]: Index(['lufei', 'namei', 'qiaoba', 'suolong'], dtype='object')s.index=list('abcd')sOut[107]: a 10.0b 30.0c 40.0d NaNdtype: float64]]></content>
      <categories>
        <category>Python-Pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小白学习Tableau-堆叠条按值排序]]></title>
    <url>%2F2017%2F07%2F30%2FTableau-handbook-03%2F</url>
    <content type="text"><![CDATA[序昨天学习了下堆叠条形图，刚刚看到个类似的教程，说的是，在堆叠条中按值进行排序，挺有意思的。 又学习到了一招，简单分享下。 按值在堆叠条中对段进行排序我们先实现一个简单的堆叠条， 使用维度：地区、类别 使用度量：销售额 下面，我们来看下，是怎样让每个类别的数据块按照销售额排序的。 我们观察下，上面的图，每个地区一个颜色，默认应该是按地区来排序的，每个数据条排序都一样。 我们首先，在颜色“地区”上右键单击，选择“属性” 这里的“维度”、“度量”我都可以理解，但是并没有找到“属性”在Tableau中是怎样定义的，按照以前对BI中维度模型的概念来理解的话， 比如，日期维度，包含3个层级，年、月、日，层级月的属性的话，可能是月份名称、月份中文名称等等。我们选完属性之后，会变成这个样子 然后，我们选中，维度“类别”和“地区”，创建一个合并字段 这个合并字段，是个什么东西呢？其实就是新增了一个联合维度，就是类别和地区的一个笛卡尔积，内容是这样的 然后，我们把这个合并字段，拖到详细信息上， 然后，我们在合并字段上，选择排序 我们，依次，选择“降序”，“按销售额来” 最后结果，是这样的 实例呢，在Tableau Public上，练习04-堆叠条按值排序 最后的话，后面还得详细理解下这里用属性和合并字段一起使用的这个操作，有领悟的话，会在分享。 附录官方教程：按值在堆叠条中对段进行排序]]></content>
      <categories>
        <category>Tableau</category>
      </categories>
      <tags>
        <tag>数据可视化</tag>
        <tag>Tableau</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小白学习Tableau-堆叠条形图]]></title>
    <url>%2F2017%2F07%2F30%2FTableau-handbook-02%2F</url>
    <content type="text"><![CDATA[序昨天在看水足迹那个可视化题目的时候，就想做一个堆叠条形图，但是发现只有一个维度，怎么也拖不出来，后来改了下数据源，成功实现了。今天搜到个例子，发现了解决办法，只能说明，还是对Tableau不熟啊，没有能领悟Tableau的内涵。 堆叠条形图教程中介绍了2种方法，我们都来实践一下。这里面，最大的收获，就是原来那个“度量名称”和“度量值”是可以拖拽过去使用的，我也是醉了，这个操作得好好研究下，理解下Tableau的机制。 这里的度量名称，应该就是所有度量的一个集合；度量值应该就是“度量名称”集合中选定的度量。 这里我们就用 http://www.makeovermonday.co.uk/ 上的数据 对每个维度上使用一个单独的条首先，我们将“食物”维度，拖到列上， 然后，我们需要观察的是3种水足迹的值，并让他们显示成堆叠图的样子 我们把度量名称，拖到颜色上 因为，我们只看3个水足迹的量，我们就筛选下 （这个颜色是我之前设置过的，大家操作完，颜色可能和我不一样，但样式是一样的）就这样，我们完成了，列上是每一种食物，然后行上面是选定的3中水足迹，一个简单的堆叠图就完成了。 对每个度量使用一个单独的条这个操作其实就是和上面的操作的相反我们将度量名称拖到列上，并筛选 然后，将度量值拖到行上 最后，将维度，食物拖到颜色上 这样，我们就在3个度量值上，看食物维度的一个堆积情况。好了，堆积图的例子就完成了，主要是理解下Tableau的思想。 小例子，发布在Tableau Public上：练习03-堆叠条形图 附录（参考资料）官方教程：使用多个度量创建堆叠条形图]]></content>
      <categories>
        <category>Tableau</category>
      </categories>
      <tags>
        <tag>数据可视化</tag>
        <tag>Tableau</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小白学习Tableau-标签云]]></title>
    <url>%2F2017%2F07%2F30%2FTableau-handbook-01%2F</url>
    <content type="text"><![CDATA[序周末参加了2天的Tableau培训，发现这个东西做可视化分析还是很方便的，用户体验也很好。 也听了很多大师的介绍，受益良多。不管怎样，要开始学些这个Tableau了。之前也有做过IBM Cognos的开发，2个产品对比一下，从直观上来看，就是Tableau的可视化效果好很多，而且还融合了一些分析功能，挺不错的。很久之前就知道了Tableau其实，只是一直都没有去研究她。学习的一开始呢，先是模仿，熟练掌握了Tableau之后，就可以任意发挥了。Tableau入门的话，官方就有很多的资料，挺全的，还有视频教程。 标签云之作这个比较简单，主要参考了jiyang大神的教程，后面有链接我们使用Tableau自带的示例库 我们主要就使用“类别”、“子类别”、“销售额” 我们把子类别拖到文本上 这里会显示，所有的子类别，然后呢，我们希望子类别可以根据销售额的大小而控制大小 把销售额拖到标记-大小上 默认的话，可能会显示这个树状图，但是我们并不想要这样，我们想要显示文本 我们修改下，我们把自动，修改为文本 好了，已经可以按销售额的大小来显示子类别的内容了当然，我们一般见到的标签云，都是有颜色的，那我们就继续用销售额的大小来控制颜色 将销售额拖到颜色上 这样呢，我们就用销售额控制了文本的大小和颜色， 一个简单的标签云就实现了 好了，这个例子就是这样简单，小例子，发布在Tableau Public上， 练习02-销售额标签云 附录（参考资料）参考学习jiyang大神的作品：https://public.tableau.com/profile/jiyang#!/vizhome/_3516/sheet0]]></content>
      <categories>
        <category>Tableau</category>
      </categories>
      <tags>
        <tag>数据可视化</tag>
        <tag>Tableau</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BeautifulSoup教程（2） - 实例-解析博客专栏]]></title>
    <url>%2F2017%2F04%2F20%2Fbs-handbook-02%2F</url>
    <content type="text"><![CDATA[前几天学习了下Beautiful Soup的使用，本来想多写些内容的，但是发现，官方的介绍实在太详细了，每种方法基本都覆盖到了， 直接看官方的例子就足够了，而且还有一个中文版的，这里的话，就简单实践下，介绍几个常用的方法和一些小经验。 官方文档：https://www.crummy.com/software/BeautifulSoup/bs4/doc/ 这里，我们就简单的解析下博客专栏，https://blog.hellobi.com/ 这里的话，没有做太多的限制，我们直接解析就可以 1. 分析网页我们再Firefox中，使用 Firebug很方便 这里的话，文档结构也很清晰，很适合我们练习 基本代码，我们获取网页信息12345678910111213# -*- coding: utf-8 -*-import urllibfrom bs4 import BeautifulSoupimport sysreload(sys)sys.setdefaultencoding('utf-8')#加载网址，获取当前页面def getHTML(url) : page = urllib.urlopen(url) html = page.read() return html 2.常用类 123html = getHTML('https://blog.hellobi.com/')soup = BeautifulSoup(html, "html.parser") 这里，我们初始化Beautiful Soup，使用“html.parser”,这是一个默认的解析器，他还有其他的解析器，官网有介绍，这里就不说了，其他也没有测试过 这里，顺便说下Python中查看类的帮助信息，使用dir()函数，可以查看类的属性和方法列表12print type(soup)print dir(soup) 我们还可以使用help函数，查看详细的帮助信息 1print help(soup) Beautiful Soup是最基础的类，其他的还有Tag，这个用起来和HTML中的标签类似 12345print soup.titleprint type(soup.title)#print soup.headprint type(soup.head) 这里，的title，head都是HTML中的tag，也是Tag类的对象 123print soup.title.stringprint type(soup.title.string)print dir(soup.title.string) 然后，我们需要使用Tag的内容时，就用到了NavigableString 3.常用函数123456789101112131415161718blog_index=1for blog in soup.find_all('div', class_='blog-item'): print '----------------------------------------------------------' print '第%s篇博客' %(blog_index) title = blog.select_one('div.caption &gt; h2 &gt; a') print '博客标题: ',title.string author = blog.select_one('div.caption ul a') print '作者: ',author.text.strip() vote = blog.select_one('div.cp div.blog-votes span') print '推荐次数: ',vote.string info = blog.select_one('div.cp div.blog-views span') print '阅读次数: ',info.string blog_index+=1 上面的代码，就是遍历的首页的所有博客信息，很简单，就是一个find_all()和一个select_one() find_all() select_one() select() 方法中传入字符串参数,即可使用CSS选择器的语法找到tag: 思想其实都一样，就是根据一定的规则，找到我们想要的标签 后面的话，我们再找到总页数，遍历一下，就可以获取所有的博客基本信息啦1https://blog.hellobi.com/?page=2 传递页码就可以了 好了，这里就举这一个简单的小例子，大家多在实践中使用就好了]]></content>
      <categories>
        <category>Python-爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>BeautifulSoup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BeautifulSoup教程（1） - 简介及安装]]></title>
    <url>%2F2017%2F04%2F14%2Fbs-handbook-01%2F</url>
    <content type="text"><![CDATA[最近在学习Python，按照一些博客练习爬虫，最简单的步骤，就是访问一个主页，根据正则表达式去获取我们想要的标签数据； 比如这样：1234567891011#加载网址，获取当前页面def getHTML(url) : page = urllib.urlopen(url) html = page.read() return htmldef getImage(html) : reg = r'src="(.+?\.jpg)"' reg2 = r'&lt;img alt="(.+?)" src="(.+?\.jpg)' image_reg = re.compile(reg2) img_list = re.findall(image_reg,html) 简单的话，这样还好，如果复杂些的话，像我一样对正则表达式不熟悉的话，可能就不太好实现了， 后面发现这个beautifulSoup解析HTML很方便，这里简单学习下， 官网地址：https://www.crummy.com/software/BeautifulSoup/ 还有中文文档：https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html 1. 简介这里说的不错，Python爬虫利器二之Beautiful Soup的用法 2. 安装Python里面安装东西很方便，直接使用pip就行了 pip install beautifulsoup4 3.小例子我们先写个小例子看看123456789101112131415161718 # -*- coding: utf-8 -*-import urllibimport refrom bs4 import BeautifulSoup#加载网址，获取当前页面def getHTML(url) : page = urllib.urlopen(url) html = page.read() return htmlhtml = getHTML('https://movie.douban.com/top250')soup = BeautifulSoup(html, "html.parser")for img in soup.find_all('img'): print img.get('src') 这里，我们就输出了所有的img标签 后面，我们再来继续练习使用]]></content>
      <categories>
        <category>Python-爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>BeautifulSoup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（十二）- 控件使用-从步骤插入数据]]></title>
    <url>%2F2017%2F04%2F14%2FKettle-handbook-12%2F</url>
    <content type="text"><![CDATA[这里介绍一个控件的小功能，也是最近才发现的，之前在“表输入”中要使用参数的话，一般都是使用变量，其实，还有个功能也可以尝试使用整体流程就是这样，我们第一个 query_paramter，就是查询了我们想设置的参数然后，就是我们真正需要的，我们再表输入中，使用 “?”来占位，然后“从步骤插入数据”，选择上一个步骤，然后会将数据替换占位符最后，我们将文件导出即可，奥对了，我们可以改成日志控件，直接输出查看刚刚，上面还有一个“执行每一行”，这个就是，如果我们有多个参数，就可以使用这个参数了，很方便，好了，就介绍到这里先。]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（十一）- 用PGP加密、加密文件]]></title>
    <url>%2F2017%2F04%2F11%2FKettle-handbook-11%2F</url>
    <content type="text"><![CDATA[看到有同学提问，以前也没用过，百度了一下，找了些资料，这里记录下。 1. 安装gpg4win这个gpg4win是干嘛的呢，我们可以去他的官网看看：gpg4win目前，只知道他是加密的，这个是对Windows平台使用的这里可能还有个PGP的概念，看看百度百科 好了，具体概念，大家可以自行找找，我们下载下来，然后安装一下即可这个是昨天安装的，就不粘贴步骤了，安装完后，我们要先创建一个证书的东西，我们打开这个管理界面打开后，是这样一个界面，（网上有这个的安装配置教程，这里也简单介绍下，不清楚的可以再百度看看）我们新建一个Certificate我们选择一个加密方式，使用第一个就可以了我们输入些基本信息然后next就可以然后，我们得输入一段密钥好了，这里，就配置完成了 2. 用PGP加密文件好了，这里，我们新建一个作业，我们主要使用这2个控件一个很简单的流程，我们做些简单的配置，一个是GPG的目录（就是我们上面安装的那个）还有就是，我们的要加密的文件和一个目标文件名，注意，这里我们得填写一下“用户ID”，就是我们前面新建的那个用户名就可以了这里，可以勾选一下，目标是一个文件好了，然后，我们执行下就可以了我们源文件：加密后的文件：下面，我们再看看，怎样解密 3. 用PGP解密文件知道了加密，解密也是一样的，这里的话，配置和上面差不多，这里，我们要填写一个“密钥”，就是我们上面创建时，输入的一个密码我们运行一下，解密后，是一样的好了，就简单介绍到这里]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（十）- 跨库查询]]></title>
    <url>%2F2017%2F04%2F10%2FKettle-handbook-10%2F</url>
    <content type="text"><![CDATA[Kettle整体使用起来，还是很方便的，熟悉应用了之后，就是对控件的熟悉和使用了，只要思路有了，就是整合下Kettle中各个控件的使用就行。这里，简单介绍下一个“跨库查询”的控件。有的时候，我们一个脚本，可能只是临时性的，或者需要实时的去查一下，同步到数仓的话，可能不太方便，我们就可以使用跨库查询的控件用到的表信息 1. 数据库连接(Database Join)我们先用这个控件来实现一下用起来也很简单表输入：是我们第一个库中的SQL数据库连接：是我们另一个库的SQL我们用关联的字段放在where条件后，使用“?”来占位，并在下面，选择要传入的参数默认的话，是JOIN，我们也可以勾选Outer Join，然后，我们看下，输出就行这是后面导出的文件，这里，我们就简单实现了跨库的查询 2. 数据库查询我们再来看另一个控件，“数据库查询”，这个控件同样可以实现跨库，但是有一个小问题首先，我们使用上一次的数据来看我们执行下，结果看上去是一样的这其实有个隐藏的问题，我们再增加几条记录看看比如：现在1号有2条记录，正常的话，我们导出也是要有2条的我们执行下看看我们会看到，数据并没有增加，这是控件导致的，先获取左边的结果集，然后一条一条去右边匹配；匹配到第一条记录后，就会跳出，直接去匹配下一个，所以，我们有2条记录，也只会找到第一个。这并不是我们想要的，我们再试下第一个控件使用这个“数据库查询”控件的话，可以通过将1-N关系汇总，将N的一方，放在前面最后的结果也是可以的]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（九）- 发送邮件]]></title>
    <url>%2F2017%2F03%2F30%2FKettle-handbook-09%2F</url>
    <content type="text"><![CDATA[在Kettle里面，我们每天执行完调度之后，想要监控下JOB的执行状态，通常我们可以会发送邮件，可以的话，还可以发送短信。 在Kettle里面，发送邮件很方便，这里，我们就简单的测试下。 1. 在作业中发送简单邮件我们只需要使用到这个控件就可以了，这样，一个简单的发送邮件流程就好了 控件的配置：收件人，抄送啊，信息，自行填写就行，多个收件人，使用“空格”分隔在服务器这里，我们填上服务器的信息就可以了这里是邮件消息的一些配置，暂时先到这里，我们测试下结果然后，查看邮箱，我们会接收到这个邮件，刚刚简单测了下这个“回复名称”，就是这里试过中文，会有问题，有乱码，可能是Windows下的原因，没有再去测试验证就是收到邮件时的一个发件人的名称，不同邮箱显示的不一样 2. 增加附件附件的话，也很简单，上面的面板中直接配置就可以了然后，我们需要将待发送的邮件，添加到结果集中在控件中，我们添加好文件就行了。我们再次发送，验证下好了，附件也可以了，思路就是这样的，实际应用时，可能还有些问题得注意下 3. 自定义邮件内容到这里，我们会看到，邮件的正文内容，可能并不是我们想要的， 我们想要的可能是这样的信息这就需要自定义正文内容，我们需要勾选下面这个选项这里是可以使用变量的，我们可以拼接HTML来实现好了，邮件的介绍，大概就这些，在转换中，也是可以使用的，大同小异]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（八）- 循环]]></title>
    <url>%2F2017%2F03%2F29%2FKettle-handbook-08%2F</url>
    <content type="text"><![CDATA[有的时候，我们想要在Kettle中实现这个循环的功能，比如，批量加载数据的时候，我们要对10张表执行同样的操作，只有表名和一些信息不一样，这时，写个循环就省事儿多了 1. 遍历结果集实现这里的话，我们主要是通过一个将结果集返回，然后通过转换的设置来实现的 1.1 query_the_result这个转换，只要是将我们要遍历的结果集返回，表输入，我们就是返回了5条记录，来做遍历复制记录到结果，这个控件的作用，就是我们可以在后面的转换继续使用这个结果集。 ##1.2 traverse_the_result这里呢，我们就是需要遍历的转换了，这里，我们只是获取结果集，然后将结果集输出还有一个很重要的一步，怎样让这个转换可以根据结果集的条数，去循环执行呢？就是这个“执行每一个输入行”我们执行下看看 2. 使用JS实现网上有很多的例子，介绍怎样用JS来控制循环，这里我们也简单的测试下 2.1 query_the_result这一步，和上面的一样，就是将结果集返回 2.2 travers_the_result这里主要是使用JS将结果集进行遍历，通过JS，将一些结果存放到变量里面，在后面的操作中就可以使用了，通过${xxx}的方式使用这个其实和Java、JS里面循环思路一样，通过结果集的总数“total_num”和下标“LoopCounter”进行判断 2.3 evaluate_the_loop_count这一步，就是判断下标的值和结果集的总数，进行对比， 2.4 print_the_log输出下，我们想要使用的变量 2.5 manage_the_loop_index这一步，给下标加一，然后获取下一条记录好了，执行下，我们看看好了，循环的使用先介绍到这里]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（七）- 资源库的使用]]></title>
    <url>%2F2017%2F03%2F29%2FKettle-handbook-07%2F</url>
    <content type="text"><![CDATA[1. 为什么使用资源库之前，我们新建转换或者作业的时候，都是直接保存在本地，而如果我们是多人开发的话，除了使用SVN等版本控制软件，还可以使用Kettle的资源库，他会将转换、作业直接保存在数据库中，而且，连接资源库的话，我们就不需要每一次都新建数据库连接了，用起来还是蛮方便的。 2. 新建资源库Kettle7.0里面，是在右上角这个Connect来连接的 2.1 资源库的类型资源库有3中类型Pentaho RepositoryDatabase Repository（使用数据库存储）File Repository（使用文件存储） 2.2 新建Pentaho Repository我们单击上面的get started 之后，就会进入新建界面http://localhost:8080/pentaho一开始还没搞懂这个Server到底怎么启动，后来google了半天发现后来又找到了这个，应该是要安装其他的组件才行，这个类型的库就放弃吧。。 2.3 Database Repository好了，这回，我们选择哪个database的资源库我们填一个connection的名字，然后配置一个资源库的连接就可以了，最好给kettle新建一个数据库使用至于数据库连接的话，和转换里面是一样的，大家可以自行新建一个配置好，以后，大家选择Finish就可以了，然后，我们可以连接下这个库，注意下，这里的用户名和密码，默认是admin/admin，大家直接登录就好了，这是Kettle自己初始化的这个怎么改呢，暂时还没有发现，待研究，等我再google看看，估计官网上会有。（找了下，发现了在哪改密码，就是刚刚的搜索资源库)连接后，我们正常使用就好了，没啥两样，会多一些功能，比如，探索资源库这里我们再保存作业和转换的话，会直接保存在数据库中，而且，很好的一个功能，个人感觉，就是数据库连接只需要创建一次，在哪里都可以用了，不需要再次创建。 2.4 File Repository这个和database的资源库，就差不多了，只不过是基于文件的，保存在本地就可以了这个就和Eclipse一个工作区差不多，转换、作业都保存在这个目录下好了，关于资源库，就简单的说这些了，大家可以自行连接，试试。]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（六）- Hop小记]]></title>
    <url>%2F2017%2F03%2F29%2FKettle-handbook-06%2F</url>
    <content type="text"><![CDATA[1. 什么是Hop在我们前面，使用Kettle过程中，控件与控件之间的连线，这里，我们详细介绍下它，它在Kettle中叫Hop（跳）。 2. Hop的发送方式（转换）在转换中，一般情况，控件和控件之间只有一个Hop，当然，如果需要的话，我们拖了2个控件出来，像这样：Kettle会提示你，下面的信息，让你选择，数据发送的方式 2.1 分发记录目标步骤轮流接收记录，其实就是你一条，我一条，轮着接收数据，这个我们试一下就知道了，我们执行下，看看这个结果试试，我们再步骤度量中，可以看到，a.txt和b.txt分别写入的数量看看结果文件，就是这样的 2.2 复制记录所有记录同时发送到所有的目标步骤，这个看起来就简单多了，比如上面的例子，2个文本文件会接收到同样的所有的数据，我们也试一下结果文件的话，就是2个节点，接收到的数据都是一样的 3.Hop的状态（作业）在作业中，Hop主要用来控制流程有3种状态，一个锁，一个绿色的对号，一个红色的叉号简单来说，：表示无论上一步执行成功还是失败，都一定会执行下一步：表示上一步执行成功才会执行下一步：表示上一步执行失败执行下一步比如我们上面的例子，我们的转换执行成功后，就结束了，如果转换执行失败了，我们就发送邮件。]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（五）- 实例-增量同步数据]]></title>
    <url>%2F2017%2F03%2F28%2FKettle-handbook-05%2F</url>
    <content type="text"><![CDATA[综合前面的几个例子，我们这里来是实现下增量数据的同步。这里只是分享一种方法，实际工作中，还会有其他更好的方案。增量同步的整体思路一般就是：首先拿到这张表的增量数据，怎么拿增量呢，源表需要有一个时间字段，代表该条记录的最新更新时间（及只要该条记录变化，该时间字段就会更新），当然有时间字段最好了，没有的话，可能需要做全表对比之类的操作；正常情况下，业务系统的表中都是有主键的，我们拿到增量数据之后，需要判断该记录的新插入的，还是更新的记录，如果是更新记录，我们需要先将数据加载到中间表，然后，根据主键将目标表中已存在的数据删除，最后再将本次的增量数据插入到目标表。 1.配置表的设计（元数据表）首先我们需要一张配置表，来保存我们要增量同步的表的基本信息1234567--元数据表create table tm_etl_table( table_name varchar(50), --表名 is_run int , --调度状态 update_time timestamp,--表数据更新时间 etl_insert_time timestamp --记录更新时间); 我们初始化一条记录，我们就以这张ods_tm_book表一些基础表准备 12345678910-- 源表create table tm_book(id int,book_name varchar(10),latest_time timestamp);-- 源表数据初始化insert into tm_book(id,book_name,latest_time)select x,x||'_name',clock_timestamp() from generate_series(1,10) x;-- 目标表和中间表create table ods_tm_book(id int,book_name varchar(10),latest_time timestamp,etl_insert_time timestamp);create table staging_tm_book(id int,book_name varchar(10),latest_time timestamp); 源表中的数据 2.同步数据的流程开发整体流程是这样的，注意下，这个只是为了简单演示了这个增量的例子，实际应用的话得修改，这是有漏洞的。 2.1更新元数据表的状态并获取表更新时间就是我们第一个状态，我们更新tm_etl_table表，更新is_run=0，表示我们开始同步数据了，update_time，初始化为 ‘1970-01-01’，表示我们要拉取所有的数据这里，我们将该表的更新时间作为变量，我们会在后面的转换中使用 2.2 加载数据到中间表我们这里，直接表对表，将数据插入到staging其中，表输入中，我们需要根据前面的更新时间变量，获取增量数据，注意，需要勾选上“替换SQL语句中的变量”这里，我们直接就表输出到中间表，每次都需将清空表数据 2.3 加载数据到目标表这里，主要有3段脚本（为了方便，就这样吧），根据主键ID，清空目标表数据，然后，将数据插入到目标表，最后，更新tm_etl_table表中的记录状态好了，用Kettle实现一个增量的逻辑大概就是这样了， 3.小结这里整理几个问题 3.1 中间表这里的话，使用了中间表，Kettle中是有一个控件的，应该叫那个“插入/更新”，可以根据主键将数据更新掉，这个控件之前使用时，发现很慢，就一直没用，后面的话，可能会写个例子，简单测试看看。使用中间表，缓存下数据，也是不错的方法。 3.2 增量流程目前公司中，增量抽取，是这样的，首先各个业务系统的数据导出到文本文件，然后批量将文件加载到数据仓库中（这里使用循环加载的）。因为每天的数据量比较大，如果知己到表的话，会很慢，使用文件，一些数据库都有批量加载的命令，很快很方便，比如：PostgreSQL中的copy命令，Greenplum中的外部表，还有Mysql中的load data等等。]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（四）- 变量的使用]]></title>
    <url>%2F2017%2F03%2F28%2FKettle-handbook-04%2F</url>
    <content type="text"><![CDATA[我们在这一回，介绍下，Kettle中全局变量的使用，我们前面说过的配置文件，其实就是配置全局变量的地方Kettle手册（三）- 配置文件的使用及密码加密 1. 全局变量 就是我们上面说的kettle.properties文件，我们在里面定义的变量，我们可以在所有的转换或者作业中获得到，比如，我们前面，说的数据库参数之前，我们已经在数据库连接中测试过，是可以，这里，我们输出下这个变量，看看 1.1 输出变量的值我们这里，用到了“获取变量”这个控件我们单击，”Get Variables”,就可以获取到当前的全局变量信息我们选择几个输出试试还有一个，”日志“控件，拖好之后，我们直接执行，日志中，我们会看到，我们定义在文件中的参数（加密的参数，我没有重启，所以显示的还是原来的）那我们，可不可以，动态的增加变量呢？ 1.2 动态增加变量刚刚也在网上找了些资料，尝试了下，这里简单分享下（貌似，这得算是对局部变量的操作，暂时就放在这里吧）我们先试下在转换中设置变量，作业中也是可以使用的，我们后面再说测试流程是这样的， 我们再表输入中，有2个时间参数，然后作为变量比如，有这样一个场景，我们每天需要定时调度一些SP，SP都有开始时间，结束时间，调用时，需要传参数进去，这个时候，我们在使用Kettle的时候，就可以通过这样的方式，去设置变量，然后再调用SP我们单击获取字段后，就可以了，这里可以修改变量存在的范围执行后，输出，后面，我们就可以使用这2个时间变量了这里使用的时候，也遇到一个问题，就是变量的默认值，一直都没有生效，不知道为什么，不管是，静态值，还是变量值，都没有办法，待研究。 2. 局部变量（命名参数） 在kettle中，相对于全局变量，我们还可以使用局部变量。感觉，这个全局变量，局部变量，都是相对而言的，就网上大部分资料来说，Kettle中的局部变量就是“命名参数”我们再转换中，右键单击，选择，转换设置 我们选择，“命名参数”，定义一个变量，我们给一个默认值然后，在日志中，将变量输出我们执行下，这个转换，运行时的界面，我们可以看到，这个参数是可以动态改变的，或者，我们再命令行调这个转换的时候，同样可以给他赋值运行结果，这个就是简单的局部变量了]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（三）- 配置文件的使用及密码加密]]></title>
    <url>%2F2017%2F03%2F28%2FKettle-handbook-03%2F</url>
    <content type="text"><![CDATA[好了，我们上一回，练习了一个从数据库导出数据到Excel的例子，我们想一下，如果有很多个转换，我们没链接一次数据库，是不是都需要重复的输入那些数据库地址啊，数据库啊，用户名啊之类的。其实是不用的，我们可以使用变量的方式，写在配置文件中，下面，我们来看看。而且，我们平时开发，都有开发环境、UAT环境、生产环境，连接的地址都不一样，也不可能手动的去修改。 1. Kettle的配置文件 配置文件在哪呢？Windows下，是再当前用户的目录下，一般再C盘，Users下面，有一个当前用户的文件夹，下面有.kettle文件夹进入之后，我们会看到一个kettle.properties的文件，我们的数据库配置信息，就可以放在这里， 我们打开之后，编辑一下保存后，我们要重新启动下Kettle，因为这个配置文件是启动时加载的重启后，我们将上一次，配置的转换打开，使用变量替换下之前的配置，Kettle中，我们使用${xxx}，表示引用一个变量，执行时，会自动替换我们测试下，同样时可以成功的。好了，这样，以后，不管是，数据库地址变化，还是部署生产，我们只需要修改配置文件就可以了。 2. 密码加密 这里，顺便说下，加密的问题，比如，我们上面的数据库密码，是明文的，这样是不太安全的，而实际上，我们都是需要对密码进行加密的我们进到Kettle的安装目录我们会看到，这里有一个Encr.bat，这就是可以加密的脚本使用方法我们输入1Encr.bat -kettle postgres 执行后，会生成，这样一个加密后的密码，然后，我们可以使用这个加密后的字符串，替换我们的密码1pg_password = Encrypted 2be98afc86aa7f2e4cb79ff228dc6fa8c 大家可以试下，这样也是可以的，好了，这个例子就到这。]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（二）- 将数据导出为Excel]]></title>
    <url>%2F2017%2F03%2F27%2FKettle-handbook-02%2F</url>
    <content type="text"><![CDATA[好了，我们先来看第一个例子，就是怎样将数据库中的数据，导出为Excel。平时，如果我们需要将数据导出Excel的话，我们可能会直接复制，然后粘贴出来，但是数据量大的话，就不好用了；或者使用Java等开发语言，写代码，导出Excel；或者一些数据库连接工具自带的导出功能。其实，我们用Kettle的话，还是很方便的，但是平时用下来，Kettle的这个功能还是有些缺陷的，比如导出Excel2007+的时候，经常会报错，我一直也没有解决，这次记录博客顺便研究看看。 1. Kettle的下载及使用 正式开始之前，我们简单说下Kettle的安装配置啥的，Kettle是绿色的，下载之后，直接运行就可以了刚刚在网上下了个最新版的，后面，我们就是用这个7.0版本介绍官网地址：Kettle官网 他这个网站，应该是不太好访问，有VPN的话，可以用起来，下载的话，大概800M左右，后面看看上传一份，昨天为了下载，现冲了个蓝灯的会员解压以后，目录大概是这样的，我们会看到，这里有.bat文件和.sh文件，.bat就是我们在windows下使用的，.sh就是在Linux下使用的，我们找到 Spoon.bat这个文件，就可以启动Kettle了，奥，对了，得先安装下Java打开后，就是这样了，都是图形界面的，很好用Kettle中，主要有2中任务，一个是作业，一个是转换。一般来说，转换是一系列具体的操作，比如：调度SP，导出Excel等等；作业的话，就是按照一定流程来调度一系列转换。大概是这样，实际上，他们也是可以嵌套调用的，我们后面可以再讨论。 2. 第一个转换-将数据导出为Excel 为了实现这个功能，我们需要： 连接到数据库 导出为Excel 首先，我们新建一个转换，新建，之后，我们可以看到，工具箱中，有很多的控件，我们都可以使用，很多我也没有用过，大家可以自行去尝试使用好了，下面，我们就开始介绍我们这次的主题，导出数据到Excel既然，是导出数据，说明我们肯定有一个源头，一个目标，源头是我们的一个数据库，我们得先连接到这个数据库 新建数据库连接我们在主对象库中，DB连接上，右键单击，新建在这里呢，我们可以看到，有很多的数据库可以选择，我们只需要填写基本的连接信息就可以了我们这里连接的是Postgresql，配置好后，测试下，（坑，刚刚在windows上装的数据库，一直连不上，白名单都加好了，就是不行，结果是防火墙忘关了。。）好了，可以连接到数据库了，下面，我们得把数据导出啊，我们需要使用输入这个控件输入下面，有很多的控件，我们这次只使用表输入，因为我们是直接从数据库中拿数据这里直接就是拖拽的，拖过去就行了，双击之后，可以编辑，这里我们就使用刚才的数据源连接，然后查询一张表，表的话，随便create一张就可以了，我们还可以预览数据源头好了，同样的思路，我们需要一个目标，就是输出了，输出到Excel同样的，我们托好之后，双击就可以编辑了，这里，我们主要关注2个配置，一个是excel保存地址，和字段我们选择一个地址，然后得，看下字段那个tab，我们单击，获取字段，就可以从源头获取表中的字段了，当然，我们可以只导出，我们需要的字段，一步一步来的话，上面获取，可能会获取不到，因为，有一步，需要将2个控件，连起来，源头有了，目标也有了，得让他们关联起来啊，再Kettle中，这个连线叫做Hop（跳），就像一个管道一样，将数据流从一个点，指向另一个点。都好了，以后，我们就运行下和Java里面，一样，绿色的话，就代表成功了我们看下文件好了，我们的第一个例子，就成功了，还是很简单的，主要就是Kettle中控件的熟悉。]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（一）- 序及Kettle简介]]></title>
    <url>%2F2017%2F03%2F27%2FKettle-handbook-01%2F</url>
    <content type="text"><![CDATA[1. 序 好久没有写博客了，新的一年总得留下点儿什么。目前主要负责数据仓库这一块任务，平时用用Kettle、SSIS这类ETL工具，而且工具的使用整理起来会方便些。所以先从Kettle开始，一点点整理下最近BI开发中掌握的知识。以前有做过BI报表Cognos开发还有些入门级的Java，都在CSDN博客上，感兴趣的同学可以去看看：于贵洋的博客好了，下面就根据自己的经验和理解，整理下Kettle的知识。 2. Kettle简介 Kettle这东西是干嘛的呢？Kettle是一个开源的ETL工具，所以基本的数据抽取、转换、加载，他都可以。比如：我要把一个mysql数据库的数据同步到一个Postgres数据库，我们有哪些办法呢?大概会有: 将数据导出为文本文件，使用PG的copy命令直接加载 数据量少的话，直接拼接成insert脚本，批量插入 一些开源的小工具，提供2种数据库直接的同步 Kettle 等等方法再比如：我每天需要统计一些系统中的异常数据，导出为Excel，用邮件发送给指定的开发人员处理，该怎样做呢？ Java或者其他开发语言做定时任务 Kettle 和其他的ETL工具相比，他有什么优势呢？ Kettle是基于Java开发的，是开源免费的，大家可以直接在网上下载；跨平台，Windows，Linux都可以使用；使用起来简单快捷。 既然开源，相比于其他收费产品，劣势也就很显然了，比如稳定性啊，BUG修复处理啊，而且基于Java，性能上会差些。当然都是相对来说，一般数据量使用或者逻辑不复杂的话，使用起来是很适合的。 刚刚也在社区上，发现了Kettle的视频，kettle视频，大家可以看看，应该用的到。Kettle的基本介绍就这些，后面会根据实际的例子，来介绍下Kettle的使用。]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
</search>
