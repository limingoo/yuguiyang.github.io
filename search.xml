<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[博客迁移整理中(置顶)]]></title>
    <url>%2F2017%2F12%2F31%2Ftop%2F</url>
    <content type="text"><![CDATA[Hi，欢迎访问我的博客。最近正在迁移文章到这里，不定时更新中，请期待。]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书笔记-数据分析实战(1、2章)]]></title>
    <url>%2F2017%2F09%2F01%2Freading_notes_data_analysis_01%2F</url>
    <content type="text"><![CDATA[读书笔记 《数据分析实战》 1. 什么是数据科学家书中通过“什么是数据”和“数据在商业中的应用”，推导出数据科学家的定义。 人们通过观测数据来推测出某种因果关系，再用这种因果关系来预测未来或者控制原因以达到预期的结果。把从事这种工作的人成为数据科学家。– 书中摘录 上面的定义觉得不是很清晰，就百度上找了找： 数据科学家是指能采用科学方法、运用数据挖掘工具对复杂多量的数字、符号、文字、网址、音频或视频等信息进行数字化重现与认识，并能寻找新的数据洞察的工程师或专家(不同于统计学家或分析师)。一个优秀的数据科学家需要具备的素质有：懂数据采集、懂数学算法、懂数学软件、懂数据分析、懂预测分析、懂市场应用、懂决策分析等。– 百度百科 我觉得数据科学家就是对于数据相关的所有门类都有一个整体的认识，感觉是个“杂家”，精通算法、什么深度学习、机器学习、AI之类的都是信手拈来，对我就是神一样的存在了，努力吧，同学。 2. 3中类型的数据科学家书中将数据科学家分成了3类，主要从所在领域分类： 商业领域出身 统计学出身 工程领域出身 这应该也是数据科学家成长的3条路线，从不同的路线出发，最终殊途同归。当然，这3个领域需要综合，才称得上是合格的数据科学家。 书中的技能配图，可以瞻仰下 数据分析的5个流程书中，将数据分析分为5个步骤，看完后，感觉很靠谱，真的很实用，这里分享下 商业数据分析的目的是解决问题，要解决问题，需要使用统计分析、机器学习、数据挖掘等各种方法。 现状和预期首先我们要确认“什么才是数据分析中的问题”。比如，“某种商品销售额下降”，这是一个现象，但它是不是一个问题呢？如果，该产品不是公司主打商品，并且就要下架了，那销售额下降并不是一个问题，或者，该商品处于正常的波动，或是季节、市场环境的外部因素导致的，可能都不是一个问题；相反，如果该商品是公司主打商品，并且没有其他外部因素导致，那销售额下降就是个问题了。 这里记录下，其实，还需要确认下，销售额取数逻辑是否有问题，确保数据没有问题，并且要知道这个下降是怎么定义的，是和什么商品，或时间段对比发现下降的。 有对比，才会有差距，既然下降了，说明他心里一定有个预期，即现状和预期之间是有差距的 发现问题有了上面的“现状和预期”，我们需要区别”现象和问题“。像“销售额下降”，“顾客流失”，这都是一个现象，我们需要从中去发现问题 现象 前提 预期 是否有问题 销售额下降 销售额比例低 维持现状 无 销售额下降 销售额比例高 将销售额恢复到良好状态 有 销售额上升 广告费用高 降低广告费用 有 销售额上升 广告费用适当 维持现状 无 从3个角度发现问题发现问题的关键是思考并理解现状和预期之间的差距。那怎样发现、理解这个差距呢？ 观察数据大小首先考虑有哪些因素会导致这些差距，并明确这些因素的影响程度大小，即找到影响最大的因素。 将数据分解后观察指从多个角度观察发生的现象，分解出构成这种现象的因素。在分解的时候，必须遵循MECE原则： Mutually 相互性 Exclusive 排重性 Collectively 完整性 Exhaustive 全面性 我感觉这个很抽象，不是很理解，书上有一个例子，说的还不错，常用的拆分方法是因数分解，比如： 销售额=人均销售额*购买人数 拆解后，找到容易调控的因子，才方面后面去解决问题 将数据比较后观察指的是将发生问题是的数据和没发生问题时的数据相互比较，并找出问题出现的原因。比如，按时间对比，看看同比、环比（使用时间序列） 昨天和今天比较 上周和本周比较 同一个商业活动前、后比较 与竞争对手数据比较 公司内部服务之间利益比较 年龄段差异 性别差异 地域差异 数据收集和整理##]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
        <tag>数据分析</tag>
        <tag>实例</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL笔试题-连续登录天数]]></title>
    <url>%2F2017%2F08%2F31%2Fdata-analyst-interview-sql-03%2F</url>
    <content type="text"><![CDATA[SQL笔试题 下面的SQL基于PostgreSQL 1.用户连续登录天数背景描述现在我们有一张用户登录日志表，记录用户每天的登录时间，我们想要统计一下，用户每次连续登录的开始日期和结束日期，以及连续登录天数。 用户ID 登录日期 1001 2017-01-01 1001 2017-01-02 1001 2017-01-04 1001 2017-01-06 1002 2017-01-02 1002 2017-01-03 同学们先思考下，整理下思路，如果没有思路或者某几个点不了解，就可以继续往下看了。 测试数据1234567891011121314151617181920212223242526272829303132333435363738CREATE TABLE interview.tm_login_log( user_id integer, login_date date)WITH ( OIDS=FALSE);-- 这里的数据是最简化的情况，每个用户每天只有一条登录信息，insert into interview.tm_login_log values(1001,'2017-01-01');insert into interview.tm_login_log values(1001,'2017-01-02');insert into interview.tm_login_log values(1001,'2017-01-04');insert into interview.tm_login_log values(1001,'2017-01-05');insert into interview.tm_login_log values(1001,'2017-01-06');insert into interview.tm_login_log values(1001,'2017-01-07');insert into interview.tm_login_log values(1001,'2017-01-08');insert into interview.tm_login_log values(1001,'2017-01-09');insert into interview.tm_login_log values(1001,'2017-01-10');insert into interview.tm_login_log values(1001,'2017-01-12');insert into interview.tm_login_log values(1001,'2017-01-13');insert into interview.tm_login_log values(1001,'2017-01-15');insert into interview.tm_login_log values(1001,'2017-01-16');insert into interview.tm_login_log values(1002,'2017-01-01');insert into interview.tm_login_log values(1002,'2017-01-02');insert into interview.tm_login_log values(1002,'2017-01-03');insert into interview.tm_login_log values(1002,'2017-01-04');insert into interview.tm_login_log values(1002,'2017-01-05');insert into interview.tm_login_log values(1002,'2017-01-06');insert into interview.tm_login_log values(1002,'2017-01-07');insert into interview.tm_login_log values(1002,'2017-01-08');insert into interview.tm_login_log values(1002,'2017-01-09');insert into interview.tm_login_log values(1002,'2017-01-10');insert into interview.tm_login_log values(1002,'2017-01-11');insert into interview.tm_login_log values(1002,'2017-01-12');insert into interview.tm_login_log values(1002,'2017-01-13');insert into interview.tm_login_log values(1002,'2017-01-16');insert into interview.tm_login_log values(1002,'2017-01-17'); 步骤拆解我们首先要思考，怎样才算连续登录呢？就是1号登录，2号也登录了，这样就连续2天登录，那我们怎么知道2号他有没有登录呢？一种思路是根据排序来判断：我们来根据日期来排个名1234567select user_id, login_date, row_number() over(partition by user_id order by login_date) day_rankfrom interview.tm_login_log; 现在，我们根据用户ID，对他的登录日期做了排序，但是我们还是没有办法知道，他是不是连续的。我们根据这个排序再思考一下，对于一个用户来说，他的登录日期排序已经是连续的了，如果登录日期也是个数字，那我们根据每行的差值，就可以判断登录日期是否连续了。我们换个角度，我们找一个起始日期，来计算一个相差的天数，用它去和排序相对比，就可以了。12345678select user_id, login_date, date_part('day',login_date::timestamp - timestamp'2017-01-01') day_interval, -- 间隔天数 row_number() over(partition by user_id order by login_date) day_rank -- 日期排序from interview.tm_login_log; 我们观察下数据，因为日期排序是连续的，我们统计的间隔天数都是一个起始日期，所以，如果登录日期是连续的，那么，排序-间隔天数的差值也应该是一样的 12345678910111213select user_id, login_date, date_part('day',login_date::timestamp - timestamp'2017-01-01') day_interval, -- 间隔天数 row_number() over(partition by user_id order by login_date) day_rank, -- 日期排序 ( row_number() over(partition by user_id order by login_date) ) - ( date_part('day',login_date::timestamp - timestamp'2017-01-01') ) diff_valuefrom interview.tm_login_log; 差值一样的记录，就是连续登录的日期 好了，连续登录的判断标准，我们已经确定了，下面就是把题目中要的数据查出来即可1234567891011121314151617181920212223select user_id, --diff_value, --差值 min(login_date) start_date, --开始日期 max(login_date) end_date, --结束日期 count(1) running_days --连续登录天数from ( select user_id, login_date, date_part('day',login_date::timestamp - timestamp'2017-01-01') day_interval, -- 间隔天数 row_number() over(partition by user_id order by login_date) day_rank, -- 日期排序 ( row_number() over(partition by user_id order by login_date) ) - ( date_part('day',login_date::timestamp - timestamp'2017-01-01') ) diff_value from interview.tm_login_log) basegroup by user_id,diff_valueorder by user_id,start_date 拓展：获取用户最大的连续登录天数及开始日期和结束日期123456789101112131415161718192021222324252627282930313233with tmp as (select user_id, diff_value, --差值 min(login_date) start_date, --开始日期 max(login_date) end_date, --结束日期 count(1) running_days --连续登录天数from ( select user_id, login_date, date_part('day',login_date::timestamp - timestamp'2017-01-01') day_interval, -- 间隔天数 row_number() over(partition by user_id order by login_date) day_rank, -- 日期排序 ( row_number() over(partition by user_id order by login_date) ) - ( date_part('day',login_date::timestamp - timestamp'2017-01-01') ) diff_value from interview.tm_login_log) basegroup by user_id,diff_value) select a.user_id,a.start_date,a.end_date,a.running_daysfrom tmp ajoin ( select user_id,max(running_days) running_days from tmp group by user_id) b on a.user_id = b.user_idand a.running_days = b.running_days; 连续5天登录用户这里补充另一个类似的问题，这里，我们想看连续登录5天的用户，使用上面的方法可以实现，这里介绍一个更快的方法：是使用一个函数123456789101112向前取n位lag(value anyelement [, offset integer [, default anyelement ]])select *from ( select a.user_id, a.login_date, --5天前的登录日期 lag(a.login_date,4) over(partition by a.user_id order by a.login_date) pre_five_day from interview.tm_login_log a )xwhere date_part('day',x.login_date::timestamp - pre_five_day::timestamp)=4 这样取连续登录的话，比较方便 思考题：连续7天未登录用户这里留一个类似的小问题，大家自行练习下 小结我们简单整理下思路，上面的例子，我认为主要是一个思路的介绍，核心就是我们要找到一个判断连续的方法，找到方法后，SQL自然就一步一步想出来了。上面只是一种思路，一定还有更优的解法，欢迎大家反馈分享。]]></content>
      <categories>
        <category>笔试题</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>笔试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL笔试题-行转列]]></title>
    <url>%2F2017%2F08%2F28%2Fdata-analyst-interview-sql-02%2F</url>
    <content type="text"><![CDATA[SQL笔试题 下面的SQL基于PostgreSQL 1. 行转列背景我们写SQL的时候，经常会遇到一些列转行、行转列的情况，有的时候是为了展现需要，有的时候是代码里就得这样转一下。总之嘞，得掌握这个技巧。下面就开始我们的练习。 测试数据1234567891011121314151617181920CREATE TABLE interview.tm_score( stu_name character varying(20), -- 学生名称 course_name character varying(20), -- 课程名称 score numeric(10,0) -- 分数)WITH ( OIDS=FALSE);-- 初始化数据insert into interview.tm_score values('路飞','数学',100);insert into interview.tm_score values('路飞','语文',62);insert into interview.tm_score values('路飞','英语',98);insert into interview.tm_score values('索隆','数学',40);insert into interview.tm_score values('索隆','语文',57);insert into interview.tm_score values('索隆','英语',40);insert into interview.tm_score values('娜美','数学',42);insert into interview.tm_score values('娜美','语文',44);insert into interview.tm_score values('娜美','英语',28); 我们先来思考第一个问题：我们怎样可以将课程变成列呢，类似交叉表那样？最容易想到的方法，就是使用case when了 case when1234567891011select stu_name, max(case course_name when '数学' then score else 0 end) as "数学", max(case course_name when '语文' then score else 0 end) as "语文", max(case course_name when '英语' then score else 0 end) as "英语" from interview.tm_scoregroup by stu_name; crosstab在新版本的PostgreSQL中有一个extension，可以方便的实现行转列我们需要先安装这个扩展，我看了下，PostgreSQL8.3以后的版本都可以安装1create extension tablefunc 官网地址：https://www.postgresql.org/docs/9.5/static/tablefunc.html 安装完后，会有这么几个函数 我们可以使用crosstab来实现上面的行转列123select *from crosstab( 'select stu_name,course_name,score from interview.tm_score') as ct(stu_name varchar(20) ,"数学" numeric(10,0),"语文" numeric(10,0), "英语" numeric(10,0)) 结果也是一样的，我们传入一个SQL，SQL里面返回3列（这3列都是默认处理的，第一列是主字段，第2列是要拆成列的字段，第3列是要显示的值），最后因为返回值是record，所以我们要定义一下类型。 这里，顺便看看这个函数的用法 要转成列的字段有Null是这样一种情况：不是所有的同学都有3门课程的分数我们删掉了几条记录来模拟 这时候使用crosstab，结果会有问题 数据会自动从第一列开始，导致错误的数据12345-- crosstab(text source_sql, text category_sql)select *from crosstab( 'select stu_name,course_name,score from interview.tm_score','select distinct course_name from interview.tm_score') as ct(stu_name varchar(20) ,"数学" numeric(10,0),"语文" numeric(10,0), "英语" numeric(10,0)) 这时候，数据就对了 source_sql 多于3列这种情况是，我们查询的结果不单单有上面说的3列，可能还有其他字段，比如，我们可以在上面的测试数据上加一个班级列我们同样也是使用上面的方式解决 12345678alter table interview.tm_score add column stu_class varchar(10);update interview.tm_score set stu_class='一班' where stu_name in ('路飞','索隆');update interview.tm_score set stu_class='二班' where stu_name not in ('路飞','索隆');select *from crosstab( 'select stu_name,stu_class,course_name,score from interview.tm_score','select distinct course_name from interview.tm_score') as ct(stu_name varchar(20) ,stu_class varchar(10),"数学" numeric(10,0),"语文" numeric(10,0), "英语" numeric(10,0)) 这里我们六个思考题，有这样一组数据： 我们想查询出这样格式的数据 大家可以练习下这个SQL该怎么写 列转行我们可以把行转成列，那要怎样把列转成行呢？ union all最简单的方法是直接union all,既然要放到一列里面，那字段类型肯定要一样，所以直接根据不同字段去union all 就好了 12345select stu_name,"数学" from xxxunion all select stu_name,"语文" from xxxunion allselect stu_name,"英语" from xxx 小结这里简单介绍了几种常见的处理方法，实际使用中，一定还有更好地方法、或一些特殊的问题，欢迎大家分享、反馈。]]></content>
      <categories>
        <category>笔试题</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>笔试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL笔试题-累计值（月累计、年累计）]]></title>
    <url>%2F2017%2F08%2F28%2Fdata-analyst-interview-sql-01%2F</url>
    <content type="text"><![CDATA[SQL笔试题 下面的SQL基于PostgreSQL 1. 累计值（月累计、年累计）背景描述比如说，我们有这样一份数据,记录的是图书每天的销量情况： 日期 图书名称 销量 2017-01-01 解忧杂货店 90 2017-01-03 解忧杂货店 50 2017-01-05 解忧杂货店 100 2017-01-01 雪落香杉树 100 2017-01-03 雪落香杉树 44 2017-01-04 雪落香杉树 99 现在，我们要统计每本书，当月的累计销量？即1号是1号的销量，2号是1号+2号当天的销量（注意：这里2号当天虽然没有销量，但是应该为1号的90+2号的0，为90）。大家先思考下，如果可以很快解答，就不需要接着读啦，有疑问的同学可以继续往下看。 测试数据123456789101112131415161718192021222324252627282930313233343536373839-- 图书的销量表CREATE TABLE interview.tm_book_sales( calendar_date date, -- 日期 book_name character varying(100), -- 图书名称 sales numeric(10,0) -- 销量)WITH ( OIDS=FALSE);-- 测试数据insert into tm_book_sales values('2017-01-01','解忧杂货店',56);insert into tm_book_sales values('2017-01-02','解忧杂货店',100);insert into tm_book_sales values('2017-01-03','解忧杂货店',70);insert into tm_book_sales values('2017-01-06','解忧杂货店',11);insert into tm_book_sales values('2017-01-07','解忧杂货店',65);insert into tm_book_sales values('2017-01-08','解忧杂货店',9);insert into tm_book_sales values('2017-01-09','解忧杂货店',30);insert into tm_book_sales values('2017-01-10','解忧杂货店',56);insert into tm_book_sales values('2017-01-01','雪落香杉树',18);insert into tm_book_sales values('2017-01-02','雪落香杉树',40);insert into tm_book_sales values('2017-01-03','雪落香杉树',2);insert into tm_book_sales values('2017-01-04','雪落香杉树',22);insert into tm_book_sales values('2017-01-05','雪落香杉树',48);insert into tm_book_sales values('2017-01-07','雪落香杉树',71);insert into tm_book_sales values('2017-01-09','雪落香杉树',73);insert into tm_book_sales values('2017-01-10','雪落香杉树',37);insert into tm_book_sales values('2017-02-03','解忧杂货店',5);insert into tm_book_sales values('2017-02-05','解忧杂货店',46);insert into tm_book_sales values('2017-02-06','解忧杂货店',35);insert into tm_book_sales values('2017-02-07','解忧杂货店',10);insert into tm_book_sales values('2017-02-09','解忧杂货店',30);insert into tm_book_sales values('2017-02-10','解忧杂货店',12);insert into tm_book_sales values('2017-02-13','解忧杂货店',43);insert into tm_book_sales values('2017-02-01','雪落香杉树',10);insert into tm_book_sales values('2017-02-04','雪落香杉树',78);insert into tm_book_sales values('2017-02-10','雪落香杉树',50);insert into tm_book_sales values('2017-02-20','雪落香杉树',93); 现在呢，我们有了图书每天的销量数据，下面，我们思考1个问题：我想要统计每本图书的当月累计销量，应该怎么做呢？ 如果只是单纯的统计每本书每个月的销量，熟悉SQL的同学，一定可以很快想到12345678910111213select to_char(calendar_date,'yyyy-mm') month_name, book_name, sum(sales) from interview.tm_book_sales group by to_char(calendar_date,'yyyy-mm'), book_nameorder by book_name, to_char(calendar_date,'yyyy-mm'); 下面，我们来想下，这个月累计怎么做？月累计值，其实就是当天的销量再加上当天之前的销量 自关联通过 interview.tm_book_sales 表，我们可以获取每一天的销量，那要怎样获取每天历史的销量呢？最简单的方式就是自关联了。其实就是自己和自己去关联，来获取历史的销量123456789101112131415161718192021222324select t_today.calendar_date, t_today.book_name, sum(t_his.sales) sales_mtdfrom interview.tm_book_sales t_todayleft join interview.tm_book_sales t_hison -- 图书名称相等 t_his.book_name = t_today.book_name and -- 月份相等，只统计当月 to_char(t_his.calendar_date,'yyyy-mm') = to_char(t_today.calendar_date,'yyyy-mm')and -- 获取当天之前的历史日期 t_his.calendar_date &lt;= t_today.calendar_dategroup by t_today.calendar_date, t_today.book_nameorder by t_today.book_name, t_today.calendar_date; 好了，上面，我们通过自关联，获取了每本图书的月累计销量，不要太高兴，我们观察下，就会发现些问题。我们看看日期，就会发现，有些日期是没有销量的，比如：《解忧杂货店》2017-01-04，2017-01-05 就没有销量，但实际上，如果是累计值得花，他是应该有数据的，因为1号、2号、3号都有数据，就算4号当天没有销量，月累计也应该要算上前3天的销量，所以我们的SQL并不严谨，还得修改。 补全没有销量的日期我们需要想办法补全缺失的日期，如果，t_today里面含有每一天每本书的数据就好了，这就要我们手动构造一个了。123456789101112select t_day.calendar_date, t_book.book_namefrom -- 日期维度表，就是存放每一天 base.dm_calendar t_daycross join -- 所有的图书信息 (select distinct book_name from interview.tm_book_sales) t_bookwhere t_day.month_id=201701; 日期维度表的话，其实是数仓中必备的地基础维度中的一个，她里面就是存放了每一天的数据，和其他一些我们会常用的字段，后面写一篇文章详细介绍下。我们通过笛卡尔积，生成了一张包含每一天每本的图书的一个全维度表。1234567891011121314151617181920212223242526272829303132333435363738394041with t_dim as ( select t_day.calendar_date, t_book.book_name from base.dm_calendar t_day cross join (select distinct book_name from interview.tm_book_sales) t_book where t_day.month_id=201701)select t_dim.calendar_date, t_dim.book_name, sum(t_his.sales) sales_mtdfrom t_dimleft join interview.tm_book_sales t_todayon t_today.calendar_date = t_dim.calendar_dateand t_today.book_name = t_dim.book_nameleft join interview.tm_book_sales t_hison -- 图书名称相等 t_his.book_name = t_dim.book_name and -- 月份相等，只统计当月 to_char(t_his.calendar_date,'yyyy-mm') = to_char(t_dim.calendar_date,'yyyy-mm')and -- 获取当天之前的历史日期 t_his.calendar_date &lt;= t_dim.calendar_dategroup by t_dim.calendar_date, t_dim.book_nameorder by t_dim.book_name, t_dim.calendar_date; 好啦，补全了日期信息后，我们的月累计算是完成了，手工。 总结简单总结下，通过上面的例子，我们要掌握什么呢？首先是对业务的理解，比如上面的月累计的统计方法；然后根据统计方法，使用SQL去实现，一步步完善；还有对日期维度表的一个综合使用。年累计的实现也是一样的，同学们可以自行练习下，有问题可以反馈。]]></content>
      <categories>
        <category>笔试题</category>
      </categories>
      <tags>
        <tag>SQL</tag>
        <tag>笔试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matplotlib手册(9) - 绘制动画]]></title>
    <url>%2F2017%2F08%2F17%2Fmatplotlib-base-flash-09%2F</url>
    <content type="text"><![CDATA[matplotlib手册(9) 绘制动画 前面，我们介绍了很多绘图的方法，matplotlib不单单可以绘制静态的图，还可以制作动态的图，下面，我们就来学习下。 我们主要参考matplotlib官网的例子http://matplotlib.org/api/animation_api.html 创建动画最简单的方式，就是使用Animation的子类,就是下面的这2个 1. FuncAnimation函数介绍及主要参数1234567891011class matplotlib.animation.FuncAnimation(fig, func, frames=None, init_func=None, fargs=None, save_count=None, **kwargs)fig : matplotlib.figure.Figurefunc : callableframes : iterable, int, generator function, or None, optionalinit_func : callable, optionalfargs : tuple or None, optionalinterval : number, optionalrepeat_delay : number, optionalrepeat : bool, optional 小栗子1234567891011121314151617181920212223242526272829# -*- coding: utf-8 -*-"""Created on Thu Aug 17 18:19:08 2017@author: yuguiyang"""import numpy as np import matplotlib.pyplot as plt import matplotlib.animation as animation from datetime import datetime fig,axes = plt.subplots() axes.plot(np.random.rand(10)) #重新绘制图形def update_line(data): print(datetime.now(),'--',data) #清空当前轴 plt.cla() #重新绘图 axes.plot(np.random.rand(10))#传入的fig中，调用update_line函数，将range(3)作为参数传给update_line，1秒调用一次ani = animation.FuncAnimation(fig, update_line, 3, interval=1000) plt.show() 上面的代码，我们定义了一个函数update_line，他会清空axes，并重新绘图；FuncAnimation会每个1秒调用一次这个函数 这里记录一个小问题，暂时还没有解决 frames 参数的问题上面的例子里，我们给的是一个常量3，按照官方的介绍，是按照range(3),来一次传给函数的，但实际测试下来，发现他的调用会有问题。我们看下上面的那个输出 刚刚发现了导致这个问题的原因，注意看这个：123init_func : callable, optional A function used to draw a clear frame. If not given, the results of drawing from the first item in the frames sequence will be used. This function will be called once before the first frame. 上面，因为我们没有指定初始化函数，所以导致，会调用一次update_line，用它返回值作为初始状态，我们改下脚本再看123456def init(): print('init') #清空当前轴 plt.cla() ani = animation.FuncAnimation(fig, update_line, 3, repeat=False, interval=3000,init_func=init) 这回输出就正常了， repeat、repeat_delay这2个参数一般会配合使用，repeat默认是true，所以上面的例子会一直循环下去，如果我们改为false,第一次循环完之后就会停止。 2. ArtistAnimation12345class matplotlib.animation.ArtistAnimation(fig, artists, *args, **kwargs)artists : list Each list entry a collection of artists that represent what needs to be enabled on each frame. These will be disabled for other frames. 使用起来和上面的差不多，这里不会调用函数，而是传入一个list，123456789101112131415import numpy as np import matplotlib.pyplot as plt import matplotlib.animation as animation fig,axes = plt.subplots() ims= []for i in range(5): ims.append(axes.plot(np.random.rand(10)))im_ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=3000, blit=True)print(ims)plt.show()]]></content>
      <categories>
        <category>数据可视化-Python&amp;R</category>
      </categories>
      <tags>
        <tag>数据可视化</tag>
        <tag>Python</tag>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy手册(1)-ndarray]]></title>
    <url>%2F2017%2F08%2F02%2Fnumpy-handbook-01%2F</url>
    <content type="text"><![CDATA[前面我们算是简单入门了Pandas，numpy也是数据分析中常用的，这里我们也来简单学习下。 1.numpy基本介绍numpy是Python的一种开源数值计算扩展，这种工具可以用来存储和处理大型矩阵。一个用Python实现的科学计算包。from 百度百科 numpy有2种基本对象，1ndarray（N-dimensional array object）和 ufunc（universal function object） ndarray是存储单一数据类型的多维数组，ufunc是能够对数组进行处理的函数。 2.ndarray我们先来看看这个数组首先，我们得引入numpy1import numpy as np 2.1 创建数组初始化的话有很多方式：Array creation routines我们可以直接使用list来初始化，array有很多的属性，比如大小，维度，元素个数123456789import numpy as npa = np.array([1,2,3])b = np.array([4,5,6])c = np.array([[1,2,3],[4,5,6],[7,8,9]])print(a,type(a),',shape:',a.shape,',ndim:',a.ndim,',size:',a.size)print(b,type(b),',shape:',b.shape,',ndim:',b.ndim,',size:',b.size)print(c,type(c),',shape:',c.shape,',ndim:',c.ndim,',size:',c.size) 这里呢，我们定义了一维数组和二维数组，比如c，是3行3列的2维数组，元素个数是9个1numpy.array(object, dtype=None, copy=True, order='K', subok=False, ndmin=0) 这里，我们再说下这个shape，这个属性可以修改123456789#原来是4行3列c = np.array([[1,2,3],[4,5,6],[7,8,9],[0,0,7]])print(c)#我们改为3行4列c.shape=(3,4)print(c)#改为2行6列c.shape=(2,6)print(c) 这里需要注意下，如果某个轴的元素为-1，将根据数组元素的个数，自动计算长度1234c.shape=(1,-1)print(c)c.shape=(-1,1)print(c) 这里的shape是改变原来的数组，另一个method，可以创建一个改变shape的新数组，而原数组保持不变12345c = np.array([[1,2,3],[4,5,6],[7,8,9],[0,0,7]])print('c:',c)d = c.reshape(2,6)print('c:',c)print('d:',d) 这里要注意的是，c和d共享内存数据存储内存区域，c变了，d也会变12345print(c[0])#修改c[0]c[0]=[-9,-8,-3]print('c:',c)print('d:',d) 我们可以通过dtype来获取元素的类型，我们可以在初始化的时候，指定dtype12345c = np.array([1,2,3])print(c.dtype) #int32d = np.array([1.1,3.3])print(d.dtype) #float64 下面，我们来看看常用的初始化方法 arange通过指定开始值，结束值和步长来创建一维数组，这里不包过终值1234567arange([start,] stop[, step,], dtype=None)np.arange(3)Out[51]: array([0, 1, 2])np.arange(1,10,3)Out[52]: array([1, 4, 7]) linspace通过指定开始值，终值和元素个数，来创建数组，这里包括终值12345np.linspace(1,10,5)Out[53]: array([ 1. , 3.25, 5.5 , 7.75, 10. ])np.linspace(1,2,3)Out[54]: array([ 1. , 1.5, 2. ]) np.zeros,np.ones这2个函数可以初始化指定长度或形状的全0或全1的数组1234567891011121314151617np.ones(3)Out[202]: array([ 1., 1., 1.])np.ones([2,2])Out[203]: array([[ 1., 1.], [ 1., 1.]])np.zeros(5)Out[204]: array([ 0., 0., 0., 0., 0.])np.zeros([4,3])Out[205]: array([[ 0., 0., 0.], [ 0., 0., 0.], [ 0., 0., 0.], [ 0., 0., 0.]]) np.empty可以创建一个没有任何具体值得数组1234567891011np.empty(2)Out[211]: array([ 7.74860419e-304, 7.74860419e-304])np.empty(2,dtype=int)Out[214]: array([ -1, 2147483647])np.empty((3,3),dtype=np.float64)Out[215]: array([[ 4.94065646e-324, 9.88131292e-324, 1.48219694e-323], [ 1.97626258e-323, 2.47032823e-323, 2.96439388e-323], [ 3.45845952e-323, 3.95252517e-323, 4.44659081e-323]]) 这要注意下，empty初始化的都是没有意思的值，不一定会初始化为0 2.2 存取元素这里直接粘贴一个例子，原始教程在这：http://old.sebug.net/paper/books/scipydoc/numpy_intro.html123456789101112131415161718&gt;&gt;&gt; a = np.arange(10)&gt;&gt;&gt; a[5] # 用整数作为下标可以获取数组中的某个元素5&gt;&gt;&gt; a[3:5] # 用范围作为下标获取数组的一个切片，包括a[3]不包括a[5]array([3, 4])&gt;&gt;&gt; a[:5] # 省略开始下标，表示从a[0]开始array([0, 1, 2, 3, 4])&gt;&gt;&gt; a[:-1] # 下标可以使用负数，表示从数组后往前数array([0, 1, 2, 3, 4, 5, 6, 7, 8])&gt;&gt;&gt; a[2:4] = 100,101 # 下标还可以用来修改元素的值&gt;&gt;&gt; aarray([ 0, 1, 100, 101, 4, 5, 6, 7, 8, 9])&gt;&gt;&gt; a[1:-1:2] # 范围中的第三个参数表示步长，2表示隔一个元素取一个元素array([ 1, 101, 5, 7])&gt;&gt;&gt; a[::-1] # 省略范围的开始下标和结束下标，步长为-1，整个数组头尾颠倒array([ 9, 8, 7, 6, 5, 4, 101, 100, 1, 0])&gt;&gt;&gt; a[5:1:-2] # 步长为负数时，开始下标必须大于结束下标array([ 5, 101]) 就2维数组来说 这是基本的获取方式，还有些高级的方法 使用整数序列这里简单来2个练习，原文例子很多，就是通过下标来筛选数据12345678910111213141516171819202122a = np.arange(-5,5,1)aOut[68]: array([-5, -4, -3, -2, -1, 0, 1, 2, 3, 4])a[[1,3,5]]Out[69]: array([-4, -2, 0])### 使用布尔数组按照传入的布尔数组，只有为True的才返回，也叫布尔型索引``` pythona=np.array([-3,1,5])aOut[72]: array([-3, 1, 5])a[[False,True,False]]Out[73]: array([1])a[[True,False,True]]Out[74]: array([-3, 5]) 3.附录（参考资料）文档：https://docs.scipy.org/doc/numpy-dev/reference/index.html#reference numpy快速处理数据]]></content>
      <categories>
        <category>Python-Numpy</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cognos资料汇总贴]]></title>
    <url>%2F2017%2F08%2F01%2Fcognos-doc-main%2F</url>
    <content type="text"><![CDATA[以前搞过Cognos，写过很多基础的教程，应该是14年的样子，都在CSDN上，这里贴个汇总贴吧，想要看的同学可以去看看，希望有帮助。 ReportStudio入门教程：http://blog.csdn.net/column/details/ygy-reportstudio.html Framework Manage入门教程：http://blog.csdn.net/column/details/ygy-frameworkmanager.html Cognos函数手册：http://blog.csdn.net/column/details/ygy-cognos-function.html Cognos相关的其他资料（主页不同的类别下看看）：http://blog.csdn.net/yuguiyang1990 好了，感兴趣的同学，可以自行去看看，好久不搞了，估计有疑问也解决不了了…]]></content>
      <categories>
        <category>数据可视化-Cognos</category>
      </categories>
      <tags>
        <tag>Cognos</tag>
        <tag>数据可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（十二）- 控件使用-从步骤插入数据]]></title>
    <url>%2F2017%2F04%2F14%2FKettle-handbook-12%2F</url>
    <content type="text"><![CDATA[这里介绍一个控件的小功能，也是最近才发现的，之前在“表输入”中要使用参数的话，一般都是使用变量，其实，还有个功能也可以尝试使用整体流程就是这样，我们第一个 query_paramter，就是查询了我们想设置的参数然后，就是我们真正需要的，我们再表输入中，使用 “?”来占位，然后“从步骤插入数据”，选择上一个步骤，然后会将数据替换占位符最后，我们将文件导出即可，奥对了，我们可以改成日志控件，直接输出查看刚刚，上面还有一个“执行每一行”，这个就是，如果我们有多个参数，就可以使用这个参数了，很方便，好了，就介绍到这里先。]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（十一）- 用PGP加密、加密文件]]></title>
    <url>%2F2017%2F04%2F11%2FKettle-handbook-11%2F</url>
    <content type="text"><![CDATA[看到有同学提问，以前也没用过，百度了一下，找了些资料，这里记录下。 1. 安装gpg4win这个gpg4win是干嘛的呢，我们可以去他的官网看看：gpg4win目前，只知道他是加密的，这个是对Windows平台使用的这里可能还有个PGP的概念，看看百度百科 好了，具体概念，大家可以自行找找，我们下载下来，然后安装一下即可这个是昨天安装的，就不粘贴步骤了，安装完后，我们要先创建一个证书的东西，我们打开这个管理界面打开后，是这样一个界面，（网上有这个的安装配置教程，这里也简单介绍下，不清楚的可以再百度看看）我们新建一个Certificate我们选择一个加密方式，使用第一个就可以了我们输入些基本信息然后next就可以然后，我们得输入一段密钥好了，这里，就配置完成了 2. 用PGP加密文件好了，这里，我们新建一个作业，我们主要使用这2个控件一个很简单的流程，我们做些简单的配置，一个是GPG的目录（就是我们上面安装的那个）还有就是，我们的要加密的文件和一个目标文件名，注意，这里我们得填写一下“用户ID”，就是我们前面新建的那个用户名就可以了这里，可以勾选一下，目标是一个文件好了，然后，我们执行下就可以了我们源文件：加密后的文件：下面，我们再看看，怎样解密 3. 用PGP解密文件知道了加密，解密也是一样的，这里的话，配置和上面差不多，这里，我们要填写一个“密钥”，就是我们上面创建时，输入的一个密码我们运行一下，解密后，是一样的好了，就简单介绍到这里]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（十）- 跨库查询]]></title>
    <url>%2F2017%2F04%2F10%2FKettle-handbook-10%2F</url>
    <content type="text"><![CDATA[Kettle整体使用起来，还是很方便的，熟悉应用了之后，就是对控件的熟悉和使用了，只要思路有了，就是整合下Kettle中各个控件的使用就行。这里，简单介绍下一个“跨库查询”的控件。有的时候，我们一个脚本，可能只是临时性的，或者需要实时的去查一下，同步到数仓的话，可能不太方便，我们就可以使用跨库查询的控件用到的表信息 1. 数据库连接(Database Join)我们先用这个控件来实现一下用起来也很简单表输入：是我们第一个库中的SQL数据库连接：是我们另一个库的SQL我们用关联的字段放在where条件后，使用“?”来占位，并在下面，选择要传入的参数默认的话，是JOIN，我们也可以勾选Outer Join，然后，我们看下，输出就行这是后面导出的文件，这里，我们就简单实现了跨库的查询 2. 数据库查询我们再来看另一个控件，“数据库查询”，这个控件同样可以实现跨库，但是有一个小问题首先，我们使用上一次的数据来看我们执行下，结果看上去是一样的这其实有个隐藏的问题，我们再增加几条记录看看比如：现在1号有2条记录，正常的话，我们导出也是要有2条的我们执行下看看我们会看到，数据并没有增加，这是控件导致的，先获取左边的结果集，然后一条一条去右边匹配；匹配到第一条记录后，就会跳出，直接去匹配下一个，所以，我们有2条记录，也只会找到第一个。这并不是我们想要的，我们再试下第一个控件使用这个“数据库查询”控件的话，可以通过将1-N关系汇总，将N的一方，放在前面最后的结果也是可以的]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（九）- 发送邮件]]></title>
    <url>%2F2017%2F03%2F30%2FKettle-handbook-09%2F</url>
    <content type="text"><![CDATA[在Kettle里面，我们每天执行完调度之后，想要监控下JOB的执行状态，通常我们可以会发送邮件，可以的话，还可以发送短信。 在Kettle里面，发送邮件很方便，这里，我们就简单的测试下。 1. 在作业中发送简单邮件我们只需要使用到这个控件就可以了，这样，一个简单的发送邮件流程就好了 控件的配置：收件人，抄送啊，信息，自行填写就行，多个收件人，使用“空格”分隔在服务器这里，我们填上服务器的信息就可以了这里是邮件消息的一些配置，暂时先到这里，我们测试下结果然后，查看邮箱，我们会接收到这个邮件，刚刚简单测了下这个“回复名称”，就是这里试过中文，会有问题，有乱码，可能是Windows下的原因，没有再去测试验证就是收到邮件时的一个发件人的名称，不同邮箱显示的不一样 2. 增加附件附件的话，也很简单，上面的面板中直接配置就可以了然后，我们需要将待发送的邮件，添加到结果集中在控件中，我们添加好文件就行了。我们再次发送，验证下好了，附件也可以了，思路就是这样的，实际应用时，可能还有些问题得注意下 3. 自定义邮件内容到这里，我们会看到，邮件的正文内容，可能并不是我们想要的， 我们想要的可能是这样的信息这就需要自定义正文内容，我们需要勾选下面这个选项这里是可以使用变量的，我们可以拼接HTML来实现好了，邮件的介绍，大概就这些，在转换中，也是可以使用的，大同小异]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（八）- 循环]]></title>
    <url>%2F2017%2F03%2F29%2FKettle-handbook-08%2F</url>
    <content type="text"><![CDATA[有的时候，我们想要在Kettle中实现这个循环的功能，比如，批量加载数据的时候，我们要对10张表执行同样的操作，只有表名和一些信息不一样，这时，写个循环就省事儿多了 1. 遍历结果集实现这里的话，我们主要是通过一个将结果集返回，然后通过转换的设置来实现的 1.1 query_the_result这个转换，只要是将我们要遍历的结果集返回，表输入，我们就是返回了5条记录，来做遍历复制记录到结果，这个控件的作用，就是我们可以在后面的转换继续使用这个结果集。 ##1.2 traverse_the_result这里呢，我们就是需要遍历的转换了，这里，我们只是获取结果集，然后将结果集输出还有一个很重要的一步，怎样让这个转换可以根据结果集的条数，去循环执行呢？就是这个“执行每一个输入行”我们执行下看看 2. 使用JS实现网上有很多的例子，介绍怎样用JS来控制循环，这里我们也简单的测试下 2.1 query_the_result这一步，和上面的一样，就是将结果集返回 2.2 travers_the_result这里主要是使用JS将结果集进行遍历，通过JS，将一些结果存放到变量里面，在后面的操作中就可以使用了，通过${xxx}的方式使用这个其实和Java、JS里面循环思路一样，通过结果集的总数“total_num”和下标“LoopCounter”进行判断 2.3 evaluate_the_loop_count这一步，就是判断下标的值和结果集的总数，进行对比， 2.4 print_the_log输出下，我们想要使用的变量 2.5 manage_the_loop_index这一步，给下标加一，然后获取下一条记录好了，执行下，我们看看好了，循环的使用先介绍到这里]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（七）- 资源库的使用]]></title>
    <url>%2F2017%2F03%2F29%2FKettle-handbook-07%2F</url>
    <content type="text"><![CDATA[1. 为什么使用资源库之前，我们新建转换或者作业的时候，都是直接保存在本地，而如果我们是多人开发的话，除了使用SVN等版本控制软件，还可以使用Kettle的资源库，他会将转换、作业直接保存在数据库中，而且，连接资源库的话，我们就不需要每一次都新建数据库连接了，用起来还是蛮方便的。 2. 新建资源库Kettle7.0里面，是在右上角这个Connect来连接的 2.1 资源库的类型资源库有3中类型Pentaho RepositoryDatabase Repository（使用数据库存储）File Repository（使用文件存储） 2.2 新建Pentaho Repository我们单击上面的get started 之后，就会进入新建界面http://localhost:8080/pentaho一开始还没搞懂这个Server到底怎么启动，后来google了半天发现后来又找到了这个，应该是要安装其他的组件才行，这个类型的库就放弃吧。。 2.3 Database Repository好了，这回，我们选择哪个database的资源库我们填一个connection的名字，然后配置一个资源库的连接就可以了，最好给kettle新建一个数据库使用至于数据库连接的话，和转换里面是一样的，大家可以自行新建一个配置好，以后，大家选择Finish就可以了，然后，我们可以连接下这个库，注意下，这里的用户名和密码，默认是admin/admin，大家直接登录就好了，这是Kettle自己初始化的这个怎么改呢，暂时还没有发现，待研究，等我再google看看，估计官网上会有。（找了下，发现了在哪改密码，就是刚刚的搜索资源库)连接后，我们正常使用就好了，没啥两样，会多一些功能，比如，探索资源库这里我们再保存作业和转换的话，会直接保存在数据库中，而且，很好的一个功能，个人感觉，就是数据库连接只需要创建一次，在哪里都可以用了，不需要再次创建。 2.4 File Repository这个和database的资源库，就差不多了，只不过是基于文件的，保存在本地就可以了这个就和Eclipse一个工作区差不多，转换、作业都保存在这个目录下好了，关于资源库，就简单的说这些了，大家可以自行连接，试试。]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（六）- Hop小记]]></title>
    <url>%2F2017%2F03%2F29%2FKettle-handbook-06%2F</url>
    <content type="text"><![CDATA[1. 什么是Hop在我们前面，使用Kettle过程中，控件与控件之间的连线，这里，我们详细介绍下它，它在Kettle中叫Hop（跳）。 2. Hop的发送方式（转换）在转换中，一般情况，控件和控件之间只有一个Hop，当然，如果需要的话，我们拖了2个控件出来，像这样：Kettle会提示你，下面的信息，让你选择，数据发送的方式 2.1 分发记录目标步骤轮流接收记录，其实就是你一条，我一条，轮着接收数据，这个我们试一下就知道了，我们执行下，看看这个结果试试，我们再步骤度量中，可以看到，a.txt和b.txt分别写入的数量看看结果文件，就是这样的 2.2 复制记录所有记录同时发送到所有的目标步骤，这个看起来就简单多了，比如上面的例子，2个文本文件会接收到同样的所有的数据，我们也试一下结果文件的话，就是2个节点，接收到的数据都是一样的 3.Hop的状态（作业）在作业中，Hop主要用来控制流程有3种状态，一个锁，一个绿色的对号，一个红色的叉号简单来说，：表示无论上一步执行成功还是失败，都一定会执行下一步：表示上一步执行成功才会执行下一步：表示上一步执行失败执行下一步比如我们上面的例子，我们的转换执行成功后，就结束了，如果转换执行失败了，我们就发送邮件。]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（五）- 实例-增量同步数据]]></title>
    <url>%2F2017%2F03%2F28%2FKettle-handbook-05%2F</url>
    <content type="text"><![CDATA[综合前面的几个例子，我们这里来是实现下增量数据的同步。这里只是分享一种方法，实际工作中，还会有其他更好的方案。增量同步的整体思路一般就是：首先拿到这张表的增量数据，怎么拿增量呢，源表需要有一个时间字段，代表该条记录的最新更新时间（及只要该条记录变化，该时间字段就会更新），当然有时间字段最好了，没有的话，可能需要做全表对比之类的操作；正常情况下，业务系统的表中都是有主键的，我们拿到增量数据之后，需要判断该记录的新插入的，还是更新的记录，如果是更新记录，我们需要先将数据加载到中间表，然后，根据主键将目标表中已存在的数据删除，最后再将本次的增量数据插入到目标表。 1.配置表的设计（元数据表）首先我们需要一张配置表，来保存我们要增量同步的表的基本信息1234567--元数据表create table tm_etl_table( table_name varchar(50), --表名 is_run int , --调度状态 update_time timestamp,--表数据更新时间 etl_insert_time timestamp --记录更新时间); 我们初始化一条记录，我们就以这张ods_tm_book表一些基础表准备 12345678910-- 源表create table tm_book(id int,book_name varchar(10),latest_time timestamp);-- 源表数据初始化insert into tm_book(id,book_name,latest_time)select x,x||'_name',clock_timestamp() from generate_series(1,10) x;-- 目标表和中间表create table ods_tm_book(id int,book_name varchar(10),latest_time timestamp,etl_insert_time timestamp);create table staging_tm_book(id int,book_name varchar(10),latest_time timestamp); 源表中的数据 2.同步数据的流程开发整体流程是这样的，注意下，这个只是为了简单演示了这个增量的例子，实际应用的话得修改，这是有漏洞的。 2.1更新元数据表的状态并获取表更新时间就是我们第一个状态，我们更新tm_etl_table表，更新is_run=0，表示我们开始同步数据了，update_time，初始化为 ‘1970-01-01’，表示我们要拉取所有的数据这里，我们将该表的更新时间作为变量，我们会在后面的转换中使用 2.2 加载数据到中间表我们这里，直接表对表，将数据插入到staging其中，表输入中，我们需要根据前面的更新时间变量，获取增量数据，注意，需要勾选上“替换SQL语句中的变量”这里，我们直接就表输出到中间表，每次都需将清空表数据 2.3 加载数据到目标表这里，主要有3段脚本（为了方便，就这样吧），根据主键ID，清空目标表数据，然后，将数据插入到目标表，最后，更新tm_etl_table表中的记录状态好了，用Kettle实现一个增量的逻辑大概就是这样了， 3.小结这里整理几个问题 3.1 中间表这里的话，使用了中间表，Kettle中是有一个控件的，应该叫那个“插入/更新”，可以根据主键将数据更新掉，这个控件之前使用时，发现很慢，就一直没用，后面的话，可能会写个例子，简单测试看看。使用中间表，缓存下数据，也是不错的方法。 3.2 增量流程目前公司中，增量抽取，是这样的，首先各个业务系统的数据导出到文本文件，然后批量将文件加载到数据仓库中（这里使用循环加载的）。因为每天的数据量比较大，如果知己到表的话，会很慢，使用文件，一些数据库都有批量加载的命令，很快很方便，比如：PostgreSQL中的copy命令，Greenplum中的外部表，还有Mysql中的load data等等。]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（四）- 变量的使用]]></title>
    <url>%2F2017%2F03%2F28%2FKettle-handbook-04%2F</url>
    <content type="text"><![CDATA[我们在这一回，介绍下，Kettle中全局变量的使用，我们前面说过的配置文件，其实就是配置全局变量的地方Kettle手册（三）- 配置文件的使用及密码加密 1. 全局变量 就是我们上面说的kettle.properties文件，我们在里面定义的变量，我们可以在所有的转换或者作业中获得到，比如，我们前面，说的数据库参数之前，我们已经在数据库连接中测试过，是可以，这里，我们输出下这个变量，看看 1.1 输出变量的值我们这里，用到了“获取变量”这个控件我们单击，”Get Variables”,就可以获取到当前的全局变量信息我们选择几个输出试试还有一个，”日志“控件，拖好之后，我们直接执行，日志中，我们会看到，我们定义在文件中的参数（加密的参数，我没有重启，所以显示的还是原来的）那我们，可不可以，动态的增加变量呢？ 1.2 动态增加变量刚刚也在网上找了些资料，尝试了下，这里简单分享下（貌似，这得算是对局部变量的操作，暂时就放在这里吧）我们先试下在转换中设置变量，作业中也是可以使用的，我们后面再说测试流程是这样的， 我们再表输入中，有2个时间参数，然后作为变量比如，有这样一个场景，我们每天需要定时调度一些SP，SP都有开始时间，结束时间，调用时，需要传参数进去，这个时候，我们在使用Kettle的时候，就可以通过这样的方式，去设置变量，然后再调用SP我们单击获取字段后，就可以了，这里可以修改变量存在的范围执行后，输出，后面，我们就可以使用这2个时间变量了这里使用的时候，也遇到一个问题，就是变量的默认值，一直都没有生效，不知道为什么，不管是，静态值，还是变量值，都没有办法，待研究。 2. 局部变量（命名参数） 在kettle中，相对于全局变量，我们还可以使用局部变量。感觉，这个全局变量，局部变量，都是相对而言的，就网上大部分资料来说，Kettle中的局部变量就是“命名参数”我们再转换中，右键单击，选择，转换设置 我们选择，“命名参数”，定义一个变量，我们给一个默认值然后，在日志中，将变量输出我们执行下，这个转换，运行时的界面，我们可以看到，这个参数是可以动态改变的，或者，我们再命令行调这个转换的时候，同样可以给他赋值运行结果，这个就是简单的局部变量了]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（三）- 配置文件的使用及密码加密]]></title>
    <url>%2F2017%2F03%2F28%2FKettle-handbook-03%2F</url>
    <content type="text"><![CDATA[好了，我们上一回，练习了一个从数据库导出数据到Excel的例子，我们想一下，如果有很多个转换，我们没链接一次数据库，是不是都需要重复的输入那些数据库地址啊，数据库啊，用户名啊之类的。其实是不用的，我们可以使用变量的方式，写在配置文件中，下面，我们来看看。而且，我们平时开发，都有开发环境、UAT环境、生产环境，连接的地址都不一样，也不可能手动的去修改。 1. Kettle的配置文件 配置文件在哪呢？Windows下，是再当前用户的目录下，一般再C盘，Users下面，有一个当前用户的文件夹，下面有.kettle文件夹进入之后，我们会看到一个kettle.properties的文件，我们的数据库配置信息，就可以放在这里， 我们打开之后，编辑一下保存后，我们要重新启动下Kettle，因为这个配置文件是启动时加载的重启后，我们将上一次，配置的转换打开，使用变量替换下之前的配置，Kettle中，我们使用${xxx}，表示引用一个变量，执行时，会自动替换我们测试下，同样时可以成功的。好了，这样，以后，不管是，数据库地址变化，还是部署生产，我们只需要修改配置文件就可以了。 2. 密码加密 这里，顺便说下，加密的问题，比如，我们上面的数据库密码，是明文的，这样是不太安全的，而实际上，我们都是需要对密码进行加密的我们进到Kettle的安装目录我们会看到，这里有一个Encr.bat，这就是可以加密的脚本使用方法我们输入1Encr.bat -kettle postgres 执行后，会生成，这样一个加密后的密码，然后，我们可以使用这个加密后的字符串，替换我们的密码1pg_password = Encrypted 2be98afc86aa7f2e4cb79ff228dc6fa8c 大家可以试下，这样也是可以的，好了，这个例子就到这。]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（二）- 将数据导出为Excel]]></title>
    <url>%2F2017%2F03%2F27%2FKettle-handbook-02%2F</url>
    <content type="text"><![CDATA[好了，我们先来看第一个例子，就是怎样将数据库中的数据，导出为Excel。平时，如果我们需要将数据导出Excel的话，我们可能会直接复制，然后粘贴出来，但是数据量大的话，就不好用了；或者使用Java等开发语言，写代码，导出Excel；或者一些数据库连接工具自带的导出功能。其实，我们用Kettle的话，还是很方便的，但是平时用下来，Kettle的这个功能还是有些缺陷的，比如导出Excel2007+的时候，经常会报错，我一直也没有解决，这次记录博客顺便研究看看。 1. Kettle的下载及使用 正式开始之前，我们简单说下Kettle的安装配置啥的，Kettle是绿色的，下载之后，直接运行就可以了刚刚在网上下了个最新版的，后面，我们就是用这个7.0版本介绍官网地址：Kettle官网 他这个网站，应该是不太好访问，有VPN的话，可以用起来，下载的话，大概800M左右，后面看看上传一份，昨天为了下载，现冲了个蓝灯的会员解压以后，目录大概是这样的，我们会看到，这里有.bat文件和.sh文件，.bat就是我们在windows下使用的，.sh就是在Linux下使用的，我们找到 Spoon.bat这个文件，就可以启动Kettle了，奥，对了，得先安装下Java打开后，就是这样了，都是图形界面的，很好用Kettle中，主要有2中任务，一个是作业，一个是转换。一般来说，转换是一系列具体的操作，比如：调度SP，导出Excel等等；作业的话，就是按照一定流程来调度一系列转换。大概是这样，实际上，他们也是可以嵌套调用的，我们后面可以再讨论。 2. 第一个转换-将数据导出为Excel 为了实现这个功能，我们需要： 连接到数据库 导出为Excel 首先，我们新建一个转换，新建，之后，我们可以看到，工具箱中，有很多的控件，我们都可以使用，很多我也没有用过，大家可以自行去尝试使用好了，下面，我们就开始介绍我们这次的主题，导出数据到Excel既然，是导出数据，说明我们肯定有一个源头，一个目标，源头是我们的一个数据库，我们得先连接到这个数据库 新建数据库连接我们在主对象库中，DB连接上，右键单击，新建在这里呢，我们可以看到，有很多的数据库可以选择，我们只需要填写基本的连接信息就可以了我们这里连接的是Postgresql，配置好后，测试下，（坑，刚刚在windows上装的数据库，一直连不上，白名单都加好了，就是不行，结果是防火墙忘关了。。）好了，可以连接到数据库了，下面，我们得把数据导出啊，我们需要使用输入这个控件输入下面，有很多的控件，我们这次只使用表输入，因为我们是直接从数据库中拿数据这里直接就是拖拽的，拖过去就行了，双击之后，可以编辑，这里我们就使用刚才的数据源连接，然后查询一张表，表的话，随便create一张就可以了，我们还可以预览数据源头好了，同样的思路，我们需要一个目标，就是输出了，输出到Excel同样的，我们托好之后，双击就可以编辑了，这里，我们主要关注2个配置，一个是excel保存地址，和字段我们选择一个地址，然后得，看下字段那个tab，我们单击，获取字段，就可以从源头获取表中的字段了，当然，我们可以只导出，我们需要的字段，一步一步来的话，上面获取，可能会获取不到，因为，有一步，需要将2个控件，连起来，源头有了，目标也有了，得让他们关联起来啊，再Kettle中，这个连线叫做Hop（跳），就像一个管道一样，将数据流从一个点，指向另一个点。都好了，以后，我们就运行下和Java里面，一样，绿色的话，就代表成功了我们看下文件好了，我们的第一个例子，就成功了，还是很简单的，主要就是Kettle中控件的熟悉。]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kettle手册（一）- 序及Kettle简介]]></title>
    <url>%2F2017%2F03%2F27%2FKettle-handbook-01%2F</url>
    <content type="text"><![CDATA[1. 序 好久没有写博客了，新的一年总得留下点儿什么。目前主要负责数据仓库这一块任务，平时用用Kettle、SSIS这类ETL工具，而且工具的使用整理起来会方便些。所以先从Kettle开始，一点点整理下最近BI开发中掌握的知识。以前有做过BI报表Cognos开发还有些入门级的Java，都在CSDN博客上，感兴趣的同学可以去看看：于贵洋的博客好了，下面就根据自己的经验和理解，整理下Kettle的知识。 2. Kettle简介 Kettle这东西是干嘛的呢？Kettle是一个开源的ETL工具，所以基本的数据抽取、转换、加载，他都可以。比如：我要把一个mysql数据库的数据同步到一个Postgres数据库，我们有哪些办法呢?大概会有: 将数据导出为文本文件，使用PG的copy命令直接加载 数据量少的话，直接拼接成insert脚本，批量插入 一些开源的小工具，提供2种数据库直接的同步 Kettle 等等方法再比如：我每天需要统计一些系统中的异常数据，导出为Excel，用邮件发送给指定的开发人员处理，该怎样做呢？ Java或者其他开发语言做定时任务 Kettle 和其他的ETL工具相比，他有什么优势呢？ Kettle是基于Java开发的，是开源免费的，大家可以直接在网上下载；跨平台，Windows，Linux都可以使用；使用起来简单快捷。 既然开源，相比于其他收费产品，劣势也就很显然了，比如稳定性啊，BUG修复处理啊，而且基于Java，性能上会差些。当然都是相对来说，一般数据量使用或者逻辑不复杂的话，使用起来是很适合的。 刚刚也在社区上，发现了Kettle的视频，kettle视频，大家可以看看，应该用的到。Kettle的基本介绍就这些，后面会根据实际的例子，来介绍下Kettle的使用。]]></content>
      <categories>
        <category>ETL-Kettle</category>
      </categories>
      <tags>
        <tag>Kettle</tag>
      </tags>
  </entry>
</search>
